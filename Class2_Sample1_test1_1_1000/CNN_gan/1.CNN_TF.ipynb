{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imutils import paths\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import argparse\n",
    "import imutils,sklearn\n",
    "import os, cv2, re, random, shutil, imageio, pickle\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def atoi(text):\n",
    "#     return int(text) if text.isdigit() else text\n",
    "\n",
    "# def natural_keys(text):\n",
    "#     return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# def load_flower_data():\n",
    "#     # grab the list of images that we'll be describing\n",
    "#     print(\"[INFO] handling images...\")\n",
    "#     TRAIN_ORIGINAL_DIR = '../train/'\n",
    "#     TRAIN_SUB_DIR = '../subsample/'\n",
    "#     TRAIN_GAN = '../../image_gan/'\n",
    "#     TEST_DIR = '../../test/'\n",
    "\n",
    "#     # use this for full dataset\n",
    "#     train_images_gan = [TRAIN_GAN + i for i in os.listdir(TRAIN_GAN)]\n",
    "#     test_images = [TEST_DIR + i for i in os.listdir(TEST_DIR)]\n",
    "    \n",
    "#     train_images = train_images_gan\n",
    "    \n",
    "#     train_images.sort(key=natural_keys)\n",
    "#     test_images.sort(key=natural_keys)\n",
    "\n",
    "#     # initialize the features matrix and labels list\n",
    "#     trainImage = []\n",
    "#     trainLabels = []\n",
    "#     testImage = []\n",
    "#     testLabels = []\n",
    "\n",
    "#     # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(train_images):\n",
    "#         # extract the class label\n",
    "#         # get the labels from the name of the images by extract the string before \"_\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # read and resize image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         trainImage.append(img)\n",
    "#         trainLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(train_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(train_images)))\n",
    "            \n",
    "#       # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(test_images):\n",
    "#         # extract the class label\n",
    "#         # our images were named as labels.image_number.format\n",
    "#         # get the labels from the name of the images by extract the string before \".\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # extract CNN features in the image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         testImage.append(img)\n",
    "#         testLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(test_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(test_images)))\n",
    "\n",
    "\n",
    "#     trainImage = np.array(trainImage,dtype = float32)\n",
    "#     trainLabels = np.array(trainLabels)\n",
    "#     testImage = np.array(testImage,dtype = float32)\n",
    "#     testLabels = np.array(testLabels)\n",
    "#     print (trainImage.shape)\n",
    "    \n",
    "#     trainImage = trainImage.astype(np.float32) / 255\n",
    "#     testImage = testImage.astype(np.float32) / 255\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(trainLabels)\n",
    "#     list(le.classes_)\n",
    "#     trainLabels = le.transform(trainLabels) \n",
    "#     testLabels = le.transform(testLabels) \n",
    "    \n",
    "#     return trainImage, trainLabels, testImage, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainImage, trainLabels, testImage, testLabels = load_flower_data()\n",
    "\n",
    "# nb_classes = 2\n",
    "\n",
    "# # Convert class vectors to binary class matrices.\n",
    "# trainLabels = keras.utils.to_categorical(trainLabels, nb_classes)\n",
    "# print (trainLabels)\n",
    "# testLabels = keras.utils.to_categorical(testLabels, nb_classes)\n",
    "# print (testLabels)\n",
    "# print (testLabels.shape)\n",
    "\n",
    "# np.save('../trainImage.npy', trainImage)\n",
    "# np.save('../trainLabels.npy', trainLabels)\n",
    "# np.save('../testImage.npy', testImage)\n",
    "# np.save('../testLabels.npy', testLabels)\n",
    "\n",
    "# print(\"[INFO] trainImage matrix: {:.2f}MB\".format(\n",
    "#     (trainImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainLabels matrix: {:.4f}MB\".format(\n",
    "#     (trainLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage matrix: {:.2f}MB\".format(\n",
    "#     (testImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testLabels matrix: {:.4f}MB\".format(\n",
    "#     (testLabels.nbytes) / (1024 * 1000.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self, sess, epoch, batch_size, dataset_name, checkpoint_dir, log_dir, trainhist_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.trainhist_dir = trainhist_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.classname = ['Iris', 'Pansy']\n",
    "\n",
    "        # parameters\n",
    "        self.input_height = 128\n",
    "        self.input_width = 128\n",
    "        self.c_dim = 3  # color dimension\n",
    "        self.nb_class = 2\n",
    "        \n",
    "        # number of convolutional filters to use  \n",
    "#         self.nb_CNN = [32, 64, 64, 64, 128]  \n",
    "        self.nb_CNN = [32, 64, 128]  \n",
    "        # number of dense filters to use  \n",
    "        self.nb_Dense = [128] \n",
    "        # size of pooling area for max pooling  \n",
    "        self.pool_size = (2, 2)  \n",
    "        # convolution kernel size  \n",
    "        self.kernel_size = (3, 3)\n",
    "        self.batch_normalization_control = True\n",
    "        \n",
    "        # train   \n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.initial_lr = 0.001 #初始学习率\n",
    "        self.decay_steps = 10\n",
    "        self.decay_rate = 0.9\n",
    "        self.lr = tf.train.exponential_decay( self.initial_lr,\n",
    "                                     global_step=self.global_step,\n",
    "                                     decay_steps=self.decay_steps,\n",
    "                                     decay_rate=self.decay_rate)\n",
    "        self.beta1 = 0.5\n",
    "        #max model to keep saving\n",
    "        self.max_to_keep = 300\n",
    "\n",
    "        # name for checkpoint\n",
    "        self.model_name = 'CNN_C%d_D%d_Kernel(%d,%d)_%d_lrdecay0.0001' % (len(self.nb_CNN), len(self.nb_Dense),\n",
    "                                                          self.kernel_size[0], self.kernel_size[1], max(self.nb_CNN))    \n",
    "        \n",
    "        # test\n",
    "\n",
    "        #load_flower_data\n",
    "        self.train_x = np.load('../trainImage.npy')\n",
    "        self.train_y= np.load('../trainLabels.npy')\n",
    "        self.test_x = np.load('../testImage.npy')\n",
    "        self.test_y= np.load('../testLabels.npy')\n",
    "        \n",
    "        print (\"Training:\",self.train_x.shape[0])\n",
    "        \n",
    "        #记录\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['losses'] = []\n",
    "        self.train_hist['accuracy'] = []\n",
    "        self.train_hist['learning_rate'] = []\n",
    "        self.train_hist['per_epoch_ptimes'] = []\n",
    "        self.train_hist['total_ptime'] = []\n",
    "        \n",
    "        # get number of batches for a single epoch\n",
    "        self.num_batches_train = len(self.train_x) // self.batch_size\n",
    "        self.num_batches_test= len(self.test_x) // self.batch_size\n",
    "\n",
    "    def cnn_model(self, x, keep_prob, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(\"cnn\", reuse=reuse):\n",
    "             \n",
    "            #初始化参数\n",
    "            W = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "            B = tf.constant_initializer(0.0)\n",
    "        \n",
    "            print(\"CNN:x\",x.get_shape()) # 128, 128, 3 \n",
    "\n",
    "            #输入,卷积核为3*3 输出维度为32\n",
    "            net = tf.layers.conv2d(inputs = x,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "\n",
    "            if self.batch_normalization_control:\n",
    "                net = tf.layers.batch_normalization(net, training=is_training)\n",
    "            net = tf.nn.relu(net, name = 'relu_conv_1')\n",
    "            net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                          pool_size = self.pool_size,\n",
    "                                          strides = (2, 2),\n",
    "                                          padding='same',\n",
    "                                          name='pool_conv_' + str(1)\n",
    "                                         )\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            for i in range(2,len(self.nb_CNN)+1):\n",
    "                net = tf.layers.conv2d(inputs = net,                 # 输入,\n",
    "                                       filters = self.nb_CNN[i-1],      # 卷积核个数,\n",
    "                                       kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                       strides = (1, 1),\n",
    "                                       padding = 'same',              # padding方法\n",
    "                                       kernel_initializer = W,\n",
    "                                       bias_initializer = B,\n",
    "                                       kernel_regularizer = None,\n",
    "                                       bias_regularizer = None,\n",
    "                                       activity_regularizer = None,\n",
    "                                       name = 'conv_'+ str(i)        # 命名用于获取变量\n",
    "                                       )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "                if self.batch_normalization_control:\n",
    "                    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_conv_' + str(i))\n",
    "                net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_' + str(i)\n",
    "                                             )\n",
    "\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "\n",
    "            #flatten\n",
    "            net = tf.reshape(net, [-1, int(net.get_shape()[1]*net.get_shape()[2]*net.get_shape()[3])],name='flatten')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #dense layer\n",
    "            for i in range(1,len(self.nb_Dense)+1):\n",
    "                net = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_Dense[i-1],\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_' + str(i)\n",
    "                                    )\n",
    "#                 net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_dense_' + str(i))\n",
    "                net = tf.layers.dropout(inputs = net,\n",
    "                                        rate=keep_prob,\n",
    "                                        noise_shape=None,\n",
    "                                        seed=None,\n",
    "                                        training = is_training,\n",
    "                                        name= 'dropout_dense_' + str(i)\n",
    "                                        )\n",
    "            #output\n",
    "            logit = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_class,\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_output'\n",
    "                                    )\n",
    "            out_logit = tf.nn.softmax(logit, name=\"softmax\")\n",
    "            print(\"CNN:out_logit\",out_logit.get_shape())\n",
    "            print(\"------------------------\")    \n",
    "\n",
    "            return out_logit, logit\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.x = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.c_dim], \n",
    "                                name='x_image')\n",
    "\n",
    "        self.y = tf.placeholder(tf.float32, shape=[self.batch_size, self.nb_class], name='y_label')\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.add_global = self.global_step.assign_add(1)\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "\n",
    "        # output of cnn_model\n",
    "        self.out_logit, self.logit = self.cnn_model(self.x, self.keep_prob, is_training=True, reuse=False)\n",
    "        \n",
    "        self.loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y,\n",
    "                                                                                         logits =self.logit))\n",
    "        \n",
    "        \"\"\" Training \"\"\"\n",
    "        tf_vars = tf.trainable_variables()\n",
    "        cnn_vars = [var for var in tf_vars if var.name.startswith('cnn')]\n",
    "\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.cnn_optim = tf.train.AdamOptimizer(self.lr, beta1=self.beta1).minimize(self.loss_cross_entropy,\n",
    "                                                                                        var_list=cnn_vars)\n",
    "            \n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        # output of cnn_model\n",
    "        self.out_logit_test, self.logit_test = self.cnn_model(self.x, self.keep_prob, is_training=False, reuse=True)\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit_test, 1), tf.argmax(self.y, 1))\n",
    "        self.predict = tf.argmax(self.logit_test, 1)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        self.loss_sum = tf.summary.scalar(\"loss\", self.loss_cross_entropy)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver(max_to_keep = self.max_to_keep)\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_epoch = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_epoch) + 1\n",
    "            counter = 1\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name+'_train_hist.pkl', 'rb') \n",
    "            self.train_hist = pickle.load(f)\n",
    "            f.close()\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "            print(\" [!] START_EPOCH is \", start_epoch)\n",
    "        else:\n",
    "            start_epoch = 1\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        for epoch_loop in range(start_epoch, self.epoch + 1):\n",
    "\n",
    "            CNN_losses = []\n",
    "  \n",
    "            epoch_start_time = time.time()\n",
    "            shuffle_idxs = random.sample(range(0, self.train_x.shape[0]), self.train_x.shape[0])\n",
    "            shuffled_set = self.train_x[shuffle_idxs]\n",
    "            shuffled_label = self.train_y[shuffle_idxs]\n",
    "    \n",
    "            # get batch data\n",
    "            for idx in range(self.num_batches_train):\n",
    "                batch_x = shuffled_set[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_y = shuffled_label[idx*self.batch_size:(idx+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                \n",
    "\n",
    "                # update D network\n",
    "                _, summary_str, cnn_loss = self.sess.run([self.cnn_optim, self.loss_sum, self.loss_cross_entropy],\n",
    "                                               feed_dict={self.x: batch_x,\n",
    "                                                          self.y: batch_y,\n",
    "                                                          self.keep_prob: 0.5}\n",
    "                                                      )\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                CNN_losses.append(cnn_loss)\n",
    "\n",
    "                # display training status            \n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f\" % (epoch_loop, idx, \n",
    "                                                                                    self.num_batches_train, \n",
    "                                                                          time.time() - start_time, cnn_loss))\n",
    "\n",
    "            # After an epoch\n",
    "            # Evaluates accuracy on test set\n",
    "            test_accuracy_list = []\n",
    "            for idx_test in range(self.num_batches_test):\n",
    "                rand_index = np.random.choice(len(self.train_x), size=batch_size)\n",
    "                batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "                batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                   self.y: batch_y_tes,\n",
    "                                                                   self.keep_prob: 1.0})\n",
    "                test_accuracy_list.append(accuracy)\n",
    "            test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "            \n",
    "            #update learning rate\n",
    "            _, rate = sess.run([self.add_global, self.lr])\n",
    "                \n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            print('[%d/%d] - ptime: %.4f loss: %.8f acc: %.5f lr: %.8f'% (epoch_loop, self.epoch, per_epoch_ptime, \n",
    "                                                                    np.mean(CNN_losses), test_accuracy, rate))\n",
    "            self.train_hist['losses'].append(np.mean(CNN_losses))\n",
    "            self.train_hist['accuracy'].append(test_accuracy)\n",
    "            self.train_hist['learning_rate'].append(rate)\n",
    "            self.train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "            \n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, epoch_loop)\n",
    "            \n",
    "            # save trainhist for train\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name + '_train_hist.pkl', 'wb') \n",
    "            pickle.dump(self.train_hist, f)\n",
    "            f.close()\n",
    "            self.show_train_hist(self.train_hist, save=True, path= self.trainhist_dir + '/' \n",
    "                                 + self.model_name + '_train_hist.png')\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_ptime = end_time - start_time\n",
    "        self.train_hist['total_ptime'].append(total_ptime)\n",
    "        print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(self.train_hist['per_epoch_ptimes']), \n",
    "                                                                          self.epoch, total_ptime))\n",
    "        print(\" [*] Training finished!\")\n",
    "    \n",
    "        \"\"\"test after train\"\"\"\n",
    "        best_acc = max(self.train_hist['accuracy'])\n",
    "        beat_epoch = self.train_hist['accuracy'].index(best_acc) + 1\n",
    "        print (\" [*] Best Epoch: \", beat_epoch, \", Accuracy: \", best_acc)\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(beat_epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                               self.y: batch_y_tes,\n",
    "                                                               self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Best Epoch:\", beat_epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "        \n",
    "    def test(self, epoch):\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                               self.y: batch_y_tes,\n",
    "                                                               self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Epoch:\", epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "        \n",
    "    def show_all_variables(self):\n",
    "        model_vars = tf.trainable_variables()\n",
    "        tf.contrib.slim.model_analyzer.analyze_vars(model_vars, print_info=True) \n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            epoch = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read [{}], epoch [{}]\".format(ckpt_name,epoch))\n",
    "            return True, epoch\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "        \n",
    "    def show_train_hist(self, hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "        x = range(1, len(hist['losses'])+1)\n",
    "\n",
    "        y1 = hist['losses']\n",
    "        y2 = hist['accuracy']\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "                            \n",
    "        ax2 = ax1.twinx()  \n",
    "\n",
    "        ax1.plot(x, y1, 'b')\n",
    "        ax2.plot(x, y2, 'r')\n",
    "                            \n",
    "        ax1.set_xlabel('Epoch')\n",
    "                            \n",
    "        ax1.set_ylabel('CNN_loss')    \n",
    "        ax2.set_ylabel('accuracy')\n",
    "                          \n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(path, dpi = 400)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 2000\n",
      "CNN:x (100, 128, 128, 3)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 64, 64, 32)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 128)\n",
      "CNN: (100, 16, 16, 128)\n",
      "CNN: (100, 32768)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "CNN:x (100, 128, 128, 3)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 64, 64, 32)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 128)\n",
      "CNN: (100, 16, 16, 128)\n",
      "CNN: (100, 32768)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "cnn/conv_1/kernel:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "cnn/conv_1/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/batch_normalization/beta:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/batch_normalization/gamma:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/conv_2/kernel:0 (float32_ref 3x3x32x64) [18432, bytes: 73728]\n",
      "cnn/conv_2/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_3/kernel:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
      "cnn/conv_3/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_2/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_2/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/dense_1/kernel:0 (float32_ref 32768x128) [4194304, bytes: 16777216]\n",
      "cnn/dense_1/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/dense_output/kernel:0 (float32_ref 128x2) [256, bytes: 1024]\n",
      "cnn/dense_output/bias:0 (float32_ref 2) [2, bytes: 8]\n",
      "Total size of variables: 4288386\n",
      "Total bytes of variables: 17153544\n",
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load failed...\n",
      "Epoch: [ 1] [   0/  20] time: 1.4071, loss: 0.75369859\n",
      "Epoch: [ 1] [   1/  20] time: 1.5629, loss: 0.29817942\n",
      "Epoch: [ 1] [   2/  20] time: 1.7198, loss: 0.79656243\n",
      "Epoch: [ 1] [   3/  20] time: 1.8709, loss: 0.52512693\n",
      "Epoch: [ 1] [   4/  20] time: 2.0277, loss: 0.00014585\n",
      "Epoch: [ 1] [   5/  20] time: 2.1773, loss: 0.48375356\n",
      "Epoch: [ 1] [   6/  20] time: 2.3334, loss: 0.12931396\n",
      "Epoch: [ 1] [   7/  20] time: 2.4891, loss: 0.30722630\n",
      "Epoch: [ 1] [   8/  20] time: 2.6453, loss: 0.02727645\n",
      "Epoch: [ 1] [   9/  20] time: 2.8018, loss: 0.25598550\n",
      "Epoch: [ 1] [  10/  20] time: 2.9589, loss: 0.04490671\n",
      "Epoch: [ 1] [  11/  20] time: 3.1085, loss: 0.09083153\n",
      "Epoch: [ 1] [  12/  20] time: 3.2652, loss: 0.00508900\n",
      "Epoch: [ 1] [  13/  20] time: 3.4165, loss: 0.12027615\n",
      "Epoch: [ 1] [  14/  20] time: 3.5731, loss: 0.19162129\n",
      "Epoch: [ 1] [  15/  20] time: 3.7264, loss: 0.00793341\n",
      "Epoch: [ 1] [  16/  20] time: 3.8822, loss: 0.15569988\n",
      "Epoch: [ 1] [  17/  20] time: 4.0384, loss: 0.04083335\n",
      "Epoch: [ 1] [  18/  20] time: 4.1948, loss: 0.05050597\n",
      "Epoch: [ 1] [  19/  20] time: 4.3463, loss: 0.06066577\n",
      "[1/30] - ptime: 4.3883 loss: 0.21728161 acc: 0.79000 lr: 0.00098952\n",
      "Epoch: [ 2] [   0/  20] time: 5.0831, loss: 0.01409632\n",
      "Epoch: [ 2] [   1/  20] time: 5.2356, loss: 0.02681808\n",
      "Epoch: [ 2] [   2/  20] time: 5.3925, loss: 0.06242109\n",
      "Epoch: [ 2] [   3/  20] time: 5.5434, loss: 0.03363260\n",
      "Epoch: [ 2] [   4/  20] time: 5.6993, loss: 0.02515586\n",
      "Epoch: [ 2] [   5/  20] time: 5.8497, loss: 0.05577119\n",
      "Epoch: [ 2] [   6/  20] time: 6.0054, loss: 0.02410315\n",
      "Epoch: [ 2] [   7/  20] time: 6.1587, loss: 0.02380292\n",
      "Epoch: [ 2] [   8/  20] time: 6.3135, loss: 0.08841258\n",
      "Epoch: [ 2] [   9/  20] time: 6.4700, loss: 0.03149701\n",
      "Epoch: [ 2] [  10/  20] time: 6.6261, loss: 0.02000386\n",
      "Epoch: [ 2] [  11/  20] time: 6.7760, loss: 0.03341014\n",
      "Epoch: [ 2] [  12/  20] time: 6.9338, loss: 0.02530027\n",
      "Epoch: [ 2] [  13/  20] time: 7.0844, loss: 0.03521896\n",
      "Epoch: [ 2] [  14/  20] time: 7.2398, loss: 0.01428069\n",
      "Epoch: [ 2] [  15/  20] time: 7.3932, loss: 0.05723806\n",
      "Epoch: [ 2] [  16/  20] time: 7.5497, loss: 0.03757530\n",
      "Epoch: [ 2] [  17/  20] time: 7.7048, loss: 0.04509233\n",
      "Epoch: [ 2] [  18/  20] time: 7.8597, loss: 0.03853400\n",
      "Epoch: [ 2] [  19/  20] time: 8.0092, loss: 0.04428512\n",
      "[2/30] - ptime: 3.2609 loss: 0.03683248 acc: 0.79000 lr: 0.00097915\n",
      "Epoch: [ 3] [   0/  20] time: 8.7293, loss: 0.02029273\n",
      "Epoch: [ 3] [   1/  20] time: 8.8806, loss: 0.04987958\n",
      "Epoch: [ 3] [   2/  20] time: 9.0367, loss: 0.02255495\n",
      "Epoch: [ 3] [   3/  20] time: 9.1861, loss: 0.07002569\n",
      "Epoch: [ 3] [   4/  20] time: 9.3425, loss: 0.02587141\n",
      "Epoch: [ 3] [   5/  20] time: 9.4978, loss: 0.02723815\n",
      "Epoch: [ 3] [   6/  20] time: 9.6546, loss: 0.01929713\n",
      "Epoch: [ 3] [   7/  20] time: 9.8098, loss: 0.02183868\n",
      "Epoch: [ 3] [   8/  20] time: 9.9653, loss: 0.04178314\n",
      "Epoch: [ 3] [   9/  20] time: 10.1156, loss: 0.00702871\n",
      "Epoch: [ 3] [  10/  20] time: 10.2705, loss: 0.01498181\n",
      "Epoch: [ 3] [  11/  20] time: 10.4219, loss: 0.06535713\n",
      "Epoch: [ 3] [  12/  20] time: 10.5792, loss: 0.10644510\n",
      "Epoch: [ 3] [  13/  20] time: 10.7346, loss: 0.05922722\n",
      "Epoch: [ 3] [  14/  20] time: 10.8899, loss: 0.00374552\n",
      "Epoch: [ 3] [  15/  20] time: 11.0452, loss: 0.01739462\n",
      "Epoch: [ 3] [  16/  20] time: 11.2000, loss: 0.05916263\n",
      "Epoch: [ 3] [  17/  20] time: 11.3531, loss: 0.02861890\n",
      "Epoch: [ 3] [  18/  20] time: 11.5092, loss: 0.13234560\n",
      "Epoch: [ 3] [  19/  20] time: 11.6580, loss: 0.00815029\n",
      "[3/30] - ptime: 3.2686 loss: 0.04006195 acc: 0.79000 lr: 0.00096889\n",
      "Epoch: [ 4] [   0/  20] time: 12.4596, loss: 0.03354195\n",
      "Epoch: [ 4] [   1/  20] time: 12.6112, loss: 0.02919848\n",
      "Epoch: [ 4] [   2/  20] time: 12.7660, loss: 0.00080878\n",
      "Epoch: [ 4] [   3/  20] time: 12.9153, loss: 0.04580307\n",
      "Epoch: [ 4] [   4/  20] time: 13.0712, loss: 0.03842448\n",
      "Epoch: [ 4] [   5/  20] time: 13.2243, loss: 0.01567494\n",
      "Epoch: [ 4] [   6/  20] time: 13.3816, loss: 0.04880330\n",
      "Epoch: [ 4] [   7/  20] time: 13.5349, loss: 0.00056071\n",
      "Epoch: [ 4] [   8/  20] time: 13.6903, loss: 0.03738491\n",
      "Epoch: [ 4] [   9/  20] time: 13.8436, loss: 0.05130539\n",
      "Epoch: [ 4] [  10/  20] time: 13.9983, loss: 0.03196197\n",
      "Epoch: [ 4] [  11/  20] time: 14.1496, loss: 0.02137177\n",
      "Epoch: [ 4] [  12/  20] time: 14.3058, loss: 0.03199002\n",
      "Epoch: [ 4] [  13/  20] time: 14.4608, loss: 0.01512544\n",
      "Epoch: [ 4] [  14/  20] time: 14.6155, loss: 0.00767680\n",
      "Epoch: [ 4] [  15/  20] time: 14.7695, loss: 0.11038707\n",
      "Epoch: [ 4] [  16/  20] time: 14.9246, loss: 0.06105502\n",
      "Epoch: [ 4] [  17/  20] time: 15.0782, loss: 0.03451397\n",
      "Epoch: [ 4] [  18/  20] time: 15.2333, loss: 0.02733676\n",
      "Epoch: [ 4] [  19/  20] time: 15.3838, loss: 0.01153260\n",
      "[4/30] - ptime: 3.2656 loss: 0.03272287 acc: 0.79000 lr: 0.00095873\n",
      "Epoch: [ 5] [   0/  20] time: 16.1026, loss: 0.01261068\n",
      "Epoch: [ 5] [   1/  20] time: 16.2505, loss: 0.02140234\n",
      "Epoch: [ 5] [   2/  20] time: 16.4086, loss: 0.02045284\n",
      "Epoch: [ 5] [   3/  20] time: 16.5590, loss: 0.01046235\n",
      "Epoch: [ 5] [   4/  20] time: 16.7152, loss: 0.00457313\n",
      "Epoch: [ 5] [   5/  20] time: 16.8697, loss: 0.01997728\n",
      "Epoch: [ 5] [   6/  20] time: 17.0261, loss: 0.02345162\n",
      "Epoch: [ 5] [   7/  20] time: 17.1813, loss: 0.03233669\n",
      "Epoch: [ 5] [   8/  20] time: 17.3366, loss: 0.03707523\n",
      "Epoch: [ 5] [   9/  20] time: 17.4876, loss: 0.00575012\n",
      "Epoch: [ 5] [  10/  20] time: 17.6457, loss: 0.05049842\n",
      "Epoch: [ 5] [  11/  20] time: 17.7947, loss: 0.03410180\n",
      "Epoch: [ 5] [  12/  20] time: 17.9511, loss: 0.01907342\n",
      "Epoch: [ 5] [  13/  20] time: 18.1049, loss: 0.02535781\n",
      "Epoch: [ 5] [  14/  20] time: 18.2600, loss: 0.00889053\n",
      "Epoch: [ 5] [  15/  20] time: 18.4173, loss: 0.03970015\n",
      "Epoch: [ 5] [  16/  20] time: 18.5747, loss: 0.03367577\n",
      "Epoch: [ 5] [  17/  20] time: 18.7265, loss: 0.03036477\n",
      "Epoch: [ 5] [  18/  20] time: 18.8822, loss: 0.01584436\n",
      "Epoch: [ 5] [  19/  20] time: 19.0317, loss: 0.00652765\n",
      "[5/30] - ptime: 3.2802 loss: 0.02260635 acc: 0.79000 lr: 0.00094868\n",
      "Epoch: [ 6] [   0/  20] time: 19.7868, loss: 0.00668142\n",
      "Epoch: [ 6] [   1/  20] time: 19.9346, loss: 0.00988205\n",
      "Epoch: [ 6] [   2/  20] time: 20.0913, loss: 0.02423396\n",
      "Epoch: [ 6] [   3/  20] time: 20.2410, loss: 0.00840017\n",
      "Epoch: [ 6] [   4/  20] time: 20.3979, loss: 0.04113792\n",
      "Epoch: [ 6] [   5/  20] time: 20.5526, loss: 0.00939516\n",
      "Epoch: [ 6] [   6/  20] time: 20.7083, loss: 0.01501280\n",
      "Epoch: [ 6] [   7/  20] time: 20.8650, loss: 0.01044764\n",
      "Epoch: [ 6] [   8/  20] time: 21.0257, loss: 0.00540300\n",
      "Epoch: [ 6] [   9/  20] time: 21.1764, loss: 0.01506875\n",
      "Epoch: [ 6] [  10/  20] time: 21.3330, loss: 0.02954123\n",
      "Epoch: [ 6] [  11/  20] time: 21.4852, loss: 0.02575800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 6] [  12/  20] time: 21.6415, loss: 0.02236666\n",
      "Epoch: [ 6] [  13/  20] time: 21.7956, loss: 0.01215334\n",
      "Epoch: [ 6] [  14/  20] time: 21.9520, loss: 0.06032801\n",
      "Epoch: [ 6] [  15/  20] time: 22.1075, loss: 0.01378124\n",
      "Epoch: [ 6] [  16/  20] time: 22.2643, loss: 0.02446437\n",
      "Epoch: [ 6] [  17/  20] time: 22.4136, loss: 0.03124159\n",
      "Epoch: [ 6] [  18/  20] time: 22.5734, loss: 0.01595325\n",
      "Epoch: [ 6] [  19/  20] time: 22.7230, loss: 0.00857198\n",
      "[6/30] - ptime: 3.2983 loss: 0.01949112 acc: 0.79000 lr: 0.00093874\n",
      "Epoch: [ 7] [   0/  20] time: 23.5257, loss: 0.01762472\n",
      "Epoch: [ 7] [   1/  20] time: 23.6735, loss: 0.00724638\n",
      "Epoch: [ 7] [   2/  20] time: 23.8315, loss: 0.03351281\n",
      "Epoch: [ 7] [   3/  20] time: 23.9810, loss: 0.01534554\n",
      "Epoch: [ 7] [   4/  20] time: 24.1374, loss: 0.02744315\n",
      "Epoch: [ 7] [   5/  20] time: 24.2937, loss: 0.01953941\n",
      "Epoch: [ 7] [   6/  20] time: 24.4508, loss: 0.03774785\n",
      "Epoch: [ 7] [   7/  20] time: 24.6107, loss: 0.06087149\n",
      "Epoch: [ 7] [   8/  20] time: 24.7680, loss: 0.01399359\n",
      "Epoch: [ 7] [   9/  20] time: 24.9171, loss: 0.01174462\n",
      "Epoch: [ 7] [  10/  20] time: 25.0766, loss: 0.00222030\n",
      "Epoch: [ 7] [  11/  20] time: 25.2258, loss: 0.01661523\n",
      "Epoch: [ 7] [  12/  20] time: 25.3836, loss: 0.02129859\n",
      "Epoch: [ 7] [  13/  20] time: 25.5381, loss: 0.00527495\n",
      "Epoch: [ 7] [  14/  20] time: 25.6949, loss: 0.02107555\n",
      "Epoch: [ 7] [  15/  20] time: 25.8642, loss: 0.02600151\n",
      "Epoch: [ 7] [  16/  20] time: 26.0380, loss: 0.01224559\n",
      "Epoch: [ 7] [  17/  20] time: 26.1909, loss: 0.01183359\n",
      "Epoch: [ 7] [  18/  20] time: 26.3481, loss: 0.02932723\n",
      "Epoch: [ 7] [  19/  20] time: 26.4990, loss: 0.01875053\n",
      "[7/30] - ptime: 3.3321 loss: 0.02048563 acc: 0.79000 lr: 0.00092890\n",
      "Epoch: [ 8] [   0/  20] time: 27.2587, loss: 0.01234196\n",
      "Epoch: [ 8] [   1/  20] time: 27.4122, loss: 0.00083353\n",
      "Epoch: [ 8] [   2/  20] time: 27.5698, loss: 0.00124726\n",
      "Epoch: [ 8] [   3/  20] time: 27.7192, loss: 0.00259427\n",
      "Epoch: [ 8] [   4/  20] time: 27.8782, loss: 0.02225865\n",
      "Epoch: [ 8] [   5/  20] time: 28.0341, loss: 0.02989103\n",
      "Epoch: [ 8] [   6/  20] time: 28.1900, loss: 0.02018097\n",
      "Epoch: [ 8] [   7/  20] time: 28.3433, loss: 0.05032988\n",
      "Epoch: [ 8] [   8/  20] time: 28.4996, loss: 0.01300055\n",
      "Epoch: [ 8] [   9/  20] time: 28.6548, loss: 0.02121979\n",
      "Epoch: [ 8] [  10/  20] time: 28.8113, loss: 0.01134926\n",
      "Epoch: [ 8] [  11/  20] time: 28.9620, loss: 0.01881989\n",
      "Epoch: [ 8] [  12/  20] time: 29.1193, loss: 0.00701796\n",
      "Epoch: [ 8] [  13/  20] time: 29.2732, loss: 0.01248616\n",
      "Epoch: [ 8] [  14/  20] time: 29.4297, loss: 0.01428827\n",
      "Epoch: [ 8] [  15/  20] time: 29.5835, loss: 0.02668402\n",
      "Epoch: [ 8] [  16/  20] time: 29.7402, loss: 0.00784418\n",
      "Epoch: [ 8] [  17/  20] time: 29.8986, loss: 0.01076249\n",
      "Epoch: [ 8] [  18/  20] time: 30.0542, loss: 0.05155258\n",
      "Epoch: [ 8] [  19/  20] time: 30.2054, loss: 0.05873300\n",
      "[8/30] - ptime: 3.2889 loss: 0.01967179 acc: 0.79000 lr: 0.00091917\n",
      "Epoch: [ 9] [   0/  20] time: 30.9229, loss: 0.02289860\n",
      "Epoch: [ 9] [   1/  20] time: 31.0724, loss: 0.02360062\n",
      "Epoch: [ 9] [   2/  20] time: 31.2277, loss: 0.03635974\n",
      "Epoch: [ 9] [   3/  20] time: 31.3783, loss: 0.00997994\n",
      "Epoch: [ 9] [   4/  20] time: 31.5353, loss: 0.05045938\n",
      "Epoch: [ 9] [   5/  20] time: 31.6887, loss: 0.00915661\n",
      "Epoch: [ 9] [   6/  20] time: 31.8457, loss: 0.02237669\n",
      "Epoch: [ 9] [   7/  20] time: 32.0012, loss: 0.08109591\n",
      "Epoch: [ 9] [   8/  20] time: 32.1570, loss: 0.02630723\n",
      "Epoch: [ 9] [   9/  20] time: 32.3127, loss: 0.10446949\n",
      "Epoch: [ 9] [  10/  20] time: 32.4695, loss: 0.36242610\n",
      "Epoch: [ 9] [  11/  20] time: 32.6207, loss: 0.07521293\n",
      "Epoch: [ 9] [  12/  20] time: 32.7794, loss: 0.01355211\n",
      "Epoch: [ 9] [  13/  20] time: 32.9331, loss: 0.03927826\n",
      "Epoch: [ 9] [  14/  20] time: 33.0885, loss: 0.00439682\n",
      "Epoch: [ 9] [  15/  20] time: 33.2420, loss: 0.02293894\n",
      "Epoch: [ 9] [  16/  20] time: 33.3996, loss: 0.04696055\n",
      "Epoch: [ 9] [  17/  20] time: 33.5558, loss: 0.01825076\n",
      "Epoch: [ 9] [  18/  20] time: 33.7125, loss: 0.02730002\n",
      "Epoch: [ 9] [  19/  20] time: 33.8642, loss: 0.08391631\n",
      "[9/30] - ptime: 3.2792 loss: 0.05404685 acc: 0.78000 lr: 0.00090953\n",
      "Epoch: [10] [   0/  20] time: 34.5777, loss: 0.00599619\n",
      "Epoch: [10] [   1/  20] time: 34.7258, loss: 0.01798329\n",
      "Epoch: [10] [   2/  20] time: 34.8869, loss: 0.15310208\n",
      "Epoch: [10] [   3/  20] time: 35.0361, loss: 0.01115836\n",
      "Epoch: [10] [   4/  20] time: 35.1922, loss: 0.01247706\n",
      "Epoch: [10] [   5/  20] time: 35.3486, loss: 0.06766353\n",
      "Epoch: [10] [   6/  20] time: 35.5054, loss: 0.02748090\n",
      "Epoch: [10] [   7/  20] time: 35.6597, loss: 0.02755033\n",
      "Epoch: [10] [   8/  20] time: 35.8153, loss: 0.00660616\n",
      "Epoch: [10] [   9/  20] time: 35.9677, loss: 0.02148017\n",
      "Epoch: [10] [  10/  20] time: 36.1317, loss: 0.05853314\n",
      "Epoch: [10] [  11/  20] time: 36.2829, loss: 0.09593007\n",
      "Epoch: [10] [  12/  20] time: 36.4392, loss: 0.07019603\n",
      "Epoch: [10] [  13/  20] time: 36.5941, loss: 0.01510502\n",
      "Epoch: [10] [  14/  20] time: 36.7500, loss: 0.04973568\n",
      "Epoch: [10] [  15/  20] time: 36.9034, loss: 0.02501106\n",
      "Epoch: [10] [  16/  20] time: 37.0603, loss: 0.01051785\n",
      "Epoch: [10] [  17/  20] time: 37.2126, loss: 0.03028018\n",
      "Epoch: [10] [  18/  20] time: 37.3697, loss: 0.00768488\n",
      "Epoch: [10] [  19/  20] time: 37.5221, loss: 0.00625614\n",
      "[10/30] - ptime: 3.2906 loss: 0.03603741 acc: 0.77000 lr: 0.00090000\n",
      "Epoch: [11] [   0/  20] time: 38.3075, loss: 0.05836018\n",
      "Epoch: [11] [   1/  20] time: 38.4575, loss: 0.01004159\n",
      "Epoch: [11] [   2/  20] time: 38.6195, loss: 0.00325176\n",
      "Epoch: [11] [   3/  20] time: 38.7712, loss: 0.01400252\n",
      "Epoch: [11] [   4/  20] time: 38.9271, loss: 0.02436075\n",
      "Epoch: [11] [   5/  20] time: 39.0855, loss: 0.01154527\n",
      "Epoch: [11] [   6/  20] time: 39.2504, loss: 0.00399244\n",
      "Epoch: [11] [   7/  20] time: 39.4202, loss: 0.01035924\n",
      "Epoch: [11] [   8/  20] time: 39.5811, loss: 0.00187645\n",
      "Epoch: [11] [   9/  20] time: 39.7462, loss: 0.01214190\n",
      "Epoch: [11] [  10/  20] time: 39.9077, loss: 0.01006925\n",
      "Epoch: [11] [  11/  20] time: 40.0688, loss: 0.02389454\n",
      "Epoch: [11] [  12/  20] time: 40.2261, loss: 0.01023505\n",
      "Epoch: [11] [  13/  20] time: 40.3891, loss: 0.02142552\n",
      "Epoch: [11] [  14/  20] time: 40.5455, loss: 0.00010949\n",
      "Epoch: [11] [  15/  20] time: 40.7079, loss: 0.00587142\n",
      "Epoch: [11] [  16/  20] time: 40.8655, loss: 0.07084389\n",
      "Epoch: [11] [  17/  20] time: 41.0293, loss: 0.02193097\n",
      "Epoch: [11] [  18/  20] time: 41.1867, loss: 0.01766365\n",
      "Epoch: [11] [  19/  20] time: 41.3410, loss: 0.02726025\n",
      "[11/30] - ptime: 3.4276 loss: 0.01796180 acc: 0.77000 lr: 0.00089057\n",
      "Epoch: [12] [   0/  20] time: 42.0812, loss: 0.00452230\n",
      "Epoch: [12] [   1/  20] time: 42.2337, loss: 0.01256586\n",
      "Epoch: [12] [   2/  20] time: 42.3911, loss: 0.00102545\n",
      "Epoch: [12] [   3/  20] time: 42.5430, loss: 0.01212826\n",
      "Epoch: [12] [   4/  20] time: 42.7014, loss: 0.00639820\n",
      "Epoch: [12] [   5/  20] time: 42.8542, loss: 0.01227752\n",
      "Epoch: [12] [   6/  20] time: 43.0096, loss: 0.01212113\n",
      "Epoch: [12] [   7/  20] time: 43.1659, loss: 0.01080312\n",
      "Epoch: [12] [   8/  20] time: 43.3223, loss: 0.00142861\n",
      "Epoch: [12] [   9/  20] time: 43.4794, loss: 0.01075515\n",
      "Epoch: [12] [  10/  20] time: 43.6362, loss: 0.01514594\n",
      "Epoch: [12] [  11/  20] time: 43.7864, loss: 0.01798910\n",
      "Epoch: [12] [  12/  20] time: 43.9427, loss: 0.01202306\n",
      "Epoch: [12] [  13/  20] time: 44.0951, loss: 0.04246576\n",
      "Epoch: [12] [  14/  20] time: 44.2507, loss: 0.00757399\n",
      "Epoch: [12] [  15/  20] time: 44.4050, loss: 0.00044721\n",
      "Epoch: [12] [  16/  20] time: 44.5613, loss: 0.03023308\n",
      "Epoch: [12] [  17/  20] time: 44.7172, loss: 0.00180467\n",
      "Epoch: [12] [  18/  20] time: 44.8733, loss: 0.00831212\n",
      "Epoch: [12] [  19/  20] time: 45.0239, loss: 0.00440324\n",
      "[12/30] - ptime: 3.2908 loss: 0.01122119 acc: 0.77000 lr: 0.00088123\n",
      "Epoch: [13] [   0/  20] time: 45.7831, loss: 0.00782496\n",
      "Epoch: [13] [   1/  20] time: 45.9454, loss: 0.00208289\n",
      "Epoch: [13] [   2/  20] time: 46.1051, loss: 0.00100136\n",
      "Epoch: [13] [   3/  20] time: 46.2672, loss: 0.03056115\n",
      "Epoch: [13] [   4/  20] time: 46.4294, loss: 0.01590897\n",
      "Epoch: [13] [   5/  20] time: 46.5870, loss: 0.00858117\n",
      "Epoch: [13] [   6/  20] time: 46.7455, loss: 0.01745512\n",
      "Epoch: [13] [   7/  20] time: 46.8987, loss: 0.00546424\n",
      "Epoch: [13] [   8/  20] time: 47.0543, loss: 0.01757199\n",
      "Epoch: [13] [   9/  20] time: 47.2124, loss: 0.00745268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [13] [  10/  20] time: 47.3688, loss: 0.00154546\n",
      "Epoch: [13] [  11/  20] time: 47.5196, loss: 0.00344721\n",
      "Epoch: [13] [  12/  20] time: 47.6781, loss: 0.02722838\n",
      "Epoch: [13] [  13/  20] time: 47.8270, loss: 0.00013720\n",
      "Epoch: [13] [  14/  20] time: 47.9865, loss: 0.00678642\n",
      "Epoch: [13] [  15/  20] time: 48.1396, loss: 0.07440519\n",
      "Epoch: [13] [  16/  20] time: 48.2956, loss: 0.02350296\n",
      "Epoch: [13] [  17/  20] time: 48.4560, loss: 0.01452453\n",
      "Epoch: [13] [  18/  20] time: 48.6176, loss: 0.00059915\n",
      "Epoch: [13] [  19/  20] time: 48.7732, loss: 0.00353232\n",
      "[13/30] - ptime: 3.3468 loss: 0.01348067 acc: 0.81000 lr: 0.00087200\n",
      "Epoch: [14] [   0/  20] time: 49.5541, loss: 0.00275753\n",
      "Epoch: [14] [   1/  20] time: 49.7159, loss: 0.00824368\n",
      "Epoch: [14] [   2/  20] time: 49.8770, loss: 0.01212340\n",
      "Epoch: [14] [   3/  20] time: 50.0270, loss: 0.01338857\n",
      "Epoch: [14] [   4/  20] time: 50.1830, loss: 0.00509912\n",
      "Epoch: [14] [   5/  20] time: 50.3333, loss: 0.00222886\n",
      "Epoch: [14] [   6/  20] time: 50.4909, loss: 0.00150291\n",
      "Epoch: [14] [   7/  20] time: 50.6442, loss: 0.00033158\n",
      "Epoch: [14] [   8/  20] time: 50.8011, loss: 0.00240005\n",
      "Epoch: [14] [   9/  20] time: 50.9576, loss: 0.00777147\n",
      "Epoch: [14] [  10/  20] time: 51.1188, loss: 0.01468473\n",
      "Epoch: [14] [  11/  20] time: 51.2786, loss: 0.01606416\n",
      "Epoch: [14] [  12/  20] time: 51.4357, loss: 0.01510765\n",
      "Epoch: [14] [  13/  20] time: 51.5861, loss: 0.02459173\n",
      "Epoch: [14] [  14/  20] time: 51.7427, loss: 0.02409856\n",
      "Epoch: [14] [  15/  20] time: 51.8960, loss: 0.00350816\n",
      "Epoch: [14] [  16/  20] time: 52.0515, loss: 0.00729764\n",
      "Epoch: [14] [  17/  20] time: 52.2049, loss: 0.00011972\n",
      "Epoch: [14] [  18/  20] time: 52.3625, loss: 0.02983629\n",
      "Epoch: [14] [  19/  20] time: 52.5186, loss: 0.01791160\n",
      "[14/30] - ptime: 3.3280 loss: 0.01045337 acc: 0.77000 lr: 0.00086286\n",
      "Epoch: [15] [   0/  20] time: 53.3100, loss: 0.00770527\n",
      "Epoch: [15] [   1/  20] time: 53.4705, loss: 0.01388327\n",
      "Epoch: [15] [   2/  20] time: 53.6303, loss: 0.00835686\n",
      "Epoch: [15] [   3/  20] time: 53.7854, loss: 0.00265506\n",
      "Epoch: [15] [   4/  20] time: 53.9404, loss: 0.00569522\n",
      "Epoch: [15] [   5/  20] time: 54.0903, loss: 0.01196270\n",
      "Epoch: [15] [   6/  20] time: 54.2482, loss: 0.00256024\n",
      "Epoch: [15] [   7/  20] time: 54.3987, loss: 0.00003221\n",
      "Epoch: [15] [   8/  20] time: 54.5554, loss: 0.08333536\n",
      "Epoch: [15] [   9/  20] time: 54.7097, loss: 0.01664749\n",
      "Epoch: [15] [  10/  20] time: 54.8659, loss: 0.00726402\n",
      "Epoch: [15] [  11/  20] time: 55.0221, loss: 0.00454944\n",
      "Epoch: [15] [  12/  20] time: 55.1774, loss: 0.05000997\n",
      "Epoch: [15] [  13/  20] time: 55.3282, loss: 0.01037529\n",
      "Epoch: [15] [  14/  20] time: 55.4851, loss: 0.00025516\n",
      "Epoch: [15] [  15/  20] time: 55.6358, loss: 0.00390749\n",
      "Epoch: [15] [  16/  20] time: 55.7929, loss: 0.02568605\n",
      "Epoch: [15] [  17/  20] time: 55.9459, loss: 0.01211297\n",
      "Epoch: [15] [  18/  20] time: 56.1019, loss: 0.00721363\n",
      "Epoch: [15] [  19/  20] time: 56.2575, loss: 0.01736526\n",
      "[15/30] - ptime: 3.3129 loss: 0.01457865 acc: 0.76000 lr: 0.00086286\n",
      "Epoch: [16] [   0/  20] time: 57.0149, loss: 0.00229649\n",
      "Epoch: [16] [   1/  20] time: 57.1660, loss: 0.00273386\n",
      "Epoch: [16] [   2/  20] time: 57.3224, loss: 0.00937418\n",
      "Epoch: [16] [   3/  20] time: 57.4777, loss: 0.00179382\n",
      "Epoch: [16] [   4/  20] time: 57.6349, loss: 0.01410541\n",
      "Epoch: [16] [   5/  20] time: 57.7846, loss: 0.01244200\n",
      "Epoch: [16] [   6/  20] time: 57.9422, loss: 0.00522378\n",
      "Epoch: [16] [   7/  20] time: 58.0915, loss: 0.00135968\n",
      "Epoch: [16] [   8/  20] time: 58.2484, loss: 0.00529718\n",
      "Epoch: [16] [   9/  20] time: 58.4009, loss: 0.00244401\n",
      "Epoch: [16] [  10/  20] time: 58.5569, loss: 0.00680620\n",
      "Epoch: [16] [  11/  20] time: 58.7135, loss: 0.01450457\n",
      "Epoch: [16] [  12/  20] time: 58.8691, loss: 0.02525315\n",
      "Epoch: [16] [  13/  20] time: 59.0188, loss: 0.00485030\n",
      "Epoch: [16] [  14/  20] time: 59.1756, loss: 0.00186893\n",
      "Epoch: [16] [  15/  20] time: 59.3262, loss: 0.00901834\n",
      "Epoch: [16] [  16/  20] time: 59.4827, loss: 0.01584626\n",
      "Epoch: [16] [  17/  20] time: 59.6368, loss: 0.01908765\n",
      "Epoch: [16] [  18/  20] time: 59.7927, loss: 0.00973242\n",
      "Epoch: [16] [  19/  20] time: 59.9470, loss: 0.04418863\n",
      "[16/30] - ptime: 3.3006 loss: 0.01041134 acc: 0.75000 lr: 0.00085381\n",
      "Epoch: [17] [   0/  20] time: 60.6515, loss: 0.00768050\n",
      "Epoch: [17] [   1/  20] time: 60.8152, loss: 0.00301650\n",
      "Epoch: [17] [   2/  20] time: 60.9738, loss: 0.00387037\n",
      "Epoch: [17] [   3/  20] time: 61.1235, loss: 0.00422322\n",
      "Epoch: [17] [   4/  20] time: 61.2802, loss: 0.02466184\n",
      "Epoch: [17] [   5/  20] time: 61.4307, loss: 0.01643405\n",
      "Epoch: [17] [   6/  20] time: 61.5877, loss: 0.00897770\n",
      "Epoch: [17] [   7/  20] time: 61.7418, loss: 0.01466602\n",
      "Epoch: [17] [   8/  20] time: 61.8982, loss: 0.00688664\n",
      "Epoch: [17] [   9/  20] time: 62.0519, loss: 0.00951091\n",
      "Epoch: [17] [  10/  20] time: 62.2069, loss: 0.01054099\n",
      "Epoch: [17] [  11/  20] time: 62.3579, loss: 0.00507696\n",
      "Epoch: [17] [  12/  20] time: 62.5158, loss: 0.00546587\n",
      "Epoch: [17] [  13/  20] time: 62.6672, loss: 0.00423954\n",
      "Epoch: [17] [  14/  20] time: 62.8245, loss: 0.00003538\n",
      "Epoch: [17] [  15/  20] time: 62.9784, loss: 0.01219302\n",
      "Epoch: [17] [  16/  20] time: 63.1350, loss: 0.00221955\n",
      "Epoch: [17] [  17/  20] time: 63.2909, loss: 0.02043922\n",
      "Epoch: [17] [  18/  20] time: 63.4466, loss: 0.01685907\n",
      "Epoch: [17] [  19/  20] time: 63.5973, loss: 0.04022357\n",
      "[17/30] - ptime: 3.2946 loss: 0.01086105 acc: 0.76000 lr: 0.00083601\n",
      "Epoch: [18] [   0/  20] time: 64.3937, loss: 0.02661616\n",
      "Epoch: [18] [   1/  20] time: 64.5570, loss: 0.00573012\n",
      "Epoch: [18] [   2/  20] time: 64.7172, loss: 0.00511472\n",
      "Epoch: [18] [   3/  20] time: 64.8703, loss: 0.01728862\n",
      "Epoch: [18] [   4/  20] time: 65.0263, loss: 0.00547493\n",
      "Epoch: [18] [   5/  20] time: 65.1767, loss: 0.02695681\n",
      "Epoch: [18] [   6/  20] time: 65.3350, loss: 0.02727942\n",
      "Epoch: [18] [   7/  20] time: 65.4886, loss: 0.01048030\n",
      "Epoch: [18] [   8/  20] time: 65.6463, loss: 0.00884168\n",
      "Epoch: [18] [   9/  20] time: 65.7988, loss: 0.00174041\n",
      "Epoch: [18] [  10/  20] time: 65.9550, loss: 0.00900982\n",
      "Epoch: [18] [  11/  20] time: 66.1084, loss: 0.00718244\n",
      "Epoch: [18] [  12/  20] time: 66.2638, loss: 0.00017769\n",
      "Epoch: [18] [  13/  20] time: 66.4146, loss: 0.00016866\n",
      "Epoch: [18] [  14/  20] time: 66.5716, loss: 0.00001939\n",
      "Epoch: [18] [  15/  20] time: 66.7265, loss: 0.00043622\n",
      "Epoch: [18] [  16/  20] time: 66.8818, loss: 0.01929042\n",
      "Epoch: [18] [  17/  20] time: 67.0331, loss: 0.00489900\n",
      "Epoch: [18] [  18/  20] time: 67.1897, loss: 0.00207211\n",
      "Epoch: [18] [  19/  20] time: 67.3442, loss: 0.01165340\n",
      "[18/30] - ptime: 3.3040 loss: 0.00952161 acc: 0.74000 lr: 0.00082725\n",
      "Epoch: [19] [   0/  20] time: 68.0580, loss: 0.04810522\n",
      "Epoch: [19] [   1/  20] time: 68.2205, loss: 0.01226359\n",
      "Epoch: [19] [   2/  20] time: 68.3796, loss: 0.00036781\n",
      "Epoch: [19] [   3/  20] time: 68.5292, loss: 0.07812289\n",
      "Epoch: [19] [   4/  20] time: 68.6869, loss: 0.03489295\n",
      "Epoch: [19] [   5/  20] time: 68.8364, loss: 0.00677794\n",
      "Epoch: [19] [   6/  20] time: 68.9941, loss: 0.00186070\n",
      "Epoch: [19] [   7/  20] time: 69.1474, loss: 0.01220750\n",
      "Epoch: [19] [   8/  20] time: 69.3033, loss: 0.00987737\n",
      "Epoch: [19] [   9/  20] time: 69.4585, loss: 0.00420631\n",
      "Epoch: [19] [  10/  20] time: 69.6141, loss: 0.00698034\n",
      "Epoch: [19] [  11/  20] time: 69.7637, loss: 0.01744443\n",
      "Epoch: [19] [  12/  20] time: 69.9209, loss: 0.00477379\n",
      "Epoch: [19] [  13/  20] time: 70.0711, loss: 0.01872870\n",
      "Epoch: [19] [  14/  20] time: 70.2280, loss: 0.01141888\n",
      "Epoch: [19] [  15/  20] time: 70.3818, loss: 0.00299646\n",
      "Epoch: [19] [  16/  20] time: 70.5371, loss: 0.03245809\n",
      "Epoch: [19] [  17/  20] time: 70.6949, loss: 0.00229292\n",
      "Epoch: [19] [  18/  20] time: 70.8515, loss: 0.01604822\n",
      "Epoch: [19] [  19/  20] time: 71.0016, loss: 0.01635197\n",
      "[19/30] - ptime: 3.2981 loss: 0.01690880 acc: 0.77000 lr: 0.00081858\n",
      "Epoch: [20] [   0/  20] time: 71.7334, loss: 0.00165495\n",
      "Epoch: [20] [   1/  20] time: 71.8967, loss: 0.00213210\n",
      "Epoch: [20] [   2/  20] time: 72.0542, loss: 0.02309099\n",
      "Epoch: [20] [   3/  20] time: 72.2032, loss: 0.00113625\n",
      "Epoch: [20] [   4/  20] time: 72.3605, loss: 0.00506418\n",
      "Epoch: [20] [   5/  20] time: 72.5104, loss: 0.02040860\n",
      "Epoch: [20] [   6/  20] time: 72.6684, loss: 0.00162375\n",
      "Epoch: [20] [   7/  20] time: 72.8221, loss: 0.01415615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20] [   8/  20] time: 72.9783, loss: 0.00129762\n",
      "Epoch: [20] [   9/  20] time: 73.1371, loss: 0.01080566\n",
      "Epoch: [20] [  10/  20] time: 73.2943, loss: 0.02683591\n",
      "Epoch: [20] [  11/  20] time: 73.4442, loss: 0.00485784\n",
      "Epoch: [20] [  12/  20] time: 73.6013, loss: 0.00363281\n",
      "Epoch: [20] [  13/  20] time: 73.7522, loss: 0.00817051\n",
      "Epoch: [20] [  14/  20] time: 73.9079, loss: 0.00520701\n",
      "Epoch: [20] [  15/  20] time: 74.0616, loss: 0.00043926\n",
      "Epoch: [20] [  16/  20] time: 74.2177, loss: 0.00786360\n",
      "Epoch: [20] [  17/  20] time: 74.3733, loss: 0.00076743\n",
      "Epoch: [20] [  18/  20] time: 74.5308, loss: 0.00077287\n",
      "Epoch: [20] [  19/  20] time: 74.6816, loss: 0.00269746\n",
      "[20/30] - ptime: 3.3105 loss: 0.00713075 acc: 0.78000 lr: 0.00081000\n",
      "Epoch: [21] [   0/  20] time: 75.4324, loss: 0.00141147\n",
      "Epoch: [21] [   1/  20] time: 75.5945, loss: 0.00667652\n",
      "Epoch: [21] [   2/  20] time: 75.7518, loss: 0.00940711\n",
      "Epoch: [21] [   3/  20] time: 75.9011, loss: 0.00068378\n",
      "Epoch: [21] [   4/  20] time: 76.0583, loss: 0.00532661\n",
      "Epoch: [21] [   5/  20] time: 76.2126, loss: 0.00001953\n",
      "Epoch: [21] [   6/  20] time: 76.3707, loss: 0.00508808\n",
      "Epoch: [21] [   7/  20] time: 76.5255, loss: 0.00486515\n",
      "Epoch: [21] [   8/  20] time: 76.6826, loss: 0.00043187\n",
      "Epoch: [21] [   9/  20] time: 76.8378, loss: 0.00015794\n",
      "Epoch: [21] [  10/  20] time: 76.9937, loss: 0.00779214\n",
      "Epoch: [21] [  11/  20] time: 77.1436, loss: 0.01058560\n",
      "Epoch: [21] [  12/  20] time: 77.3045, loss: 0.00507535\n",
      "Epoch: [21] [  13/  20] time: 77.4538, loss: 0.02390441\n",
      "Epoch: [21] [  14/  20] time: 77.6097, loss: 0.00045581\n",
      "Epoch: [21] [  15/  20] time: 77.7632, loss: 0.08026939\n",
      "Epoch: [21] [  16/  20] time: 77.9203, loss: 0.00019959\n",
      "Epoch: [21] [  17/  20] time: 78.0779, loss: 0.00111660\n",
      "Epoch: [21] [  18/  20] time: 78.2360, loss: 0.00191842\n",
      "Epoch: [21] [  19/  20] time: 78.3864, loss: 0.00001598\n",
      "[21/30] - ptime: 3.3211 loss: 0.00827007 acc: 0.79000 lr: 0.00080151\n",
      "Epoch: [22] [   0/  20] time: 79.1021, loss: 0.03121552\n",
      "Epoch: [22] [   1/  20] time: 79.2657, loss: 0.00179819\n",
      "Epoch: [22] [   2/  20] time: 79.4252, loss: 0.00585501\n",
      "Epoch: [22] [   3/  20] time: 79.5762, loss: 0.00220477\n",
      "Epoch: [22] [   4/  20] time: 79.7342, loss: 0.00625120\n",
      "Epoch: [22] [   5/  20] time: 79.8879, loss: 0.00250201\n",
      "Epoch: [22] [   6/  20] time: 80.0435, loss: 0.00084831\n",
      "Epoch: [22] [   7/  20] time: 80.1951, loss: 0.00631317\n",
      "Epoch: [22] [   8/  20] time: 80.3509, loss: 0.00203598\n",
      "Epoch: [22] [   9/  20] time: 80.5049, loss: 0.01878733\n",
      "Epoch: [22] [  10/  20] time: 80.6623, loss: 0.02956021\n",
      "Epoch: [22] [  11/  20] time: 80.8135, loss: 0.00773257\n",
      "Epoch: [22] [  12/  20] time: 80.9706, loss: 0.00069391\n",
      "Epoch: [22] [  13/  20] time: 81.1258, loss: 0.10870536\n",
      "Epoch: [22] [  14/  20] time: 81.2828, loss: 0.02278754\n",
      "Epoch: [22] [  15/  20] time: 81.4364, loss: 0.02677242\n",
      "Epoch: [22] [  16/  20] time: 81.5927, loss: 0.00315200\n",
      "Epoch: [22] [  17/  20] time: 81.7484, loss: 0.01159198\n",
      "Epoch: [22] [  18/  20] time: 81.9039, loss: 0.01395140\n",
      "Epoch: [22] [  19/  20] time: 82.0546, loss: 0.00811422\n",
      "[22/30] - ptime: 3.3036 loss: 0.01554365 acc: 0.77000 lr: 0.00079311\n",
      "Epoch: [23] [   0/  20] time: 82.7937, loss: 0.02006294\n",
      "Epoch: [23] [   1/  20] time: 82.9450, loss: 0.00393605\n",
      "Epoch: [23] [   2/  20] time: 83.1016, loss: 0.00292884\n",
      "Epoch: [23] [   3/  20] time: 83.2515, loss: 0.00497133\n",
      "Epoch: [23] [   4/  20] time: 83.4123, loss: 0.00370513\n",
      "Epoch: [23] [   5/  20] time: 83.5661, loss: 0.02200558\n",
      "Epoch: [23] [   6/  20] time: 83.7228, loss: 0.00024157\n",
      "Epoch: [23] [   7/  20] time: 83.8767, loss: 0.01174750\n",
      "Epoch: [23] [   8/  20] time: 84.0321, loss: 0.00048852\n",
      "Epoch: [23] [   9/  20] time: 84.1875, loss: 0.00859106\n",
      "Epoch: [23] [  10/  20] time: 84.3436, loss: 0.01103543\n",
      "Epoch: [23] [  11/  20] time: 84.4929, loss: 0.00051811\n",
      "Epoch: [23] [  12/  20] time: 84.6518, loss: 0.01930165\n",
      "Epoch: [23] [  13/  20] time: 84.8017, loss: 0.00765397\n",
      "Epoch: [23] [  14/  20] time: 84.9564, loss: 0.00294175\n",
      "Epoch: [23] [  15/  20] time: 85.1107, loss: 0.00026603\n",
      "Epoch: [23] [  16/  20] time: 85.2662, loss: 0.00407434\n",
      "Epoch: [23] [  17/  20] time: 85.4225, loss: 0.00007444\n",
      "Epoch: [23] [  18/  20] time: 85.5792, loss: 0.00054373\n",
      "Epoch: [23] [  19/  20] time: 85.7296, loss: 0.01294441\n",
      "[23/30] - ptime: 3.2948 loss: 0.00690162 acc: 0.77000 lr: 0.00078480\n",
      "Epoch: [24] [   0/  20] time: 86.4608, loss: 0.01066823\n",
      "Epoch: [24] [   1/  20] time: 86.6236, loss: 0.00071899\n",
      "Epoch: [24] [   2/  20] time: 86.7830, loss: 0.00398010\n",
      "Epoch: [24] [   3/  20] time: 86.9324, loss: 0.00269356\n",
      "Epoch: [24] [   4/  20] time: 87.0892, loss: 0.00572700\n",
      "Epoch: [24] [   5/  20] time: 87.2424, loss: 0.00733762\n",
      "Epoch: [24] [   6/  20] time: 87.3991, loss: 0.00138289\n",
      "Epoch: [24] [   7/  20] time: 87.5526, loss: 0.00058562\n",
      "Epoch: [24] [   8/  20] time: 87.7112, loss: 0.00341935\n",
      "Epoch: [24] [   9/  20] time: 87.8659, loss: 0.00141701\n",
      "Epoch: [24] [  10/  20] time: 88.0208, loss: 0.00821865\n",
      "Epoch: [24] [  11/  20] time: 88.1706, loss: 0.00061515\n",
      "Epoch: [24] [  12/  20] time: 88.3298, loss: 0.00011812\n",
      "Epoch: [24] [  13/  20] time: 88.4790, loss: 0.00055281\n",
      "Epoch: [24] [  14/  20] time: 88.6353, loss: 0.01853669\n",
      "Epoch: [24] [  15/  20] time: 88.7918, loss: 0.00100952\n",
      "Epoch: [24] [  16/  20] time: 88.9471, loss: 0.00080198\n",
      "Epoch: [24] [  17/  20] time: 89.0993, loss: 0.02350518\n",
      "Epoch: [24] [  18/  20] time: 89.2554, loss: 0.01385089\n",
      "Epoch: [24] [  19/  20] time: 89.4060, loss: 0.00255504\n",
      "[24/30] - ptime: 3.2987 loss: 0.00538472 acc: 0.74000 lr: 0.00077657\n",
      "Epoch: [25] [   0/  20] time: 90.1539, loss: 0.02412163\n",
      "Epoch: [25] [   1/  20] time: 90.3110, loss: 0.01169954\n",
      "Epoch: [25] [   2/  20] time: 90.4686, loss: 0.00017830\n",
      "Epoch: [25] [   3/  20] time: 90.6226, loss: 0.00817491\n",
      "Epoch: [25] [   4/  20] time: 90.7797, loss: 0.00417365\n",
      "Epoch: [25] [   5/  20] time: 90.9332, loss: 0.00172606\n",
      "Epoch: [25] [   6/  20] time: 91.0911, loss: 0.00244936\n",
      "Epoch: [25] [   7/  20] time: 91.2472, loss: 0.00710052\n",
      "Epoch: [25] [   8/  20] time: 91.4037, loss: 0.00543008\n",
      "Epoch: [25] [   9/  20] time: 91.5535, loss: 0.05293974\n",
      "Epoch: [25] [  10/  20] time: 91.7122, loss: 0.00827246\n",
      "Epoch: [25] [  11/  20] time: 91.8626, loss: 0.00076979\n",
      "Epoch: [25] [  12/  20] time: 92.0217, loss: 0.00479797\n",
      "Epoch: [25] [  13/  20] time: 92.1753, loss: 0.00266704\n",
      "Epoch: [25] [  14/  20] time: 92.3316, loss: 0.00067719\n",
      "Epoch: [25] [  15/  20] time: 92.4866, loss: 0.00082135\n",
      "Epoch: [25] [  16/  20] time: 92.6426, loss: 0.01607629\n",
      "Epoch: [25] [  17/  20] time: 92.7925, loss: 0.00163505\n",
      "Epoch: [25] [  18/  20] time: 92.9538, loss: 0.00144685\n",
      "Epoch: [25] [  19/  20] time: 93.1074, loss: 0.02256387\n",
      "[25/30] - ptime: 3.3340 loss: 0.00888608 acc: 0.76000 lr: 0.00076843\n",
      "Epoch: [26] [   0/  20] time: 93.8967, loss: 0.00068975\n",
      "Epoch: [26] [   1/  20] time: 94.0578, loss: 0.00554460\n",
      "Epoch: [26] [   2/  20] time: 94.2194, loss: 0.00544464\n",
      "Epoch: [26] [   3/  20] time: 94.3707, loss: 0.01713505\n",
      "Epoch: [26] [   4/  20] time: 94.5275, loss: 0.00625008\n",
      "Epoch: [26] [   5/  20] time: 94.6818, loss: 0.00978269\n",
      "Epoch: [26] [   6/  20] time: 94.8408, loss: 0.00024766\n",
      "Epoch: [26] [   7/  20] time: 94.9965, loss: 0.00348346\n",
      "Epoch: [26] [   8/  20] time: 95.1519, loss: 0.00099993\n",
      "Epoch: [26] [   9/  20] time: 95.3029, loss: 0.00099380\n",
      "Epoch: [26] [  10/  20] time: 95.4599, loss: 0.00771314\n",
      "Epoch: [26] [  11/  20] time: 95.6110, loss: 0.00018662\n",
      "Epoch: [26] [  12/  20] time: 95.7681, loss: 0.00120076\n",
      "Epoch: [26] [  13/  20] time: 95.9213, loss: 0.00032941\n",
      "Epoch: [26] [  14/  20] time: 96.0771, loss: 0.00022003\n",
      "Epoch: [26] [  15/  20] time: 96.2322, loss: 0.00220565\n",
      "Epoch: [26] [  16/  20] time: 96.3886, loss: 0.00073124\n",
      "Epoch: [26] [  17/  20] time: 96.5388, loss: 0.00233778\n",
      "Epoch: [26] [  18/  20] time: 96.6975, loss: 0.00782196\n",
      "Epoch: [26] [  19/  20] time: 96.8471, loss: 0.00016308\n",
      "[26/30] - ptime: 3.3493 loss: 0.00367407 acc: 0.72000 lr: 0.00076038\n",
      "Epoch: [27] [   0/  20] time: 97.6310, loss: 0.00171646\n",
      "Epoch: [27] [   1/  20] time: 97.7921, loss: 0.00016022\n",
      "Epoch: [27] [   2/  20] time: 97.9526, loss: 0.00135574\n",
      "Epoch: [27] [   3/  20] time: 98.1043, loss: 0.02514297\n",
      "Epoch: [27] [   4/  20] time: 98.2608, loss: 0.00431087\n",
      "Epoch: [27] [   5/  20] time: 98.4159, loss: 0.00195719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [27] [   6/  20] time: 98.5734, loss: 0.00089693\n",
      "Epoch: [27] [   7/  20] time: 98.7298, loss: 0.00468132\n",
      "Epoch: [27] [   8/  20] time: 98.8867, loss: 0.00151335\n",
      "Epoch: [27] [   9/  20] time: 99.0363, loss: 0.00005929\n",
      "Epoch: [27] [  10/  20] time: 99.1952, loss: 0.00167537\n",
      "Epoch: [27] [  11/  20] time: 99.3470, loss: 0.00081660\n",
      "Epoch: [27] [  12/  20] time: 99.5043, loss: 0.00002220\n",
      "Epoch: [27] [  13/  20] time: 99.6596, loss: 0.00942311\n",
      "Epoch: [27] [  14/  20] time: 99.8157, loss: 0.04214340\n",
      "Epoch: [27] [  15/  20] time: 99.9720, loss: 0.00769900\n",
      "Epoch: [27] [  16/  20] time: 100.1275, loss: 0.03009879\n",
      "Epoch: [27] [  17/  20] time: 100.2778, loss: 0.00904062\n",
      "Epoch: [27] [  18/  20] time: 100.4378, loss: 0.00112682\n",
      "Epoch: [27] [  19/  20] time: 100.5873, loss: 0.00861149\n",
      "[27/30] - ptime: 3.3473 loss: 0.00762259 acc: 0.71000 lr: 0.00075241\n",
      "Epoch: [28] [   0/  20] time: 101.4007, loss: 0.00009169\n",
      "Epoch: [28] [   1/  20] time: 101.5493, loss: 0.00001556\n",
      "Epoch: [28] [   2/  20] time: 101.7084, loss: 0.00012906\n",
      "Epoch: [28] [   3/  20] time: 101.8579, loss: 0.00468507\n",
      "Epoch: [28] [   4/  20] time: 102.0132, loss: 0.01022730\n",
      "Epoch: [28] [   5/  20] time: 102.1664, loss: 0.00012439\n",
      "Epoch: [28] [   6/  20] time: 102.3222, loss: 0.00011977\n",
      "Epoch: [28] [   7/  20] time: 102.4781, loss: 0.00035541\n",
      "Epoch: [28] [   8/  20] time: 102.6341, loss: 0.00110590\n",
      "Epoch: [28] [   9/  20] time: 102.7849, loss: 0.00254760\n",
      "Epoch: [28] [  10/  20] time: 102.9431, loss: 0.00013296\n",
      "Epoch: [28] [  11/  20] time: 103.0927, loss: 0.00742298\n",
      "Epoch: [28] [  12/  20] time: 103.2478, loss: 0.00015651\n",
      "Epoch: [28] [  13/  20] time: 103.4008, loss: 0.00094220\n",
      "Epoch: [28] [  14/  20] time: 103.5565, loss: 0.00374884\n",
      "Epoch: [28] [  15/  20] time: 103.7128, loss: 0.00599660\n",
      "Epoch: [28] [  16/  20] time: 103.8691, loss: 0.00303251\n",
      "Epoch: [28] [  17/  20] time: 104.0190, loss: 0.00726342\n",
      "Epoch: [28] [  18/  20] time: 104.1774, loss: 0.00577733\n",
      "Epoch: [28] [  19/  20] time: 104.3285, loss: 0.01123316\n",
      "[28/30] - ptime: 3.3346 loss: 0.00325541 acc: 0.72000 lr: 0.00074452\n",
      "Epoch: [29] [   0/  20] time: 105.1305, loss: 0.00256142\n",
      "Epoch: [29] [   1/  20] time: 105.2928, loss: 0.00043277\n",
      "Epoch: [29] [   2/  20] time: 105.4539, loss: 0.00397707\n",
      "Epoch: [29] [   3/  20] time: 105.6055, loss: 0.00812306\n",
      "Epoch: [29] [   4/  20] time: 105.7625, loss: 0.05395769\n",
      "Epoch: [29] [   5/  20] time: 105.9161, loss: 0.00056615\n",
      "Epoch: [29] [   6/  20] time: 106.0718, loss: 0.00051041\n",
      "Epoch: [29] [   7/  20] time: 106.2274, loss: 0.00652910\n",
      "Epoch: [29] [   8/  20] time: 106.3858, loss: 0.02387300\n",
      "Epoch: [29] [   9/  20] time: 106.5354, loss: 0.01876992\n",
      "Epoch: [29] [  10/  20] time: 106.6928, loss: 0.01742361\n",
      "Epoch: [29] [  11/  20] time: 106.8421, loss: 0.06436991\n",
      "Epoch: [29] [  12/  20] time: 106.9983, loss: 0.00024851\n",
      "Epoch: [29] [  13/  20] time: 107.1525, loss: 0.00084712\n",
      "Epoch: [29] [  14/  20] time: 107.3085, loss: 0.00005160\n",
      "Epoch: [29] [  15/  20] time: 107.4637, loss: 0.00210420\n",
      "Epoch: [29] [  16/  20] time: 107.6195, loss: 0.00347719\n",
      "Epoch: [29] [  17/  20] time: 107.7694, loss: 0.00002959\n",
      "Epoch: [29] [  18/  20] time: 107.9263, loss: 0.03688326\n",
      "Epoch: [29] [  19/  20] time: 108.0772, loss: 0.00100898\n",
      "[29/30] - ptime: 3.3593 loss: 0.01228723 acc: 0.72000 lr: 0.00073672\n",
      "Epoch: [30] [   0/  20] time: 108.8841, loss: 0.02095261\n",
      "Epoch: [30] [   1/  20] time: 109.0455, loss: 0.00632322\n",
      "Epoch: [30] [   2/  20] time: 109.2037, loss: 0.00003500\n",
      "Epoch: [30] [   3/  20] time: 109.3549, loss: 0.01076480\n",
      "Epoch: [30] [   4/  20] time: 109.5123, loss: 0.00190429\n",
      "Epoch: [30] [   5/  20] time: 109.6674, loss: 0.00674526\n",
      "Epoch: [30] [   6/  20] time: 109.8237, loss: 0.07024588\n",
      "Epoch: [30] [   7/  20] time: 109.9794, loss: 0.00077297\n",
      "Epoch: [30] [   8/  20] time: 110.1352, loss: 0.02043623\n",
      "Epoch: [30] [   9/  20] time: 110.2862, loss: 0.00196157\n",
      "Epoch: [30] [  10/  20] time: 110.4465, loss: 0.00008866\n",
      "Epoch: [30] [  11/  20] time: 110.5972, loss: 0.00166000\n",
      "Epoch: [30] [  12/  20] time: 110.7536, loss: 0.02430085\n",
      "Epoch: [30] [  13/  20] time: 110.9134, loss: 0.00004590\n",
      "Epoch: [30] [  14/  20] time: 111.0747, loss: 0.02247093\n",
      "Epoch: [30] [  15/  20] time: 111.2379, loss: 0.00014781\n",
      "Epoch: [30] [  16/  20] time: 111.4017, loss: 0.00158251\n",
      "Epoch: [30] [  17/  20] time: 111.5624, loss: 0.00927271\n",
      "Epoch: [30] [  18/  20] time: 111.7242, loss: 0.00260777\n",
      "Epoch: [30] [  19/  20] time: 111.8854, loss: 0.01761328\n",
      "[30/30] - ptime: 3.4001 loss: 0.01099661 acc: 0.78000 lr: 0.00072900\n",
      "Avg per epoch ptime: 3.35, total 30 epochs ptime: 112.33\n",
      " [*] Training finished!\n",
      " [*] Best Epoch:  13 , Accuracy:  0.8100000023841858\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/CNN_C3_D1_Kernel(3,3)_128_lrdecay0.0001/CNN_C3_D1_Kernel(3,3)_128_lrdecay0.0001-13\n",
      " [*] Finished testing Best Epoch: 13 , accuracy:  0.809999942779541 !\n"
     ]
    }
   ],
   "source": [
    "gan_type = 'CNN'\n",
    "dataset = '4_Flowers'\n",
    "epoch = 30\n",
    "batch_size = 100\n",
    "checkpoint_dir = 'checkpoint'\n",
    "log_dir = 'logs'\n",
    "trainhist_dir = 'trainhist'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "# --log_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# --trainhist_dir\n",
    "if not os.path.exists(trainhist_dir):\n",
    "    os.makedirs(trainhist_dir)\n",
    "\n",
    "# open session\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    \n",
    "    # declare instance for GAN\n",
    "    CNN = CNN(sess, epoch=epoch, batch_size=batch_size, dataset_name=dataset, checkpoint_dir=checkpoint_dir, \n",
    "                log_dir=log_dir, trainhist_dir=trainhist_dir)\n",
    "\n",
    "    # build graph\n",
    "    CNN.build_model()\n",
    "\n",
    "    # show network architecture\n",
    "    CNN.show_all_variables()\n",
    "\n",
    "    # launch the graph in a session\n",
    "    CNN.train()\n",
    "    \n",
    "    #CNN.test(epoch)\n",
    "        \n",
    "sess.close()\n",
    "        \n",
    "\n",
    "# lrdecay\n",
    "# Avg per epoch ptime: 3.21, total 100 epochs ptime: 359.54\n",
    "#  [*] Training finished!\n",
    "#  [*] Best Epoch:  15 , Accuracy:  0.8199999928474426\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_C5_D1_Kernel(3,3)_128_lrdecay/CNN_C5_D1_Kernel(3,3)_128_lrdecay-15\n",
    "#  [*] Finished testing Best Epoch: 15 , accuracy:  0.8199999928474426 !\n",
    "\n",
    "# Avg per epoch ptime: 3.30, total 30 epochs ptime: 110.90\n",
    "#  [*] Training finished!\n",
    "#  [*] Best Epoch:  15 , Accuracy:  0.8299999833106995\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_C4_D1_Kernel(3,3)_128_lrdecay/CNN_C4_D1_Kernel(3,3)_128_lrdecay-15\n",
    "#  [*] Finished testing Best Epoch: 15 , accuracy:  0.8300000429153442 !\n",
    "\n",
    "# Avg per epoch ptime: 3.35, total 30 epochs ptime: 112.33\n",
    "#  [*] Training finished!\n",
    "#  [*] Best Epoch:  13 , Accuracy:  0.8100000023841858\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_C3_D1_Kernel(3,3)_128_lrdecay0.0001/CNN_C3_D1_Kernel(3,3)_128_lrdecay0.0001-13\n",
    "#  [*] Finished testing Best Epoch: 13 , accuracy:  0.809999942779541 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
