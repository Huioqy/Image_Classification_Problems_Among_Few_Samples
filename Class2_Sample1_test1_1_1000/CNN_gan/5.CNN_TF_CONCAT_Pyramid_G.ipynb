{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imutils import paths\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import argparse\n",
    "import imutils,sklearn\n",
    "import os, cv2, re, random, shutil, imageio, pickle\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyramid\n",
    "def batch_pyramid(images):\n",
    "    pyramid_list = []\n",
    "    for i in tqdm(range(len(images))):\n",
    "        img = (images.astype(np.float32)* 255)[i, :, :, :].astype(np.uint8)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        G = img.copy()\n",
    "        gp = [G]\n",
    "        for i in range(4):\n",
    "            G = cv2.pyrDown(G)\n",
    "            gp.append(G)\n",
    "        gp_out = [cv2.resize(gp[2], (128,128)),cv2.resize(gp[3], (128,128))]\n",
    "        pyramid_list.append((np.array(gp_out)).astype(np.uint8))\n",
    "    pyramid_list = np.array(pyramid_list,dtype = float32).reshape([images.shape[0],images.shape[1],images.shape[2],2])\n",
    "    \n",
    "    print (pyramid_list.shape)\n",
    "    return pyramid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def atoi(text):\n",
    "#     return int(text) if text.isdigit() else text\n",
    "\n",
    "# def natural_keys(text):\n",
    "#     return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# def load_flower_data():\n",
    "#     # grab the list of images that we'll be describing\n",
    "#     print(\"[INFO] handling images...\")\n",
    "#     TRAIN_ORIGINAL_DIR = '../train/'\n",
    "#     TRAIN_SUB_DIR = '../subsample/'\n",
    "#     TRAIN_GAN = '../../image_gan/'\n",
    "#     TEST_DIR = '../../test/'\n",
    "\n",
    "#     # use this for full dataset\n",
    "#     train_images_gan = [TRAIN_GAN + i for i in os.listdir(TRAIN_GAN)]\n",
    "#     test_images = [TEST_DIR + i for i in os.listdir(TEST_DIR)]\n",
    "    \n",
    "#     train_images = train_images_gan\n",
    "    \n",
    "#     train_images.sort(key=natural_keys)\n",
    "#     test_images.sort(key=natural_keys)\n",
    "\n",
    "#     # initialize the features matrix and labels list\n",
    "#     trainImage = []\n",
    "#     trainLabels = []\n",
    "#     testImage = []\n",
    "#     testLabels = []\n",
    "\n",
    "#     # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(train_images):\n",
    "#         # extract the class label\n",
    "#         # get the labels from the name of the images by extract the string before \"_\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # read and resize image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         trainImage.append(img)\n",
    "#         trainLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(train_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(train_images)))\n",
    "            \n",
    "#       # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(test_images):\n",
    "#         # extract the class label\n",
    "#         # our images were named as labels.image_number.format\n",
    "#         # get the labels from the name of the images by extract the string before \".\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # extract CNN features in the image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         testImage.append(img)\n",
    "#         testLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(test_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(test_images)))\n",
    "\n",
    "\n",
    "#     trainImage = np.array(trainImage,dtype = float32)\n",
    "#     trainLabels = np.array(trainLabels)\n",
    "#     testImage = np.array(testImage,dtype = float32)\n",
    "#     testLabels = np.array(testLabels)\n",
    "#     print (trainImage.shape)\n",
    "    \n",
    "#     trainImage = trainImage.astype(np.float32) / 255\n",
    "#     testImage = testImage.astype(np.float32) / 255\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(trainLabels)\n",
    "#     list(le.classes_)\n",
    "#     trainLabels = le.transform(trainLabels) \n",
    "#     testLabels = le.transform(testLabels) \n",
    "    \n",
    "#     return trainImage, trainLabels, testImage, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] handling images...\n",
      "[INFO] processed 1000/2000\n",
      "[INFO] processed 2000/2000\n",
      "[INFO] processed 158/158\n",
      "(2000, 128, 128, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:42<00:00,  4.97it/s]\n",
      "  8%|▊         | 13/158 [00:00<00:01, 124.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 128, 128, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [00:01<00:00, 128.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158, 128, 128, 2)\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "(158, 2)\n",
      "[INFO] trainImage matrix: 384.00MB\n",
      "[INFO] trainLabels matrix: 0.0312MB\n",
      "[INFO] testImage matrix: 30.34MB\n",
      "[INFO] testLabels matrix: 0.0025MB\n",
      "[INFO] trainImage_pyramid matrix: 256.00MB\n",
      "[INFO] testImage_pyramid matrix: 20.2240MB\n"
     ]
    }
   ],
   "source": [
    "# trainImage, trainLabels, testImage, testLabels = load_flower_data()\n",
    "\n",
    "# trainImage_pyramid = batch_pyramid(trainImage)\n",
    "# testImage_pyramid = batch_pyramid(testImage)\n",
    "# nb_classes = 2\n",
    "\n",
    "# # Convert class vectors to binary class matrices.\n",
    "# trainLabels = keras.utils.to_categorical(trainLabels, nb_classes)\n",
    "# print (trainLabels)\n",
    "# testLabels = keras.utils.to_categorical(testLabels, nb_classes)\n",
    "# print (testLabels)\n",
    "# print (testLabels.shape)\n",
    "\n",
    "# np.save('../trainImage.npy', trainImage)\n",
    "# np.save('../trainLabels.npy', trainLabels)\n",
    "# np.save('../testImage.npy', testImage)\n",
    "# np.save('../testLabels.npy', testLabels)\n",
    "# np.save('../trainImage_pyramid_G.npy', trainImage_pyramid)\n",
    "# np.save('../testImage_pyramid_G.npy', testImage_pyramid)\n",
    "\n",
    "# print(\"[INFO] trainImage matrix: {:.2f}MB\".format(\n",
    "#     (trainImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainLabels matrix: {:.4f}MB\".format(\n",
    "#     (trainLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage matrix: {:.2f}MB\".format(\n",
    "#     (testImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testLabels matrix: {:.4f}MB\".format(\n",
    "#     (testLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainImage_pyramid matrix: {:.2f}MB\".format(\n",
    "#     (trainImage_pyramid.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage_pyramid matrix: {:.4f}MB\".format(\n",
    "#     (testImage_pyramid.nbytes) / (1024 * 1000.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self, sess, epoch, batch_size, dataset_name, checkpoint_dir, log_dir, trainhist_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.trainhist_dir = trainhist_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.classname = ['Iris', 'Pansy']\n",
    "\n",
    "        # parameters\n",
    "        self.input_height = 128\n",
    "        self.input_width = 128\n",
    "        self.c_dim = 3  # color dimension\n",
    "        self.pyramid_dim = 2  # color dimension\n",
    "        self.nb_class = 2\n",
    "        \n",
    "        # number of convolutional filters to use  \n",
    "        self.nb_CNN = [32, 64, 64, 64, 128]  \n",
    "        # number of dense filters to use  \n",
    "        self.nb_Dense = [256] \n",
    "        # size of pooling area for max pooling  \n",
    "        self.pool_size = (2, 2)  \n",
    "        # convolution kernel size  \n",
    "        self.kernel_size = (3, 3)\n",
    "        self.batch_normalization_control = True\n",
    "        \n",
    "        # name for checkpoint\n",
    "        self.model_name = 'CNN_Pyramid_G_C%d_D%d_Kernel(%d,%d)_%d_lrdecay' % (len(self.nb_CNN), len(self.nb_Dense),\n",
    "                                                          self.kernel_size[0], self.kernel_size[1], max(self.nb_CNN))\n",
    "\n",
    "        # train\n",
    "        #设置一个全局的计数器\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(0.001, \n",
    "                                             global_step=self.global_step, \n",
    "                                             decay_steps=10, \n",
    "                                             decay_rate=0.9, \n",
    "                                             staircase=True)\n",
    "        self.beta1 = 0.5\n",
    "        #max model to keep saving\n",
    "        self.max_to_keep = 300\n",
    "        \n",
    "        # test\n",
    "\n",
    "        #load_flower_data\n",
    "        self.train_x = np.load('../trainImage.npy')\n",
    "        self.train_y = np.load('../trainLabels.npy')\n",
    "        self.test_x = np.load('../testImage.npy')\n",
    "        self.test_y = np.load('../testLabels.npy')\n",
    "        self.train_x_pyramid = np.load('../trainImage_pyramid_L.npy')\n",
    "        self.test_x_pyramid = np.load('../testImage_pyramid_L.npy')\n",
    "        \n",
    "        #记录\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['losses'] = []\n",
    "        self.train_hist['accuracy'] = []\n",
    "        self.train_hist['learning_rate'] = []\n",
    "        self.train_hist['per_epoch_ptimes'] = []\n",
    "        self.train_hist['total_ptime'] = []\n",
    "        \n",
    "        # get number of batches for a single epoch\n",
    "        self.num_batches_train = len(self.train_x) // self.batch_size\n",
    "        self.num_batches_test= len(self.test_x) // self.batch_size\n",
    "\n",
    "    def cnn_model(self, x, x_pyramid, keep_prob, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(\"cnn\", reuse=reuse):\n",
    "             \n",
    "            #初始化参数\n",
    "            W = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "            B = tf.constant_initializer(0.0)\n",
    "        \n",
    "            print(\"CNN:x\",x.get_shape()) # 128, 128, 3 \n",
    "            print(\"CNN:x_pyramid\",x_pyramid.get_shape()) # 128, 128, 3 \n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_1 = tf.layers.conv2d(inputs = x,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_1'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_1.get_shape())\n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_2 = tf.layers.conv2d(inputs = x_pyramid,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_2'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_2.get_shape())\n",
    "\n",
    "            #把数据和边缘进行连接\n",
    "            net = tf.concat([net1_1, net1_2], 3)\n",
    "            net = tf.layers.batch_normalization(net, training=is_training)\n",
    "            net = tf.nn.relu(net, name = 'relu_conv_1')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_1'\n",
    "                                             )\n",
    "            \n",
    "            for i in range(2,len(self.nb_CNN)+1):\n",
    "                net = tf.layers.conv2d(inputs = net,                 # 输入,\n",
    "                                       filters = self.nb_CNN[i-1],      # 卷积核个数,\n",
    "                                       kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                       strides = (1, 1),\n",
    "                                       padding = 'same',              # padding方法\n",
    "                                       kernel_initializer = W,\n",
    "                                       bias_initializer = B,\n",
    "                                       kernel_regularizer = None,\n",
    "                                       bias_regularizer = None,\n",
    "                                       activity_regularizer = None,\n",
    "                                       name = 'conv_'+ str(i)        # 命名用于获取变量\n",
    "                                       )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "                if self.batch_normalization_control:\n",
    "                    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_conv_' + str(i))\n",
    "                net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_' + str(i)\n",
    "                                             )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #flatten\n",
    "            net = tf.reshape(net, [-1, int(net.get_shape()[1]*net.get_shape()[2]*net.get_shape()[3])],name='flatten')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #dense layer\n",
    "            for i in range(1,len(self.nb_Dense)+1):\n",
    "                net = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_Dense[i-1],\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_' + str(i)\n",
    "                                    )\n",
    "#                 net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_dense_' + str(i))\n",
    "                net = tf.layers.dropout(inputs = net,\n",
    "                                        rate=keep_prob,\n",
    "                                        noise_shape=None,\n",
    "                                        seed=None,\n",
    "                                        training = is_training,\n",
    "                                        name= 'dropout_dense_' + str(i)\n",
    "                                        )\n",
    "            #output\n",
    "            logit = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_class,\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_output'\n",
    "                                    )\n",
    "            out_logit = tf.nn.softmax(logit, name=\"softmax\")\n",
    "            print(\"CNN:out_logit\",out_logit.get_shape())\n",
    "            print(\"------------------------\")    \n",
    "\n",
    "            return out_logit, logit\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.x = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.c_dim], \n",
    "                                name='x_image')\n",
    "        \n",
    "        self.x_pyramid = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.pyramid_dim], \n",
    "                                name='x_pyramid')\n",
    "\n",
    "        self.y = tf.placeholder(tf.float32, shape=[self.batch_size, self.nb_class], name='y_label')\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.add_global = self.global_step.assign_add(1)\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "\n",
    "        # output of cnn_model\n",
    "        self.out_logit, self.logit = self.cnn_model(self.x, self.x_pyramid, self.keep_prob, is_training=True, reuse=False)\n",
    "        \n",
    "        self.loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y,\n",
    "                                                                                         logits =self.logit))\n",
    "        \n",
    "        \"\"\" Training \"\"\"\n",
    "        # trainable variables into a group\n",
    "        tf_vars = tf.trainable_variables()\n",
    "        cnn_vars = [var for var in tf_vars if var.name.startswith('cnn')]\n",
    "\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.cnn_optim = tf.train.AdamOptimizer(self.lr, beta1=self.beta1).minimize(self.loss_cross_entropy,\n",
    "                                                                                        var_list=cnn_vars)\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        # output of cnn_model\n",
    "        self.out_logit_test, self.logit_test = self.cnn_model(self.x, self.x_pyramid, self.keep_prob, is_training=False, reuse=True)\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit_test, 1), tf.argmax(self.y, 1))\n",
    "        self.predict = tf.argmax(self.logit_test, 1)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        self.loss_sum = tf.summary.scalar(\"loss\", self.loss_cross_entropy)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver(max_to_keep = self.max_to_keep)\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_epoch = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_epoch) + 1\n",
    "            counter = 1\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name+'.pkl', 'rb') \n",
    "            self.train_hist = pickle.load(f)\n",
    "            f.close()\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "            print(\" [!] START_EPOCH is \", start_epoch)\n",
    "        else:\n",
    "            start_epoch = 1\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        for epoch_loop in range(start_epoch, self.epoch + 1):\n",
    "\n",
    "            CNN_losses = []\n",
    "  \n",
    "            epoch_start_time = time.time()\n",
    "            shuffle_idxs = random.sample(range(0, self.train_x.shape[0]), self.train_x.shape[0])\n",
    "            shuffled_set = self.train_x[shuffle_idxs]\n",
    "            shuffled_set_pyramid = self.train_x_pyramid[shuffle_idxs]\n",
    "            shuffled_label = self.train_y[shuffle_idxs]\n",
    "    \n",
    "            # get batch data\n",
    "            for idx in range(self.num_batches_train):\n",
    "                batch_x = shuffled_set[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_x_pyramid= shuffled_set_pyramid[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_y = shuffled_label[idx*self.batch_size:(idx+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                \n",
    "\n",
    "                # update D network\n",
    "                _, summary_str, cnn_loss = self.sess.run([self.cnn_optim, self.loss_sum, self.loss_cross_entropy],\n",
    "                                               feed_dict={self.x: batch_x,\n",
    "                                                          self.x_pyramid: batch_x_pyramid,\n",
    "                                                          self.y: batch_y,\n",
    "                                                          self.keep_prob: 0.5}\n",
    "                                                      )\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                CNN_losses.append(cnn_loss)\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f\" % (epoch_loop, idx, self.num_batches_train, \n",
    "                                                                          time.time() - start_time, cnn_loss))\n",
    "\n",
    "            # After an epoch\n",
    "            # Evaluates accuracy on test set\n",
    "            test_accuracy_list = []\n",
    "            for idx_test in range(self.num_batches_test):\n",
    "                batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "                batch_x_pyramid_test =self.test_x_pyramid[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "                batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                    self.x_pyramid: batch_x_pyramid_test,\n",
    "                                                                    self.y: batch_y_tes,\n",
    "                                                                    self.keep_prob: 1.0})\n",
    "                test_accuracy_list.append(accuracy)\n",
    "            test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        \n",
    "            #update learning rate\n",
    "            _, rate = sess.run([self.add_global, self.lr])\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            print('[%d/%d] - ptime: %.4f loss: %.8f acc: %.5f lr: %.8f'% (epoch_loop, self.epoch, per_epoch_ptime, \n",
    "                                                                    np.mean(CNN_losses), test_accuracy, rate))\n",
    "            \n",
    "            self.train_hist['losses'].append(np.mean(CNN_losses))\n",
    "            self.train_hist['accuracy'].append( test_accuracy)\n",
    "            self.train_hist['learning_rate'].append(rate)\n",
    "            self.train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "            \n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, epoch_loop)\n",
    "            \n",
    "            # save trainhist for train\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name + '.pkl', 'wb') \n",
    "            pickle.dump(self.train_hist, f)\n",
    "            f.close()\n",
    "            self.show_train_hist(self.train_hist, save=True, path= self.trainhist_dir + '/' \n",
    "                                 + self.model_name + '.png')\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_ptime = end_time - start_time\n",
    "        self.train_hist['total_ptime'].append(total_ptime)\n",
    "        print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(self.train_hist['per_epoch_ptimes']), \n",
    "                                                                          self.epoch, total_ptime))\n",
    "        print(\" [*] Training finished!\")\n",
    "        \n",
    "        \"\"\"test after train\"\"\"\n",
    "        best_acc = max(self.train_hist['accuracy'])\n",
    "        beat_epoch = self.train_hist['accuracy'].index(best_acc) + 1\n",
    "        print (\" [*] Best Epoch: \", beat_epoch, \", Accuracy: \", best_acc)\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(beat_epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_pyramid_test =self.test_x_pyramid[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_pyramid: batch_x_pyramid_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Best Epoch:\", beat_epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "\n",
    "    def test(self, epoch):\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_pyramidtest =self.test_x_pyramid[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_pyramid: batch_x_pyramid_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Epoch:\", epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "        \n",
    "    def show_all_variables(self):\n",
    "        model_vars = tf.trainable_variables()\n",
    "        tf.contrib.slim.model_analyzer.analyze_vars(model_vars, print_info=True) \n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            epoch = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read [{}], epoch [{}]\".format(ckpt_name,epoch))\n",
    "            return True, epoch\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "        \n",
    "    def show_train_hist(self, hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "        x = range(1, len(hist['losses'])+1)\n",
    "\n",
    "        y1 = hist['losses']\n",
    "        y2 = hist['accuracy']\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "                            \n",
    "        ax2 = ax1.twinx()  \n",
    "\n",
    "        ax1.plot(x, y1, 'b')\n",
    "        ax2.plot(x, y2, 'r')\n",
    "                            \n",
    "        ax1.set_xlabel('Epoch')\n",
    "                            \n",
    "        ax1.set_ylabel('CNN_loss')    \n",
    "        ax2.set_ylabel('accuracy')\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(path, dpi = 400)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_pyramid (100, 128, 128, 2)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_pyramid (100, 128, 128, 2)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "cnn/conv_1_1/kernel:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "cnn/conv_1_1/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/conv_1_2/kernel:0 (float32_ref 3x3x2x32) [576, bytes: 2304]\n",
      "cnn/conv_1_2/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/batch_normalization/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_2/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_2/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_3/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_3/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_4/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_4/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_5/kernel:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
      "cnn/conv_5/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/dense_1/kernel:0 (float32_ref 2048x256) [524288, bytes: 2097152]\n",
      "cnn/dense_1/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "cnn/dense_output/kernel:0 (float32_ref 256x2) [512, bytes: 2048]\n",
      "cnn/dense_output/bias:0 (float32_ref 2) [2, bytes: 8]\n",
      "Total size of variables: 711970\n",
      "Total bytes of variables: 2847880\n",
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load failed...\n",
      "Epoch: [ 1] [   0/  20] time: 1.7114, loss: 0.76322770\n",
      "Epoch: [ 1] [   1/  20] time: 1.9469, loss: 1.32000124\n",
      "Epoch: [ 1] [   2/  20] time: 2.1861, loss: 0.51695979\n",
      "Epoch: [ 1] [   3/  20] time: 2.4257, loss: 0.26732430\n",
      "Epoch: [ 1] [   4/  20] time: 2.6631, loss: 0.12811512\n",
      "Epoch: [ 1] [   5/  20] time: 2.9012, loss: 0.18108886\n",
      "Epoch: [ 1] [   6/  20] time: 3.1392, loss: 0.08304425\n",
      "Epoch: [ 1] [   7/  20] time: 3.3766, loss: 0.10561310\n",
      "Epoch: [ 1] [   8/  20] time: 3.6133, loss: 0.05032036\n",
      "Epoch: [ 1] [   9/  20] time: 3.8513, loss: 0.03456468\n",
      "Epoch: [ 1] [  10/  20] time: 4.0892, loss: 0.06740852\n",
      "Epoch: [ 1] [  11/  20] time: 4.3276, loss: 0.03757983\n",
      "Epoch: [ 1] [  12/  20] time: 4.5671, loss: 0.00812958\n",
      "Epoch: [ 1] [  13/  20] time: 4.8045, loss: 0.07724933\n",
      "Epoch: [ 1] [  14/  20] time: 5.0415, loss: 0.05549484\n",
      "Epoch: [ 1] [  15/  20] time: 5.2779, loss: 0.03990437\n",
      "Epoch: [ 1] [  16/  20] time: 5.5171, loss: 0.08602837\n",
      "Epoch: [ 1] [  17/  20] time: 5.7556, loss: 0.03013550\n",
      "Epoch: [ 1] [  18/  20] time: 5.9927, loss: 0.06374233\n",
      "Epoch: [ 1] [  19/  20] time: 6.2312, loss: 0.05624090\n",
      "[1/50] - ptime: 6.2969 loss: 0.19860865 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 2] [   0/  20] time: 7.1218, loss: 0.02940705\n",
      "Epoch: [ 2] [   1/  20] time: 7.3579, loss: 0.05769698\n",
      "Epoch: [ 2] [   2/  20] time: 7.5954, loss: 0.02308148\n",
      "Epoch: [ 2] [   3/  20] time: 7.8324, loss: 0.06106474\n",
      "Epoch: [ 2] [   4/  20] time: 8.0692, loss: 0.00640564\n",
      "Epoch: [ 2] [   5/  20] time: 8.3078, loss: 0.14088678\n",
      "Epoch: [ 2] [   6/  20] time: 8.5472, loss: 0.03483355\n",
      "Epoch: [ 2] [   7/  20] time: 8.7869, loss: 0.03457200\n",
      "Epoch: [ 2] [   8/  20] time: 9.0266, loss: 0.02954624\n",
      "Epoch: [ 2] [   9/  20] time: 9.2674, loss: 0.10525073\n",
      "Epoch: [ 2] [  10/  20] time: 9.5068, loss: 0.03105418\n",
      "Epoch: [ 2] [  11/  20] time: 9.7447, loss: 0.02094302\n",
      "Epoch: [ 2] [  12/  20] time: 9.9825, loss: 0.02698456\n",
      "Epoch: [ 2] [  13/  20] time: 10.2219, loss: 0.06511126\n",
      "Epoch: [ 2] [  14/  20] time: 10.4600, loss: 0.09536758\n",
      "Epoch: [ 2] [  15/  20] time: 10.6972, loss: 0.03104263\n",
      "Epoch: [ 2] [  16/  20] time: 10.9356, loss: 0.06751794\n",
      "Epoch: [ 2] [  17/  20] time: 11.1752, loss: 0.02364876\n",
      "Epoch: [ 2] [  18/  20] time: 11.4153, loss: 0.02316058\n",
      "Epoch: [ 2] [  19/  20] time: 11.6539, loss: 0.06850455\n",
      "[2/50] - ptime: 5.0430 loss: 0.04880401 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 3] [   0/  20] time: 12.5103, loss: 0.03631242\n",
      "Epoch: [ 3] [   1/  20] time: 12.7459, loss: 0.01977540\n",
      "Epoch: [ 3] [   2/  20] time: 12.9829, loss: 0.01895653\n",
      "Epoch: [ 3] [   3/  20] time: 13.2228, loss: 0.04641795\n",
      "Epoch: [ 3] [   4/  20] time: 13.4613, loss: 0.02312228\n",
      "Epoch: [ 3] [   5/  20] time: 13.6994, loss: 0.06782538\n",
      "Epoch: [ 3] [   6/  20] time: 13.9385, loss: 0.03802757\n",
      "Epoch: [ 3] [   7/  20] time: 14.1776, loss: 0.03238207\n",
      "Epoch: [ 3] [   8/  20] time: 14.4166, loss: 0.02250361\n",
      "Epoch: [ 3] [   9/  20] time: 14.6543, loss: 0.01275488\n",
      "Epoch: [ 3] [  10/  20] time: 14.8911, loss: 0.04308251\n",
      "Epoch: [ 3] [  11/  20] time: 15.1288, loss: 0.01238946\n",
      "Epoch: [ 3] [  12/  20] time: 15.3682, loss: 0.01652132\n",
      "Epoch: [ 3] [  13/  20] time: 15.6074, loss: 0.02668064\n",
      "Epoch: [ 3] [  14/  20] time: 15.8450, loss: 0.04006764\n",
      "Epoch: [ 3] [  15/  20] time: 16.0828, loss: 0.06215137\n",
      "Epoch: [ 3] [  16/  20] time: 16.3215, loss: 0.01840525\n",
      "Epoch: [ 3] [  17/  20] time: 16.5601, loss: 0.00447800\n",
      "Epoch: [ 3] [  18/  20] time: 16.7970, loss: 0.01456189\n",
      "Epoch: [ 3] [  19/  20] time: 17.0383, loss: 0.03800709\n",
      "[3/50] - ptime: 5.0393 loss: 0.02972116 acc: 0.81000 lr: 0.00100000\n",
      "Epoch: [ 4] [   0/  20] time: 17.9829, loss: 0.01826682\n",
      "Epoch: [ 4] [   1/  20] time: 18.2204, loss: 0.03972517\n",
      "Epoch: [ 4] [   2/  20] time: 18.4602, loss: 0.00881775\n",
      "Epoch: [ 4] [   3/  20] time: 18.6976, loss: 0.01195613\n",
      "Epoch: [ 4] [   4/  20] time: 18.9364, loss: 0.03089309\n",
      "Epoch: [ 4] [   5/  20] time: 19.1755, loss: 0.00982344\n",
      "Epoch: [ 4] [   6/  20] time: 19.4150, loss: 0.03562562\n",
      "Epoch: [ 4] [   7/  20] time: 19.6537, loss: 0.01695541\n",
      "Epoch: [ 4] [   8/  20] time: 19.8916, loss: 0.03139307\n",
      "Epoch: [ 4] [   9/  20] time: 20.1300, loss: 0.02198481\n",
      "Epoch: [ 4] [  10/  20] time: 20.3681, loss: 0.00520092\n",
      "Epoch: [ 4] [  11/  20] time: 20.6060, loss: 0.04058184\n",
      "Epoch: [ 4] [  12/  20] time: 20.8450, loss: 0.00330184\n",
      "Epoch: [ 4] [  13/  20] time: 21.0818, loss: 0.00991842\n",
      "Epoch: [ 4] [  14/  20] time: 21.3215, loss: 0.00585683\n",
      "Epoch: [ 4] [  15/  20] time: 21.5604, loss: 0.00243094\n",
      "Epoch: [ 4] [  16/  20] time: 21.7979, loss: 0.04616374\n",
      "Epoch: [ 4] [  17/  20] time: 22.0378, loss: 0.00834839\n",
      "Epoch: [ 4] [  18/  20] time: 22.2775, loss: 0.06322435\n",
      "Epoch: [ 4] [  19/  20] time: 22.5169, loss: 0.06737732\n",
      "[4/50] - ptime: 5.0460 loss: 0.02389229 acc: 0.83000 lr: 0.00100000\n",
      "Epoch: [ 5] [   0/  20] time: 23.4145, loss: 0.10865910\n",
      "Epoch: [ 5] [   1/  20] time: 23.6498, loss: 0.02450001\n",
      "Epoch: [ 5] [   2/  20] time: 23.8875, loss: 0.01971504\n",
      "Epoch: [ 5] [   3/  20] time: 24.1262, loss: 0.02393561\n",
      "Epoch: [ 5] [   4/  20] time: 24.3649, loss: 0.05963256\n",
      "Epoch: [ 5] [   5/  20] time: 24.6021, loss: 0.01057869\n",
      "Epoch: [ 5] [   6/  20] time: 24.8397, loss: 0.06364225\n",
      "Epoch: [ 5] [   7/  20] time: 25.0771, loss: 0.03515955\n",
      "Epoch: [ 5] [   8/  20] time: 25.3155, loss: 0.02286267\n",
      "Epoch: [ 5] [   9/  20] time: 25.5538, loss: 0.01538970\n",
      "Epoch: [ 5] [  10/  20] time: 25.7928, loss: 0.04467177\n",
      "Epoch: [ 5] [  11/  20] time: 26.0301, loss: 0.01493038\n",
      "Epoch: [ 5] [  12/  20] time: 26.2682, loss: 0.03186202\n",
      "Epoch: [ 5] [  13/  20] time: 26.5085, loss: 0.02718575\n",
      "Epoch: [ 5] [  14/  20] time: 26.7461, loss: 0.04590037\n",
      "Epoch: [ 5] [  15/  20] time: 26.9825, loss: 0.02200723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 5] [  16/  20] time: 27.2216, loss: 0.00396382\n",
      "Epoch: [ 5] [  17/  20] time: 27.4615, loss: 0.00971569\n",
      "Epoch: [ 5] [  18/  20] time: 27.6992, loss: 0.01792888\n",
      "Epoch: [ 5] [  19/  20] time: 27.9380, loss: 0.02458555\n",
      "[5/50] - ptime: 5.0362 loss: 0.03134133 acc: 0.78000 lr: 0.00100000\n",
      "Epoch: [ 6] [   0/  20] time: 28.8479, loss: 0.00889613\n",
      "Epoch: [ 6] [   1/  20] time: 29.0844, loss: 0.02998620\n",
      "Epoch: [ 6] [   2/  20] time: 29.3245, loss: 0.02045661\n",
      "Epoch: [ 6] [   3/  20] time: 29.5630, loss: 0.02132231\n",
      "Epoch: [ 6] [   4/  20] time: 29.8015, loss: 0.04078565\n",
      "Epoch: [ 6] [   5/  20] time: 30.0407, loss: 0.00900039\n",
      "Epoch: [ 6] [   6/  20] time: 30.2818, loss: 0.02459550\n",
      "Epoch: [ 6] [   7/  20] time: 30.5225, loss: 0.01126353\n",
      "Epoch: [ 6] [   8/  20] time: 30.7611, loss: 0.02309922\n",
      "Epoch: [ 6] [   9/  20] time: 30.9989, loss: 0.01149140\n",
      "Epoch: [ 6] [  10/  20] time: 31.2383, loss: 0.01130339\n",
      "Epoch: [ 6] [  11/  20] time: 31.4773, loss: 0.03808407\n",
      "Epoch: [ 6] [  12/  20] time: 31.7146, loss: 0.01375879\n",
      "Epoch: [ 6] [  13/  20] time: 31.9527, loss: 0.04673627\n",
      "Epoch: [ 6] [  14/  20] time: 32.1927, loss: 0.02264647\n",
      "Epoch: [ 6] [  15/  20] time: 32.4328, loss: 0.00986746\n",
      "Epoch: [ 6] [  16/  20] time: 32.6726, loss: 0.03479455\n",
      "Epoch: [ 6] [  17/  20] time: 32.9111, loss: 0.01158794\n",
      "Epoch: [ 6] [  18/  20] time: 33.1506, loss: 0.00476854\n",
      "Epoch: [ 6] [  19/  20] time: 33.3900, loss: 0.01445045\n",
      "[6/50] - ptime: 5.0592 loss: 0.02044474 acc: 0.52000 lr: 0.00100000\n",
      "Epoch: [ 7] [   0/  20] time: 34.2446, loss: 0.02417407\n",
      "Epoch: [ 7] [   1/  20] time: 34.4822, loss: 0.00995960\n",
      "Epoch: [ 7] [   2/  20] time: 34.7208, loss: 0.01711049\n",
      "Epoch: [ 7] [   3/  20] time: 34.9599, loss: 0.03659300\n",
      "Epoch: [ 7] [   4/  20] time: 35.2003, loss: 0.00962025\n",
      "Epoch: [ 7] [   5/  20] time: 35.4396, loss: 0.02977348\n",
      "Epoch: [ 7] [   6/  20] time: 35.6779, loss: 0.00935434\n",
      "Epoch: [ 7] [   7/  20] time: 35.9177, loss: 0.00247835\n",
      "Epoch: [ 7] [   8/  20] time: 36.1565, loss: 0.04329375\n",
      "Epoch: [ 7] [   9/  20] time: 36.3963, loss: 0.01892592\n",
      "Epoch: [ 7] [  10/  20] time: 36.6344, loss: 0.01599683\n",
      "Epoch: [ 7] [  11/  20] time: 36.8746, loss: 0.00155041\n",
      "Epoch: [ 7] [  12/  20] time: 37.1125, loss: 0.00555186\n",
      "Epoch: [ 7] [  13/  20] time: 37.3513, loss: 0.10963542\n",
      "Epoch: [ 7] [  14/  20] time: 37.5895, loss: 0.00894981\n",
      "Epoch: [ 7] [  15/  20] time: 37.8283, loss: 0.01698335\n",
      "Epoch: [ 7] [  16/  20] time: 38.0670, loss: 0.01697722\n",
      "Epoch: [ 7] [  17/  20] time: 38.3069, loss: 0.02163018\n",
      "Epoch: [ 7] [  18/  20] time: 38.5470, loss: 0.03776468\n",
      "Epoch: [ 7] [  19/  20] time: 38.7853, loss: 0.02573750\n",
      "[7/50] - ptime: 5.0517 loss: 0.02310303 acc: 0.54000 lr: 0.00100000\n",
      "Epoch: [ 8] [   0/  20] time: 39.6430, loss: 0.03224564\n",
      "Epoch: [ 8] [   1/  20] time: 39.8790, loss: 0.02352909\n",
      "Epoch: [ 8] [   2/  20] time: 40.1164, loss: 0.00681721\n",
      "Epoch: [ 8] [   3/  20] time: 40.3558, loss: 0.03397945\n",
      "Epoch: [ 8] [   4/  20] time: 40.5948, loss: 0.05411135\n",
      "Epoch: [ 8] [   5/  20] time: 40.8332, loss: 0.02360853\n",
      "Epoch: [ 8] [   6/  20] time: 41.0722, loss: 0.00290524\n",
      "Epoch: [ 8] [   7/  20] time: 41.3120, loss: 0.01779196\n",
      "Epoch: [ 8] [   8/  20] time: 41.5507, loss: 0.00368071\n",
      "Epoch: [ 8] [   9/  20] time: 41.7897, loss: 0.01465786\n",
      "Epoch: [ 8] [  10/  20] time: 42.0278, loss: 0.02877082\n",
      "Epoch: [ 8] [  11/  20] time: 42.2668, loss: 0.01266739\n",
      "Epoch: [ 8] [  12/  20] time: 42.5059, loss: 0.00675853\n",
      "Epoch: [ 8] [  13/  20] time: 42.7444, loss: 0.00759758\n",
      "Epoch: [ 8] [  14/  20] time: 42.9832, loss: 0.00792907\n",
      "Epoch: [ 8] [  15/  20] time: 43.2242, loss: 0.00228069\n",
      "Epoch: [ 8] [  16/  20] time: 43.4635, loss: 0.01284909\n",
      "Epoch: [ 8] [  17/  20] time: 43.7028, loss: 0.02293124\n",
      "Epoch: [ 8] [  18/  20] time: 43.9412, loss: 0.00334371\n",
      "Epoch: [ 8] [  19/  20] time: 44.1801, loss: 0.00033431\n",
      "[8/50] - ptime: 5.0480 loss: 0.01593947 acc: 0.76000 lr: 0.00100000\n",
      "Epoch: [ 9] [   0/  20] time: 45.0963, loss: 0.01178636\n",
      "Epoch: [ 9] [   1/  20] time: 45.3339, loss: 0.00031995\n",
      "Epoch: [ 9] [   2/  20] time: 45.5737, loss: 0.00831515\n",
      "Epoch: [ 9] [   3/  20] time: 45.8122, loss: 0.01563951\n",
      "Epoch: [ 9] [   4/  20] time: 46.0506, loss: 0.01002857\n",
      "Epoch: [ 9] [   5/  20] time: 46.2928, loss: 0.01208194\n",
      "Epoch: [ 9] [   6/  20] time: 46.5330, loss: 0.01094816\n",
      "Epoch: [ 9] [   7/  20] time: 46.7729, loss: 0.00716330\n",
      "Epoch: [ 9] [   8/  20] time: 47.0116, loss: 0.00426021\n",
      "Epoch: [ 9] [   9/  20] time: 47.2547, loss: 0.00034871\n",
      "Epoch: [ 9] [  10/  20] time: 47.4950, loss: 0.01952564\n",
      "Epoch: [ 9] [  11/  20] time: 47.7336, loss: 0.02243222\n",
      "Epoch: [ 9] [  12/  20] time: 47.9723, loss: 0.01611752\n",
      "Epoch: [ 9] [  13/  20] time: 48.2113, loss: 0.02571975\n",
      "Epoch: [ 9] [  14/  20] time: 48.4505, loss: 0.01527108\n",
      "Epoch: [ 9] [  15/  20] time: 48.6900, loss: 0.02106860\n",
      "Epoch: [ 9] [  16/  20] time: 48.9278, loss: 0.00457383\n",
      "Epoch: [ 9] [  17/  20] time: 49.1654, loss: 0.01822845\n",
      "Epoch: [ 9] [  18/  20] time: 49.4055, loss: 0.01665453\n",
      "Epoch: [ 9] [  19/  20] time: 49.6450, loss: 0.02333335\n",
      "[9/50] - ptime: 5.0580 loss: 0.01319084 acc: 0.74000 lr: 0.00100000\n",
      "Epoch: [10] [   0/  20] time: 50.5372, loss: 0.01882432\n",
      "Epoch: [10] [   1/  20] time: 50.7732, loss: 0.01337066\n",
      "Epoch: [10] [   2/  20] time: 51.0109, loss: 0.01000600\n",
      "Epoch: [10] [   3/  20] time: 51.2509, loss: 0.02771283\n",
      "Epoch: [10] [   4/  20] time: 51.4904, loss: 0.00346932\n",
      "Epoch: [10] [   5/  20] time: 51.7300, loss: 0.00727866\n",
      "Epoch: [10] [   6/  20] time: 51.9688, loss: 0.00825675\n",
      "Epoch: [10] [   7/  20] time: 52.2089, loss: 0.00065830\n",
      "Epoch: [10] [   8/  20] time: 52.4495, loss: 0.01070027\n",
      "Epoch: [10] [   9/  20] time: 52.6879, loss: 0.00609394\n",
      "Epoch: [10] [  10/  20] time: 52.9266, loss: 0.00299243\n",
      "Epoch: [10] [  11/  20] time: 53.1646, loss: 0.00454175\n",
      "Epoch: [10] [  12/  20] time: 53.4044, loss: 0.02023555\n",
      "Epoch: [10] [  13/  20] time: 53.6420, loss: 0.00051135\n",
      "Epoch: [10] [  14/  20] time: 53.8797, loss: 0.00765802\n",
      "Epoch: [10] [  15/  20] time: 54.1188, loss: 0.00477531\n",
      "Epoch: [10] [  16/  20] time: 54.3590, loss: 0.01362563\n",
      "Epoch: [10] [  17/  20] time: 54.5980, loss: 0.00291065\n",
      "Epoch: [10] [  18/  20] time: 54.8367, loss: 0.00180226\n",
      "Epoch: [10] [  19/  20] time: 55.0756, loss: 0.00278511\n",
      "[10/50] - ptime: 5.0489 loss: 0.00841046 acc: 0.80000 lr: 0.00090000\n",
      "Epoch: [11] [   0/  20] time: 55.9248, loss: 0.01091513\n",
      "Epoch: [11] [   1/  20] time: 56.1617, loss: 0.00235767\n",
      "Epoch: [11] [   2/  20] time: 56.4045, loss: 0.00012678\n",
      "Epoch: [11] [   3/  20] time: 56.6444, loss: 0.00027074\n",
      "Epoch: [11] [   4/  20] time: 56.8785, loss: 0.00013513\n",
      "Epoch: [11] [   5/  20] time: 57.1160, loss: 0.00570492\n",
      "Epoch: [11] [   6/  20] time: 57.3559, loss: 0.01195507\n",
      "Epoch: [11] [   7/  20] time: 57.5940, loss: 0.00540450\n",
      "Epoch: [11] [   8/  20] time: 57.8325, loss: 0.00041302\n",
      "Epoch: [11] [   9/  20] time: 58.0719, loss: 0.00129122\n",
      "Epoch: [11] [  10/  20] time: 58.3114, loss: 0.01972178\n",
      "Epoch: [11] [  11/  20] time: 58.5503, loss: 0.01609997\n",
      "Epoch: [11] [  12/  20] time: 58.7896, loss: 0.00141482\n",
      "Epoch: [11] [  13/  20] time: 59.0282, loss: 0.00193886\n",
      "Epoch: [11] [  14/  20] time: 59.2682, loss: 0.01047198\n",
      "Epoch: [11] [  15/  20] time: 59.5080, loss: 0.00434725\n",
      "Epoch: [11] [  16/  20] time: 59.7461, loss: 0.01065504\n",
      "Epoch: [11] [  17/  20] time: 59.9839, loss: 0.00194303\n",
      "Epoch: [11] [  18/  20] time: 60.2221, loss: 0.00073736\n",
      "Epoch: [11] [  19/  20] time: 60.4613, loss: 0.00761260\n",
      "[11/50] - ptime: 5.0461 loss: 0.00567584 acc: 0.82000 lr: 0.00090000\n",
      "Epoch: [12] [   0/  20] time: 61.3141, loss: 0.00478882\n",
      "Epoch: [12] [   1/  20] time: 61.5536, loss: 0.00672832\n",
      "Epoch: [12] [   2/  20] time: 61.7931, loss: 0.00029087\n",
      "Epoch: [12] [   3/  20] time: 62.0309, loss: 0.00984001\n",
      "Epoch: [12] [   4/  20] time: 62.2701, loss: 0.00042047\n",
      "Epoch: [12] [   5/  20] time: 62.5094, loss: 0.00035417\n",
      "Epoch: [12] [   6/  20] time: 62.7467, loss: 0.00145797\n",
      "Epoch: [12] [   7/  20] time: 62.9863, loss: 0.00216483\n",
      "Epoch: [12] [   8/  20] time: 63.2263, loss: 0.00454335\n",
      "Epoch: [12] [   9/  20] time: 63.4663, loss: 0.00344738\n",
      "Epoch: [12] [  10/  20] time: 63.7043, loss: 0.00087390\n",
      "Epoch: [12] [  11/  20] time: 63.9429, loss: 0.00332427\n",
      "Epoch: [12] [  12/  20] time: 64.1824, loss: 0.00052618\n",
      "Epoch: [12] [  13/  20] time: 64.4231, loss: 0.00259729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12] [  14/  20] time: 64.6621, loss: 0.00251351\n",
      "Epoch: [12] [  15/  20] time: 64.9004, loss: 0.00013421\n",
      "Epoch: [12] [  16/  20] time: 65.1387, loss: 0.00060437\n",
      "Epoch: [12] [  17/  20] time: 65.3783, loss: 0.00040199\n",
      "Epoch: [12] [  18/  20] time: 65.6162, loss: 0.00309230\n",
      "Epoch: [12] [  19/  20] time: 65.8560, loss: 0.00009235\n",
      "[12/50] - ptime: 5.0535 loss: 0.00240983 acc: 0.82000 lr: 0.00090000\n",
      "Epoch: [13] [   0/  20] time: 66.7197, loss: 0.00096265\n",
      "Epoch: [13] [   1/  20] time: 66.9574, loss: 0.00137912\n",
      "Epoch: [13] [   2/  20] time: 67.2000, loss: 0.00677651\n",
      "Epoch: [13] [   3/  20] time: 67.4410, loss: 0.00105344\n",
      "Epoch: [13] [   4/  20] time: 67.6793, loss: 0.00223841\n",
      "Epoch: [13] [   5/  20] time: 67.9176, loss: 0.00192875\n",
      "Epoch: [13] [   6/  20] time: 68.1577, loss: 0.02147016\n",
      "Epoch: [13] [   7/  20] time: 68.4005, loss: 0.00015801\n",
      "Epoch: [13] [   8/  20] time: 68.6400, loss: 0.01695956\n",
      "Epoch: [13] [   9/  20] time: 68.8796, loss: 0.00355072\n",
      "Epoch: [13] [  10/  20] time: 69.1192, loss: 0.01411915\n",
      "Epoch: [13] [  11/  20] time: 69.3615, loss: 0.00006952\n",
      "Epoch: [13] [  12/  20] time: 69.6002, loss: 0.00136911\n",
      "Epoch: [13] [  13/  20] time: 69.8396, loss: 0.00058090\n",
      "Epoch: [13] [  14/  20] time: 70.0786, loss: 0.00119867\n",
      "Epoch: [13] [  15/  20] time: 70.3199, loss: 0.00208439\n",
      "Epoch: [13] [  16/  20] time: 70.5601, loss: 0.00086072\n",
      "Epoch: [13] [  17/  20] time: 70.8001, loss: 0.00035647\n",
      "Epoch: [13] [  18/  20] time: 71.0390, loss: 0.00041441\n",
      "Epoch: [13] [  19/  20] time: 71.2788, loss: 0.00332035\n",
      "[13/50] - ptime: 5.0706 loss: 0.00404255 acc: 0.78000 lr: 0.00090000\n",
      "Epoch: [14] [   0/  20] time: 72.1455, loss: 0.00049285\n",
      "Epoch: [14] [   1/  20] time: 72.3865, loss: 0.00352350\n",
      "Epoch: [14] [   2/  20] time: 72.6209, loss: 0.00280155\n",
      "Epoch: [14] [   3/  20] time: 72.8599, loss: 0.00029517\n",
      "Epoch: [14] [   4/  20] time: 73.0989, loss: 0.00031699\n",
      "Epoch: [14] [   5/  20] time: 73.3396, loss: 0.00017893\n",
      "Epoch: [14] [   6/  20] time: 73.5750, loss: 0.00235121\n",
      "Epoch: [14] [   7/  20] time: 73.8160, loss: 0.00231951\n",
      "Epoch: [14] [   8/  20] time: 74.0554, loss: 0.00029266\n",
      "Epoch: [14] [   9/  20] time: 74.3033, loss: 0.00003270\n",
      "Epoch: [14] [  10/  20] time: 74.5432, loss: 0.00104243\n",
      "Epoch: [14] [  11/  20] time: 74.7818, loss: 0.00303188\n",
      "Epoch: [14] [  12/  20] time: 75.0208, loss: 0.00021011\n",
      "Epoch: [14] [  13/  20] time: 75.2597, loss: 0.00359668\n",
      "Epoch: [14] [  14/  20] time: 75.5008, loss: 0.00062591\n",
      "Epoch: [14] [  15/  20] time: 75.7404, loss: 0.00244006\n",
      "Epoch: [14] [  16/  20] time: 75.9806, loss: 0.00020984\n",
      "Epoch: [14] [  17/  20] time: 76.2213, loss: 0.00054288\n",
      "Epoch: [14] [  18/  20] time: 76.4613, loss: 0.00031567\n",
      "Epoch: [14] [  19/  20] time: 76.7005, loss: 0.00071893\n",
      "[14/50] - ptime: 5.0796 loss: 0.00126697 acc: 0.81000 lr: 0.00090000\n",
      "Epoch: [15] [   0/  20] time: 77.5608, loss: 0.00837545\n",
      "Epoch: [15] [   1/  20] time: 77.7978, loss: 0.00093805\n",
      "Epoch: [15] [   2/  20] time: 78.0363, loss: 0.01854463\n",
      "Epoch: [15] [   3/  20] time: 78.2760, loss: 0.00477089\n",
      "Epoch: [15] [   4/  20] time: 78.5156, loss: 0.13099985\n",
      "Epoch: [15] [   5/  20] time: 78.7544, loss: 0.01510704\n",
      "Epoch: [15] [   6/  20] time: 78.9932, loss: 0.03196409\n",
      "Epoch: [15] [   7/  20] time: 79.2331, loss: 0.00778281\n",
      "Epoch: [15] [   8/  20] time: 79.4751, loss: 0.01256809\n",
      "Epoch: [15] [   9/  20] time: 79.7141, loss: 0.00954300\n",
      "Epoch: [15] [  10/  20] time: 79.9549, loss: 0.01291350\n",
      "Epoch: [15] [  11/  20] time: 80.1955, loss: 0.01398794\n",
      "Epoch: [15] [  12/  20] time: 80.4366, loss: 0.06880342\n",
      "Epoch: [15] [  13/  20] time: 80.6757, loss: 0.01535271\n",
      "Epoch: [15] [  14/  20] time: 80.9156, loss: 0.00200963\n",
      "Epoch: [15] [  15/  20] time: 81.1541, loss: 0.01775000\n",
      "Epoch: [15] [  16/  20] time: 81.3965, loss: 0.03567812\n",
      "Epoch: [15] [  17/  20] time: 81.6347, loss: 0.00582475\n",
      "Epoch: [15] [  18/  20] time: 81.8750, loss: 0.01594445\n",
      "Epoch: [15] [  19/  20] time: 82.1141, loss: 0.04909413\n",
      "[15/50] - ptime: 5.0658 loss: 0.02389763 acc: 0.79000 lr: 0.00090000\n",
      "Epoch: [16] [   0/  20] time: 83.0678, loss: 0.00817109\n",
      "Epoch: [16] [   1/  20] time: 83.3063, loss: 0.00654425\n",
      "Epoch: [16] [   2/  20] time: 83.5464, loss: 0.02215255\n",
      "Epoch: [16] [   3/  20] time: 83.7851, loss: 0.04764546\n",
      "Epoch: [16] [   4/  20] time: 84.0250, loss: 0.00471404\n",
      "Epoch: [16] [   5/  20] time: 84.2659, loss: 0.03867797\n",
      "Epoch: [16] [   6/  20] time: 84.5059, loss: 0.01826903\n",
      "Epoch: [16] [   7/  20] time: 84.7456, loss: 0.01275351\n",
      "Epoch: [16] [   8/  20] time: 84.9840, loss: 0.08907555\n",
      "Epoch: [16] [   9/  20] time: 85.2233, loss: 0.00575090\n",
      "Epoch: [16] [  10/  20] time: 85.4636, loss: 0.03053057\n",
      "Epoch: [16] [  11/  20] time: 85.7018, loss: 0.02818728\n",
      "Epoch: [16] [  12/  20] time: 85.9410, loss: 0.01759653\n",
      "Epoch: [16] [  13/  20] time: 86.1801, loss: 0.01245458\n",
      "Epoch: [16] [  14/  20] time: 86.4210, loss: 0.01669830\n",
      "Epoch: [16] [  15/  20] time: 86.6596, loss: 0.19918233\n",
      "Epoch: [16] [  16/  20] time: 86.8981, loss: 0.02185844\n",
      "Epoch: [16] [  17/  20] time: 87.1365, loss: 0.11887961\n",
      "Epoch: [16] [  18/  20] time: 87.3779, loss: 0.01119589\n",
      "Epoch: [16] [  19/  20] time: 87.6171, loss: 0.00875515\n",
      "[16/50] - ptime: 5.0592 loss: 0.03595465 acc: 0.82000 lr: 0.00090000\n",
      "Epoch: [17] [   0/  20] time: 88.4881, loss: 0.01810578\n",
      "Epoch: [17] [   1/  20] time: 88.7247, loss: 0.01366160\n",
      "Epoch: [17] [   2/  20] time: 88.9641, loss: 0.02084164\n",
      "Epoch: [17] [   3/  20] time: 89.2020, loss: 0.00115777\n",
      "Epoch: [17] [   4/  20] time: 89.4434, loss: 0.00422508\n",
      "Epoch: [17] [   5/  20] time: 89.6822, loss: 0.00494898\n",
      "Epoch: [17] [   6/  20] time: 89.9216, loss: 0.00983902\n",
      "Epoch: [17] [   7/  20] time: 90.1607, loss: 0.01263231\n",
      "Epoch: [17] [   8/  20] time: 90.4026, loss: 0.00334653\n",
      "Epoch: [17] [   9/  20] time: 90.6415, loss: 0.00251401\n",
      "Epoch: [17] [  10/  20] time: 90.8804, loss: 0.00966047\n",
      "Epoch: [17] [  11/  20] time: 91.1200, loss: 0.00821658\n",
      "Epoch: [17] [  12/  20] time: 91.3604, loss: 0.02402344\n",
      "Epoch: [17] [  13/  20] time: 91.5983, loss: 0.00274437\n",
      "Epoch: [17] [  14/  20] time: 91.8362, loss: 0.01749991\n",
      "Epoch: [17] [  15/  20] time: 92.0757, loss: 0.03069574\n",
      "Epoch: [17] [  16/  20] time: 92.3152, loss: 0.00629524\n",
      "Epoch: [17] [  17/  20] time: 92.5544, loss: 0.00249604\n",
      "Epoch: [17] [  18/  20] time: 92.7943, loss: 0.00031366\n",
      "Epoch: [17] [  19/  20] time: 93.0328, loss: 0.00292207\n",
      "[17/50] - ptime: 5.0576 loss: 0.00980701 acc: 0.82000 lr: 0.00090000\n",
      "Epoch: [18] [   0/  20] time: 93.8953, loss: 0.01478450\n",
      "Epoch: [18] [   1/  20] time: 94.1371, loss: 0.00617747\n",
      "Epoch: [18] [   2/  20] time: 94.3789, loss: 0.00123300\n",
      "Epoch: [18] [   3/  20] time: 94.6181, loss: 0.00119962\n",
      "Epoch: [18] [   4/  20] time: 94.8579, loss: 0.00684203\n",
      "Epoch: [18] [   5/  20] time: 95.0972, loss: 0.01108138\n",
      "Epoch: [18] [   6/  20] time: 95.3394, loss: 0.00214594\n",
      "Epoch: [18] [   7/  20] time: 95.5792, loss: 0.00014575\n",
      "Epoch: [18] [   8/  20] time: 95.8194, loss: 0.01519606\n",
      "Epoch: [18] [   9/  20] time: 96.0579, loss: 0.00121825\n",
      "Epoch: [18] [  10/  20] time: 96.2988, loss: 0.00463312\n",
      "Epoch: [18] [  11/  20] time: 96.5383, loss: 0.00135758\n",
      "Epoch: [18] [  12/  20] time: 96.7778, loss: 0.00160288\n",
      "Epoch: [18] [  13/  20] time: 97.0167, loss: 0.00303180\n",
      "Epoch: [18] [  14/  20] time: 97.2547, loss: 0.00211480\n",
      "Epoch: [18] [  15/  20] time: 97.4946, loss: 0.00157708\n",
      "Epoch: [18] [  16/  20] time: 97.7325, loss: 0.00701559\n",
      "Epoch: [18] [  17/  20] time: 97.9708, loss: 0.00220084\n",
      "Epoch: [18] [  18/  20] time: 98.2096, loss: 0.00007250\n",
      "Epoch: [18] [  19/  20] time: 98.4505, loss: 0.00025691\n",
      "[18/50] - ptime: 5.0658 loss: 0.00419436 acc: 0.85000 lr: 0.00090000\n",
      "Epoch: [19] [   0/  20] time: 99.3133, loss: 0.00052368\n",
      "Epoch: [19] [   1/  20] time: 99.5517, loss: 0.01302547\n",
      "Epoch: [19] [   2/  20] time: 99.7923, loss: 0.00044930\n",
      "Epoch: [19] [   3/  20] time: 100.0316, loss: 0.00065722\n",
      "Epoch: [19] [   4/  20] time: 100.2714, loss: 0.00053500\n",
      "Epoch: [19] [   5/  20] time: 100.5135, loss: 0.00183348\n",
      "Epoch: [19] [   6/  20] time: 100.7514, loss: 0.00009204\n",
      "Epoch: [19] [   7/  20] time: 100.9906, loss: 0.00142838\n",
      "Epoch: [19] [   8/  20] time: 101.2296, loss: 0.00057534\n",
      "Epoch: [19] [   9/  20] time: 101.4707, loss: 0.00110237\n",
      "Epoch: [19] [  10/  20] time: 101.7094, loss: 0.00034978\n",
      "Epoch: [19] [  11/  20] time: 101.9477, loss: 0.00151877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19] [  12/  20] time: 102.1859, loss: 0.00006191\n",
      "Epoch: [19] [  13/  20] time: 102.4278, loss: 0.00246348\n",
      "Epoch: [19] [  14/  20] time: 102.6677, loss: 0.00085639\n",
      "Epoch: [19] [  15/  20] time: 102.9082, loss: 0.00171179\n",
      "Epoch: [19] [  16/  20] time: 103.1491, loss: 0.00003291\n",
      "Epoch: [19] [  17/  20] time: 103.3916, loss: 0.00005389\n",
      "Epoch: [19] [  18/  20] time: 103.6305, loss: 0.00015532\n",
      "Epoch: [19] [  19/  20] time: 103.8702, loss: 0.00151418\n",
      "[19/50] - ptime: 5.0680 loss: 0.00144703 acc: 0.80000 lr: 0.00090000\n",
      "Epoch: [20] [   0/  20] time: 104.7317, loss: 0.00013203\n",
      "Epoch: [20] [   1/  20] time: 104.9702, loss: 0.00015456\n",
      "Epoch: [20] [   2/  20] time: 105.2113, loss: 0.00069818\n",
      "Epoch: [20] [   3/  20] time: 105.4533, loss: 0.00013117\n",
      "Epoch: [20] [   4/  20] time: 105.6938, loss: 0.00034899\n",
      "Epoch: [20] [   5/  20] time: 105.9330, loss: 0.00127127\n",
      "Epoch: [20] [   6/  20] time: 106.1729, loss: 0.00125610\n",
      "Epoch: [20] [   7/  20] time: 106.4201, loss: 0.00003673\n",
      "Epoch: [20] [   8/  20] time: 106.6590, loss: 0.00008023\n",
      "Epoch: [20] [   9/  20] time: 106.8986, loss: 0.00031789\n",
      "Epoch: [20] [  10/  20] time: 107.1391, loss: 0.00038864\n",
      "Epoch: [20] [  11/  20] time: 107.3849, loss: 0.00318585\n",
      "Epoch: [20] [  12/  20] time: 107.6245, loss: 0.00025648\n",
      "Epoch: [20] [  13/  20] time: 107.8638, loss: 0.00097507\n",
      "Epoch: [20] [  14/  20] time: 108.1022, loss: 0.00013081\n",
      "Epoch: [20] [  15/  20] time: 108.3436, loss: 0.00058199\n",
      "Epoch: [20] [  16/  20] time: 108.5801, loss: 0.00051494\n",
      "Epoch: [20] [  17/  20] time: 108.8193, loss: 0.00043935\n",
      "Epoch: [20] [  18/  20] time: 109.0594, loss: 0.00007031\n",
      "Epoch: [20] [  19/  20] time: 109.3003, loss: 0.00004176\n",
      "[20/50] - ptime: 5.0802 loss: 0.00055062 acc: 0.79000 lr: 0.00081000\n",
      "Epoch: [21] [   0/  20] time: 110.2296, loss: 0.00026796\n",
      "Epoch: [21] [   1/  20] time: 110.4691, loss: 0.00018109\n",
      "Epoch: [21] [   2/  20] time: 110.7090, loss: 0.00055760\n",
      "Epoch: [21] [   3/  20] time: 110.9484, loss: 0.00033302\n",
      "Epoch: [21] [   4/  20] time: 111.1885, loss: 0.00069313\n",
      "Epoch: [21] [   5/  20] time: 111.4308, loss: 0.00010955\n",
      "Epoch: [21] [   6/  20] time: 111.6701, loss: 0.00228468\n",
      "Epoch: [21] [   7/  20] time: 111.9100, loss: 0.00028914\n",
      "Epoch: [21] [   8/  20] time: 112.1503, loss: 0.00035456\n",
      "Epoch: [21] [   9/  20] time: 112.3927, loss: 0.00004807\n",
      "Epoch: [21] [  10/  20] time: 112.6323, loss: 0.00027842\n",
      "Epoch: [21] [  11/  20] time: 112.8717, loss: 0.00006828\n",
      "Epoch: [21] [  12/  20] time: 113.1124, loss: 0.00052118\n",
      "Epoch: [21] [  13/  20] time: 113.3539, loss: 0.00138153\n",
      "Epoch: [21] [  14/  20] time: 113.5941, loss: 0.00030187\n",
      "Epoch: [21] [  15/  20] time: 113.8342, loss: 0.00002585\n",
      "Epoch: [21] [  16/  20] time: 114.0750, loss: 0.00603288\n",
      "Epoch: [21] [  17/  20] time: 114.3155, loss: 0.00039403\n",
      "Epoch: [21] [  18/  20] time: 114.5561, loss: 0.00047925\n",
      "Epoch: [21] [  19/  20] time: 114.7954, loss: 0.00137650\n",
      "[21/50] - ptime: 5.0755 loss: 0.00079893 acc: 0.77000 lr: 0.00081000\n",
      "Epoch: [22] [   0/  20] time: 115.6834, loss: 0.00146908\n",
      "Epoch: [22] [   1/  20] time: 115.9210, loss: 0.00058500\n",
      "Epoch: [22] [   2/  20] time: 116.1611, loss: 0.00001231\n",
      "Epoch: [22] [   3/  20] time: 116.4050, loss: 0.00251288\n",
      "Epoch: [22] [   4/  20] time: 116.6439, loss: 0.00009175\n",
      "Epoch: [22] [   5/  20] time: 116.8841, loss: 0.00019597\n",
      "Epoch: [22] [   6/  20] time: 117.1236, loss: 0.00056734\n",
      "Epoch: [22] [   7/  20] time: 117.3655, loss: 0.00001255\n",
      "Epoch: [22] [   8/  20] time: 117.6049, loss: 0.00004842\n",
      "Epoch: [22] [   9/  20] time: 117.8442, loss: 0.00064387\n",
      "Epoch: [22] [  10/  20] time: 118.0838, loss: 0.00141947\n",
      "Epoch: [22] [  11/  20] time: 118.3235, loss: 0.00083314\n",
      "Epoch: [22] [  12/  20] time: 118.5631, loss: 0.00004840\n",
      "Epoch: [22] [  13/  20] time: 118.8017, loss: 0.00005221\n",
      "Epoch: [22] [  14/  20] time: 119.0421, loss: 0.00007542\n",
      "Epoch: [22] [  15/  20] time: 119.2810, loss: 0.00021070\n",
      "Epoch: [22] [  16/  20] time: 119.5201, loss: 0.00005843\n",
      "Epoch: [22] [  17/  20] time: 119.7594, loss: 0.00170190\n",
      "Epoch: [22] [  18/  20] time: 119.9986, loss: 0.00095683\n",
      "Epoch: [22] [  19/  20] time: 120.2384, loss: 0.00904020\n",
      "[22/50] - ptime: 5.0645 loss: 0.00102679 acc: 0.75000 lr: 0.00081000\n",
      "Epoch: [23] [   0/  20] time: 121.1029, loss: 0.00009676\n",
      "Epoch: [23] [   1/  20] time: 121.3417, loss: 0.00101888\n",
      "Epoch: [23] [   2/  20] time: 121.5827, loss: 0.00015269\n",
      "Epoch: [23] [   3/  20] time: 121.8221, loss: 0.00035168\n",
      "Epoch: [23] [   4/  20] time: 122.0620, loss: 0.06176185\n",
      "Epoch: [23] [   5/  20] time: 122.3023, loss: 0.00433250\n",
      "Epoch: [23] [   6/  20] time: 122.5434, loss: 0.00193164\n",
      "Epoch: [23] [   7/  20] time: 122.7834, loss: 0.00163664\n",
      "Epoch: [23] [   8/  20] time: 123.0224, loss: 0.00028256\n",
      "Epoch: [23] [   9/  20] time: 123.2626, loss: 0.00062905\n",
      "Epoch: [23] [  10/  20] time: 123.5042, loss: 0.00005514\n",
      "Epoch: [23] [  11/  20] time: 123.7437, loss: 0.00001366\n",
      "Epoch: [23] [  12/  20] time: 123.9827, loss: 0.00024727\n",
      "Epoch: [23] [  13/  20] time: 124.2214, loss: 0.00063768\n",
      "Epoch: [23] [  14/  20] time: 124.4649, loss: 0.00001802\n",
      "Epoch: [23] [  15/  20] time: 124.7037, loss: 0.00009057\n",
      "Epoch: [23] [  16/  20] time: 124.9439, loss: 0.00001907\n",
      "Epoch: [23] [  17/  20] time: 125.1834, loss: 0.00314789\n",
      "Epoch: [23] [  18/  20] time: 125.4261, loss: 0.00034010\n",
      "Epoch: [23] [  19/  20] time: 125.6671, loss: 0.00250024\n",
      "[23/50] - ptime: 5.0750 loss: 0.00396320 acc: 0.72000 lr: 0.00081000\n",
      "Epoch: [24] [   0/  20] time: 126.5315, loss: 0.00015405\n",
      "Epoch: [24] [   1/  20] time: 126.7692, loss: 0.00090158\n",
      "Epoch: [24] [   2/  20] time: 127.0104, loss: 0.00004707\n",
      "Epoch: [24] [   3/  20] time: 127.2491, loss: 0.00171834\n",
      "Epoch: [24] [   4/  20] time: 127.4898, loss: 0.00044191\n",
      "Epoch: [24] [   5/  20] time: 127.7286, loss: 0.00010645\n",
      "Epoch: [24] [   6/  20] time: 127.9672, loss: 0.00033920\n",
      "Epoch: [24] [   7/  20] time: 128.2057, loss: 0.00001649\n",
      "Epoch: [24] [   8/  20] time: 128.4479, loss: 0.00055154\n",
      "Epoch: [24] [   9/  20] time: 128.6874, loss: 0.00015972\n",
      "Epoch: [24] [  10/  20] time: 128.9272, loss: 0.00024816\n",
      "Epoch: [24] [  11/  20] time: 129.1660, loss: 0.00018502\n",
      "Epoch: [24] [  12/  20] time: 129.4086, loss: 0.00010648\n",
      "Epoch: [24] [  13/  20] time: 129.6495, loss: 0.00008574\n",
      "Epoch: [24] [  14/  20] time: 129.8884, loss: 0.00002796\n",
      "Epoch: [24] [  15/  20] time: 130.1285, loss: 0.00018073\n",
      "Epoch: [24] [  16/  20] time: 130.3698, loss: 0.00032008\n",
      "Epoch: [24] [  17/  20] time: 130.6099, loss: 0.00028262\n",
      "Epoch: [24] [  18/  20] time: 130.8498, loss: 0.00020060\n",
      "Epoch: [24] [  19/  20] time: 131.0900, loss: 0.00497727\n",
      "[24/50] - ptime: 5.0715 loss: 0.00055255 acc: 0.72000 lr: 0.00081000\n",
      "Epoch: [25] [   0/  20] time: 131.9937, loss: 0.00015671\n",
      "Epoch: [25] [   1/  20] time: 132.2346, loss: 0.00038270\n",
      "Epoch: [25] [   2/  20] time: 132.4761, loss: 0.00021159\n",
      "Epoch: [25] [   3/  20] time: 132.7172, loss: 0.00237194\n",
      "Epoch: [25] [   4/  20] time: 132.9563, loss: 0.00000590\n",
      "Epoch: [25] [   5/  20] time: 133.1956, loss: 0.00007078\n",
      "Epoch: [25] [   6/  20] time: 133.4386, loss: 0.00007487\n",
      "Epoch: [25] [   7/  20] time: 133.6783, loss: 0.00002244\n",
      "Epoch: [25] [   8/  20] time: 133.9176, loss: 0.00025265\n",
      "Epoch: [25] [   9/  20] time: 134.1577, loss: 0.00148694\n",
      "Epoch: [25] [  10/  20] time: 134.4001, loss: 0.00050768\n",
      "Epoch: [25] [  11/  20] time: 134.6399, loss: 0.00009954\n",
      "Epoch: [25] [  12/  20] time: 134.8789, loss: 0.00027141\n",
      "Epoch: [25] [  13/  20] time: 135.1181, loss: 0.00000991\n",
      "Epoch: [25] [  14/  20] time: 135.3589, loss: 0.00010654\n",
      "Epoch: [25] [  15/  20] time: 135.5978, loss: 0.00007856\n",
      "Epoch: [25] [  16/  20] time: 135.8369, loss: 0.00002034\n",
      "Epoch: [25] [  17/  20] time: 136.0755, loss: 0.00001368\n",
      "Epoch: [25] [  18/  20] time: 136.3139, loss: 0.00038694\n",
      "Epoch: [25] [  19/  20] time: 136.5532, loss: 0.00008143\n",
      "[25/50] - ptime: 5.0789 loss: 0.00033063 acc: 0.72000 lr: 0.00081000\n",
      "Epoch: [26] [   0/  20] time: 137.4162, loss: 0.00002679\n",
      "Epoch: [26] [   1/  20] time: 137.6537, loss: 0.00013691\n",
      "Epoch: [26] [   2/  20] time: 137.8959, loss: 0.00004767\n",
      "Epoch: [26] [   3/  20] time: 138.1363, loss: 0.00020633\n",
      "Epoch: [26] [   4/  20] time: 138.3801, loss: 0.00026922\n",
      "Epoch: [26] [   5/  20] time: 138.6188, loss: 0.00021336\n",
      "Epoch: [26] [   6/  20] time: 138.8581, loss: 0.00014110\n",
      "Epoch: [26] [   7/  20] time: 139.0974, loss: 0.00065978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26] [   8/  20] time: 139.3385, loss: 0.00007413\n",
      "Epoch: [26] [   9/  20] time: 139.5784, loss: 0.00011771\n",
      "Epoch: [26] [  10/  20] time: 139.8180, loss: 0.00075853\n",
      "Epoch: [26] [  11/  20] time: 140.0579, loss: 0.00003245\n",
      "Epoch: [26] [  12/  20] time: 140.3018, loss: 0.00011733\n",
      "Epoch: [26] [  13/  20] time: 140.5426, loss: 0.00004312\n",
      "Epoch: [26] [  14/  20] time: 140.7828, loss: 0.00003279\n",
      "Epoch: [26] [  15/  20] time: 141.0232, loss: 0.00079984\n",
      "Epoch: [26] [  16/  20] time: 141.2637, loss: 0.00002087\n",
      "Epoch: [26] [  17/  20] time: 141.5050, loss: 0.00018427\n",
      "Epoch: [26] [  18/  20] time: 141.7475, loss: 0.00001970\n",
      "Epoch: [26] [  19/  20] time: 141.9866, loss: 0.00016181\n",
      "[26/50] - ptime: 5.0826 loss: 0.00020318 acc: 0.73000 lr: 0.00081000\n",
      "Epoch: [27] [   0/  20] time: 142.8473, loss: 0.00020062\n",
      "Epoch: [27] [   1/  20] time: 143.0824, loss: 0.00000517\n",
      "Epoch: [27] [   2/  20] time: 143.3214, loss: 0.00005127\n",
      "Epoch: [27] [   3/  20] time: 143.5630, loss: 0.00002669\n",
      "Epoch: [27] [   4/  20] time: 143.8020, loss: 0.00001432\n",
      "Epoch: [27] [   5/  20] time: 144.0424, loss: 0.00000815\n",
      "Epoch: [27] [   6/  20] time: 144.2821, loss: 0.00023623\n",
      "Epoch: [27] [   7/  20] time: 144.5229, loss: 0.00001565\n",
      "Epoch: [27] [   8/  20] time: 144.7630, loss: 0.00008804\n",
      "Epoch: [27] [   9/  20] time: 145.0023, loss: 0.00005756\n",
      "Epoch: [27] [  10/  20] time: 145.2414, loss: 0.00004540\n",
      "Epoch: [27] [  11/  20] time: 145.4831, loss: 0.00018932\n",
      "Epoch: [27] [  12/  20] time: 145.7235, loss: 0.00001824\n",
      "Epoch: [27] [  13/  20] time: 145.9631, loss: 0.00007376\n",
      "Epoch: [27] [  14/  20] time: 146.2026, loss: 0.00007054\n",
      "Epoch: [27] [  15/  20] time: 146.4435, loss: 0.00012104\n",
      "Epoch: [27] [  16/  20] time: 146.6824, loss: 0.00007674\n",
      "Epoch: [27] [  17/  20] time: 146.9206, loss: 0.00009030\n",
      "Epoch: [27] [  18/  20] time: 147.1601, loss: 0.00035260\n",
      "Epoch: [27] [  19/  20] time: 147.4027, loss: 0.00005633\n",
      "[27/50] - ptime: 5.0670 loss: 0.00008990 acc: 0.72000 lr: 0.00081000\n",
      "Epoch: [28] [   0/  20] time: 148.2840, loss: 0.00011804\n",
      "Epoch: [28] [   1/  20] time: 148.5246, loss: 0.00021325\n",
      "Epoch: [28] [   2/  20] time: 148.7651, loss: 0.00002307\n",
      "Epoch: [28] [   3/  20] time: 149.0049, loss: 0.00006237\n",
      "Epoch: [28] [   4/  20] time: 149.2452, loss: 0.00000934\n",
      "Epoch: [28] [   5/  20] time: 149.4879, loss: 0.00000666\n",
      "Epoch: [28] [   6/  20] time: 149.7281, loss: 0.00011769\n",
      "Epoch: [28] [   7/  20] time: 149.9678, loss: 0.00007385\n",
      "Epoch: [28] [   8/  20] time: 150.2081, loss: 0.00006406\n",
      "Epoch: [28] [   9/  20] time: 150.4491, loss: 0.00008238\n",
      "Epoch: [28] [  10/  20] time: 150.6892, loss: 0.00001910\n",
      "Epoch: [28] [  11/  20] time: 150.9290, loss: 0.00000294\n",
      "Epoch: [28] [  12/  20] time: 151.1690, loss: 0.00002734\n",
      "Epoch: [28] [  13/  20] time: 151.4121, loss: 0.00041666\n",
      "Epoch: [28] [  14/  20] time: 151.6510, loss: 0.00003924\n",
      "Epoch: [28] [  15/  20] time: 151.8913, loss: 0.00012915\n",
      "Epoch: [28] [  16/  20] time: 152.1305, loss: 0.00006135\n",
      "Epoch: [28] [  17/  20] time: 152.3715, loss: 0.00002079\n",
      "Epoch: [28] [  18/  20] time: 152.6111, loss: 0.00003410\n",
      "Epoch: [28] [  19/  20] time: 152.8507, loss: 0.00000529\n",
      "[28/50] - ptime: 5.0763 loss: 0.00007633 acc: 0.72000 lr: 0.00081000\n",
      "Epoch: [29] [   0/  20] time: 153.7053, loss: 0.00001741\n",
      "Epoch: [29] [   1/  20] time: 153.9432, loss: 0.00002132\n",
      "Epoch: [29] [   2/  20] time: 154.1835, loss: 0.00010162\n",
      "Epoch: [29] [   3/  20] time: 154.4262, loss: 0.00061040\n",
      "Epoch: [29] [   4/  20] time: 154.6658, loss: 0.00001430\n",
      "Epoch: [29] [   5/  20] time: 154.9060, loss: 0.00019760\n",
      "Epoch: [29] [   6/  20] time: 155.1459, loss: 0.00004260\n",
      "Epoch: [29] [   7/  20] time: 155.3875, loss: 0.00030750\n",
      "Epoch: [29] [   8/  20] time: 155.6272, loss: 0.00005691\n",
      "Epoch: [29] [   9/  20] time: 155.8677, loss: 0.00016895\n",
      "Epoch: [29] [  10/  20] time: 156.1079, loss: 0.00000441\n",
      "Epoch: [29] [  11/  20] time: 156.3474, loss: 0.00003440\n",
      "Epoch: [29] [  12/  20] time: 156.5879, loss: 0.00005345\n",
      "Epoch: [29] [  13/  20] time: 156.8307, loss: 0.00000572\n",
      "Epoch: [29] [  14/  20] time: 157.0703, loss: 0.00000466\n",
      "Epoch: [29] [  15/  20] time: 157.3104, loss: 0.00013554\n",
      "Epoch: [29] [  16/  20] time: 157.5523, loss: 0.00001295\n",
      "Epoch: [29] [  17/  20] time: 157.7921, loss: 0.00000350\n",
      "Epoch: [29] [  18/  20] time: 158.0332, loss: 0.00011520\n",
      "Epoch: [29] [  19/  20] time: 158.2741, loss: 0.00004427\n",
      "[29/50] - ptime: 5.0794 loss: 0.00009764 acc: 0.72000 lr: 0.00081000\n",
      "Epoch: [30] [   0/  20] time: 159.1337, loss: 0.00001632\n",
      "Epoch: [30] [   1/  20] time: 159.3717, loss: 0.00001908\n",
      "Epoch: [30] [   2/  20] time: 159.6133, loss: 0.00009391\n",
      "Epoch: [30] [   3/  20] time: 159.8529, loss: 0.00000575\n",
      "Epoch: [30] [   4/  20] time: 160.0933, loss: 0.00010519\n",
      "Epoch: [30] [   5/  20] time: 160.3336, loss: 0.00002161\n",
      "Epoch: [30] [   6/  20] time: 160.5744, loss: 0.00000850\n",
      "Epoch: [30] [   7/  20] time: 160.8146, loss: 0.00009824\n",
      "Epoch: [30] [   8/  20] time: 161.0533, loss: 0.00010012\n",
      "Epoch: [30] [   9/  20] time: 161.2925, loss: 0.00008029\n",
      "Epoch: [30] [  10/  20] time: 161.5340, loss: 0.00001161\n",
      "Epoch: [30] [  11/  20] time: 161.7728, loss: 0.00001817\n",
      "Epoch: [30] [  12/  20] time: 162.0119, loss: 0.00000244\n",
      "Epoch: [30] [  13/  20] time: 162.2521, loss: 0.00000240\n",
      "Epoch: [30] [  14/  20] time: 162.4946, loss: 0.00005918\n",
      "Epoch: [30] [  15/  20] time: 162.7337, loss: 0.00008414\n",
      "Epoch: [30] [  16/  20] time: 162.9720, loss: 0.00000377\n",
      "Epoch: [30] [  17/  20] time: 163.2113, loss: 0.00008633\n",
      "Epoch: [30] [  18/  20] time: 163.4531, loss: 0.00016792\n",
      "Epoch: [30] [  19/  20] time: 163.6925, loss: 0.00011082\n",
      "[30/50] - ptime: 5.0701 loss: 0.00005479 acc: 0.72000 lr: 0.00072900\n",
      "Epoch: [31] [   0/  20] time: 164.6382, loss: 0.00007113\n",
      "Epoch: [31] [   1/  20] time: 164.8756, loss: 0.00000783\n",
      "Epoch: [31] [   2/  20] time: 165.1162, loss: 0.00006040\n",
      "Epoch: [31] [   3/  20] time: 165.3553, loss: 0.00007388\n",
      "Epoch: [31] [   4/  20] time: 165.5965, loss: 0.00007513\n",
      "Epoch: [31] [   5/  20] time: 165.8360, loss: 0.00001907\n",
      "Epoch: [31] [   6/  20] time: 166.0765, loss: 0.00000860\n",
      "Epoch: [31] [   7/  20] time: 166.3173, loss: 0.00002038\n",
      "Epoch: [31] [   8/  20] time: 166.5576, loss: 0.00000479\n",
      "Epoch: [31] [   9/  20] time: 166.7983, loss: 0.00000390\n",
      "Epoch: [31] [  10/  20] time: 167.0379, loss: 0.00008505\n",
      "Epoch: [31] [  11/  20] time: 167.2783, loss: 0.00003150\n",
      "Epoch: [31] [  12/  20] time: 167.5201, loss: 0.00017365\n",
      "Epoch: [31] [  13/  20] time: 167.7606, loss: 0.00001447\n",
      "Epoch: [31] [  14/  20] time: 168.0019, loss: 0.00001282\n",
      "Epoch: [31] [  15/  20] time: 168.2414, loss: 0.00004245\n",
      "Epoch: [31] [  16/  20] time: 168.4849, loss: 0.00011440\n",
      "Epoch: [31] [  17/  20] time: 168.7248, loss: 0.00003049\n",
      "Epoch: [31] [  18/  20] time: 168.9656, loss: 0.00001007\n",
      "Epoch: [31] [  19/  20] time: 169.2051, loss: 0.00000665\n",
      "[31/50] - ptime: 5.0762 loss: 0.00004333 acc: 0.72000 lr: 0.00072900\n",
      "Epoch: [32] [   0/  20] time: 170.0877, loss: 0.00034483\n",
      "Epoch: [32] [   1/  20] time: 170.3269, loss: 0.00005040\n",
      "Epoch: [32] [   2/  20] time: 170.5673, loss: 0.00000740\n",
      "Epoch: [32] [   3/  20] time: 170.8071, loss: 0.00005125\n",
      "Epoch: [32] [   4/  20] time: 171.0489, loss: 0.00000283\n",
      "Epoch: [32] [   5/  20] time: 171.2892, loss: 0.00010511\n",
      "Epoch: [32] [   6/  20] time: 171.5324, loss: 0.00001063\n",
      "Epoch: [32] [   7/  20] time: 171.7727, loss: 0.00004816\n",
      "Epoch: [32] [   8/  20] time: 172.0133, loss: 0.00001519\n",
      "Epoch: [32] [   9/  20] time: 172.2542, loss: 0.00004760\n",
      "Epoch: [32] [  10/  20] time: 172.4982, loss: 0.00003861\n",
      "Epoch: [32] [  11/  20] time: 172.7373, loss: 0.00000462\n",
      "Epoch: [32] [  12/  20] time: 172.9778, loss: 0.00001062\n",
      "Epoch: [32] [  13/  20] time: 173.2173, loss: 0.00000576\n",
      "Epoch: [32] [  14/  20] time: 173.4601, loss: 0.00000190\n",
      "Epoch: [32] [  15/  20] time: 173.7008, loss: 0.00008308\n",
      "Epoch: [32] [  16/  20] time: 173.9408, loss: 0.00000644\n",
      "Epoch: [32] [  17/  20] time: 174.1821, loss: 0.00002040\n",
      "Epoch: [32] [  18/  20] time: 174.4222, loss: 0.00000522\n",
      "Epoch: [32] [  19/  20] time: 174.6628, loss: 0.00001939\n",
      "[32/50] - ptime: 5.0869 loss: 0.00004397 acc: 0.72000 lr: 0.00072900\n",
      "Epoch: [33] [   0/  20] time: 175.5235, loss: 0.00001709\n",
      "Epoch: [33] [   1/  20] time: 175.7611, loss: 0.00019232\n",
      "Epoch: [33] [   2/  20] time: 176.0012, loss: 0.00003245\n",
      "Epoch: [33] [   3/  20] time: 176.2407, loss: 0.00005353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33] [   4/  20] time: 176.4846, loss: 0.00008807\n",
      "Epoch: [33] [   5/  20] time: 176.7239, loss: 0.00004590\n",
      "Epoch: [33] [   6/  20] time: 176.9643, loss: 0.00001291\n",
      "Epoch: [33] [   7/  20] time: 177.2045, loss: 0.00000338\n",
      "Epoch: [33] [   8/  20] time: 177.4483, loss: 0.00001389\n",
      "Epoch: [33] [   9/  20] time: 177.6855, loss: 0.00000424\n",
      "Epoch: [33] [  10/  20] time: 177.9271, loss: 0.00001770\n",
      "Epoch: [33] [  11/  20] time: 178.1679, loss: 0.00000416\n",
      "Epoch: [33] [  12/  20] time: 178.4122, loss: 0.00001167\n",
      "Epoch: [33] [  13/  20] time: 178.6522, loss: 0.00001955\n",
      "Epoch: [33] [  14/  20] time: 178.8922, loss: 0.00002605\n",
      "Epoch: [33] [  15/  20] time: 179.1326, loss: 0.00001127\n",
      "Epoch: [33] [  16/  20] time: 179.3731, loss: 0.00007371\n",
      "Epoch: [33] [  17/  20] time: 179.6125, loss: 0.00001171\n",
      "Epoch: [33] [  18/  20] time: 179.8515, loss: 0.00003491\n",
      "Epoch: [33] [  19/  20] time: 180.0905, loss: 0.00004983\n",
      "[33/50] - ptime: 5.0810 loss: 0.00003622 acc: 0.70000 lr: 0.00072900\n",
      "Epoch: [34] [   0/  20] time: 180.9554, loss: 0.00024509\n",
      "Epoch: [34] [   1/  20] time: 181.1932, loss: 0.00000257\n",
      "Epoch: [34] [   2/  20] time: 181.4344, loss: 0.00000283\n",
      "Epoch: [34] [   3/  20] time: 181.6737, loss: 0.00001327\n",
      "Epoch: [34] [   4/  20] time: 181.9132, loss: 0.00002751\n",
      "Epoch: [34] [   5/  20] time: 182.1527, loss: 0.00000173\n",
      "Epoch: [34] [   6/  20] time: 182.3935, loss: 0.00003684\n",
      "Epoch: [34] [   7/  20] time: 182.6340, loss: 0.00013585\n",
      "Epoch: [34] [   8/  20] time: 182.8732, loss: 0.00000312\n",
      "Epoch: [34] [   9/  20] time: 183.1130, loss: 0.00023640\n",
      "Epoch: [34] [  10/  20] time: 183.3531, loss: 0.00002010\n",
      "Epoch: [34] [  11/  20] time: 183.5943, loss: 0.00037745\n",
      "Epoch: [34] [  12/  20] time: 183.8348, loss: 0.00004873\n",
      "Epoch: [34] [  13/  20] time: 184.0756, loss: 0.00001549\n",
      "Epoch: [34] [  14/  20] time: 184.3163, loss: 0.00000940\n",
      "Epoch: [34] [  15/  20] time: 184.5588, loss: 0.00015211\n",
      "Epoch: [34] [  16/  20] time: 184.7982, loss: 0.00011934\n",
      "Epoch: [34] [  17/  20] time: 185.0382, loss: 0.00001066\n",
      "Epoch: [34] [  18/  20] time: 185.2796, loss: 0.00000118\n",
      "Epoch: [34] [  19/  20] time: 185.5207, loss: 0.00000395\n",
      "[34/50] - ptime: 5.0770 loss: 0.00007318 acc: 0.70000 lr: 0.00072900\n",
      "Epoch: [35] [   0/  20] time: 186.4062, loss: 0.00002125\n",
      "Epoch: [35] [   1/  20] time: 186.6444, loss: 0.00001476\n",
      "Epoch: [35] [   2/  20] time: 186.8851, loss: 0.00001137\n",
      "Epoch: [35] [   3/  20] time: 187.1246, loss: 0.00004868\n",
      "Epoch: [35] [   4/  20] time: 187.3655, loss: 0.00000337\n",
      "Epoch: [35] [   5/  20] time: 187.6061, loss: 0.00000406\n",
      "Epoch: [35] [   6/  20] time: 187.8463, loss: 0.00000284\n",
      "Epoch: [35] [   7/  20] time: 188.0870, loss: 0.00011498\n",
      "Epoch: [35] [   8/  20] time: 188.3278, loss: 0.00001122\n",
      "Epoch: [35] [   9/  20] time: 188.5719, loss: 0.00000515\n",
      "Epoch: [35] [  10/  20] time: 188.8123, loss: 0.00000533\n",
      "Epoch: [35] [  11/  20] time: 189.0522, loss: 0.00003906\n",
      "Epoch: [35] [  12/  20] time: 189.2931, loss: 0.00001936\n",
      "Epoch: [35] [  13/  20] time: 189.5362, loss: 0.00022833\n",
      "Epoch: [35] [  14/  20] time: 189.7754, loss: 0.00000249\n",
      "Epoch: [35] [  15/  20] time: 190.0156, loss: 0.00001160\n",
      "Epoch: [35] [  16/  20] time: 190.2555, loss: 0.00004340\n",
      "Epoch: [35] [  17/  20] time: 190.4975, loss: 0.00001258\n",
      "Epoch: [35] [  18/  20] time: 190.7377, loss: 0.00002227\n",
      "Epoch: [35] [  19/  20] time: 190.9752, loss: 0.00000082\n",
      "[35/50] - ptime: 5.0823 loss: 0.00003115 acc: 0.70000 lr: 0.00072900\n",
      "Epoch: [36] [   0/  20] time: 191.8328, loss: 0.00000534\n",
      "Epoch: [36] [   1/  20] time: 192.0756, loss: 0.00001749\n",
      "Epoch: [36] [   2/  20] time: 192.3262, loss: 0.00000934\n",
      "Epoch: [36] [   3/  20] time: 192.5674, loss: 0.00000678\n",
      "Epoch: [36] [   4/  20] time: 192.8065, loss: 0.00000396\n",
      "Epoch: [36] [   5/  20] time: 193.0457, loss: 0.00003432\n",
      "Epoch: [36] [   6/  20] time: 193.2860, loss: 0.00000844\n",
      "Epoch: [36] [   7/  20] time: 193.5280, loss: 0.00000300\n",
      "Epoch: [36] [   8/  20] time: 193.7691, loss: 0.00016539\n",
      "Epoch: [36] [   9/  20] time: 194.0081, loss: 0.00002076\n",
      "Epoch: [36] [  10/  20] time: 194.2624, loss: 0.00001893\n",
      "Epoch: [36] [  11/  20] time: 194.5069, loss: 0.00001355\n",
      "Epoch: [36] [  12/  20] time: 194.7475, loss: 0.00019814\n",
      "Epoch: [36] [  13/  20] time: 194.9901, loss: 0.00002189\n",
      "Epoch: [36] [  14/  20] time: 195.2301, loss: 0.00001642\n",
      "Epoch: [36] [  15/  20] time: 195.4705, loss: 0.00001153\n",
      "Epoch: [36] [  16/  20] time: 195.7105, loss: 0.00003460\n",
      "Epoch: [36] [  17/  20] time: 195.9515, loss: 0.00004409\n",
      "Epoch: [36] [  18/  20] time: 196.1910, loss: 0.00008310\n",
      "Epoch: [36] [  19/  20] time: 196.4340, loss: 0.00001252\n",
      "[36/50] - ptime: 5.1123 loss: 0.00003648 acc: 0.70000 lr: 0.00072900\n",
      "Epoch: [37] [   0/  20] time: 197.2949, loss: 0.00000137\n",
      "Epoch: [37] [   1/  20] time: 197.5361, loss: 0.00001096\n",
      "Epoch: [37] [   2/  20] time: 197.7767, loss: 0.00001059\n",
      "Epoch: [37] [   3/  20] time: 198.0168, loss: 0.00001881\n",
      "Epoch: [37] [   4/  20] time: 198.2572, loss: 0.00000614\n",
      "Epoch: [37] [   5/  20] time: 198.5000, loss: 0.00000639\n",
      "Epoch: [37] [   6/  20] time: 198.7399, loss: 0.00008039\n",
      "Epoch: [37] [   7/  20] time: 198.9800, loss: 0.00000086\n",
      "Epoch: [37] [   8/  20] time: 199.2201, loss: 0.00001532\n",
      "Epoch: [37] [   9/  20] time: 199.4630, loss: 0.00000750\n",
      "Epoch: [37] [  10/  20] time: 199.7025, loss: 0.00000357\n",
      "Epoch: [37] [  11/  20] time: 199.9428, loss: 0.00001771\n",
      "Epoch: [37] [  12/  20] time: 200.1853, loss: 0.00002316\n",
      "Epoch: [37] [  13/  20] time: 200.4264, loss: 0.00002463\n",
      "Epoch: [37] [  14/  20] time: 200.6668, loss: 0.00000552\n",
      "Epoch: [37] [  15/  20] time: 200.9070, loss: 0.00002994\n",
      "Epoch: [37] [  16/  20] time: 201.1473, loss: 0.00000962\n",
      "Epoch: [37] [  17/  20] time: 201.3877, loss: 0.00000301\n",
      "Epoch: [37] [  18/  20] time: 201.6291, loss: 0.00001168\n",
      "Epoch: [37] [  19/  20] time: 201.8688, loss: 0.00000981\n",
      "[37/50] - ptime: 5.0861 loss: 0.00001485 acc: 0.68000 lr: 0.00072900\n",
      "Epoch: [38] [   0/  20] time: 202.7333, loss: 0.00001728\n",
      "Epoch: [38] [   1/  20] time: 202.9745, loss: 0.00001349\n",
      "Epoch: [38] [   2/  20] time: 203.2145, loss: 0.00003502\n",
      "Epoch: [38] [   3/  20] time: 203.4569, loss: 0.00000493\n",
      "Epoch: [38] [   4/  20] time: 203.6963, loss: 0.00004134\n",
      "Epoch: [38] [   5/  20] time: 203.9366, loss: 0.00000252\n",
      "Epoch: [38] [   6/  20] time: 204.1765, loss: 0.00001489\n",
      "Epoch: [38] [   7/  20] time: 204.4184, loss: 0.00001259\n",
      "Epoch: [38] [   8/  20] time: 204.6601, loss: 0.00001649\n",
      "Epoch: [38] [   9/  20] time: 204.8998, loss: 0.00000813\n",
      "Epoch: [38] [  10/  20] time: 205.1399, loss: 0.00004739\n",
      "Epoch: [38] [  11/  20] time: 205.3801, loss: 0.00000339\n",
      "Epoch: [38] [  12/  20] time: 205.6206, loss: 0.00015690\n",
      "Epoch: [38] [  13/  20] time: 205.8601, loss: 0.00001541\n",
      "Epoch: [38] [  14/  20] time: 206.1006, loss: 0.00000598\n",
      "Epoch: [38] [  15/  20] time: 206.3407, loss: 0.00000137\n",
      "Epoch: [38] [  16/  20] time: 206.5831, loss: 0.00000373\n",
      "Epoch: [38] [  17/  20] time: 206.8290, loss: 0.00000534\n",
      "Epoch: [38] [  18/  20] time: 207.0685, loss: 0.00002234\n",
      "Epoch: [38] [  19/  20] time: 207.3085, loss: 0.00000928\n",
      "[38/50] - ptime: 5.0873 loss: 0.00002189 acc: 0.68000 lr: 0.00072900\n",
      "Epoch: [39] [   0/  20] time: 208.1690, loss: 0.00002143\n",
      "Epoch: [39] [   1/  20] time: 208.4077, loss: 0.00000329\n",
      "Epoch: [39] [   2/  20] time: 208.6489, loss: 0.00002903\n",
      "Epoch: [39] [   3/  20] time: 208.8886, loss: 0.00000607\n",
      "Epoch: [39] [   4/  20] time: 209.1300, loss: 0.00000347\n",
      "Epoch: [39] [   5/  20] time: 209.3699, loss: 0.00000173\n",
      "Epoch: [39] [   6/  20] time: 209.6117, loss: 0.00004495\n",
      "Epoch: [39] [   7/  20] time: 209.8530, loss: 0.00000331\n",
      "Epoch: [39] [   8/  20] time: 210.0924, loss: 0.00000064\n",
      "Epoch: [39] [   9/  20] time: 210.3327, loss: 0.00000648\n",
      "Epoch: [39] [  10/  20] time: 210.5755, loss: 0.00000181\n",
      "Epoch: [39] [  11/  20] time: 210.8157, loss: 0.00002701\n",
      "Epoch: [39] [  12/  20] time: 211.0544, loss: 0.00002163\n",
      "Epoch: [39] [  13/  20] time: 211.2948, loss: 0.00001231\n",
      "Epoch: [39] [  14/  20] time: 211.5363, loss: 0.00004181\n",
      "Epoch: [39] [  15/  20] time: 211.7764, loss: 0.00026166\n",
      "Epoch: [39] [  16/  20] time: 212.0203, loss: 0.00000143\n",
      "Epoch: [39] [  17/  20] time: 212.2597, loss: 0.00015624\n",
      "Epoch: [39] [  18/  20] time: 212.5032, loss: 0.00000452\n",
      "Epoch: [39] [  19/  20] time: 212.7429, loss: 0.00002805\n",
      "[39/50] - ptime: 5.0847 loss: 0.00003384 acc: 0.67000 lr: 0.00072900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40] [   0/  20] time: 213.6075, loss: 0.00004192\n",
      "Epoch: [40] [   1/  20] time: 213.8458, loss: 0.00001009\n",
      "Epoch: [40] [   2/  20] time: 214.0866, loss: 0.00000201\n",
      "Epoch: [40] [   3/  20] time: 214.3270, loss: 0.00000065\n",
      "Epoch: [40] [   4/  20] time: 214.5689, loss: 0.00000401\n",
      "Epoch: [40] [   5/  20] time: 214.8089, loss: 0.00003225\n",
      "Epoch: [40] [   6/  20] time: 215.0497, loss: 0.00000431\n",
      "Epoch: [40] [   7/  20] time: 215.2895, loss: 0.00001596\n",
      "Epoch: [40] [   8/  20] time: 215.5326, loss: 0.00000974\n",
      "Epoch: [40] [   9/  20] time: 215.7723, loss: 0.00000683\n",
      "Epoch: [40] [  10/  20] time: 216.0130, loss: 0.00004581\n",
      "Epoch: [40] [  11/  20] time: 216.2528, loss: 0.00000914\n",
      "Epoch: [40] [  12/  20] time: 216.4960, loss: 0.00000510\n",
      "Epoch: [40] [  13/  20] time: 216.7371, loss: 0.00000441\n",
      "Epoch: [40] [  14/  20] time: 216.9777, loss: 0.00001122\n",
      "Epoch: [40] [  15/  20] time: 217.2190, loss: 0.00004104\n",
      "Epoch: [40] [  16/  20] time: 217.4558, loss: 0.00001245\n",
      "Epoch: [40] [  17/  20] time: 217.6959, loss: 0.00000121\n",
      "Epoch: [40] [  18/  20] time: 217.9355, loss: 0.00001299\n",
      "Epoch: [40] [  19/  20] time: 218.1748, loss: 0.00007363\n",
      "[40/50] - ptime: 5.0796 loss: 0.00001724 acc: 0.67000 lr: 0.00065610\n",
      "Epoch: [41] [   0/  20] time: 219.0553, loss: 0.00000155\n",
      "Epoch: [41] [   1/  20] time: 219.2927, loss: 0.00000837\n",
      "Epoch: [41] [   2/  20] time: 219.5332, loss: 0.00001645\n",
      "Epoch: [41] [   3/  20] time: 219.7717, loss: 0.00001161\n",
      "Epoch: [41] [   4/  20] time: 220.0118, loss: 0.00001884\n",
      "Epoch: [41] [   5/  20] time: 220.2518, loss: 0.00001505\n",
      "Epoch: [41] [   6/  20] time: 220.4935, loss: 0.00000429\n",
      "Epoch: [41] [   7/  20] time: 220.7334, loss: 0.00001509\n",
      "Epoch: [41] [   8/  20] time: 220.9734, loss: 0.00003069\n",
      "Epoch: [41] [   9/  20] time: 221.2135, loss: 0.00001463\n",
      "Epoch: [41] [  10/  20] time: 221.4551, loss: 0.00000776\n",
      "Epoch: [41] [  11/  20] time: 221.6952, loss: 0.00002067\n",
      "Epoch: [41] [  12/  20] time: 221.9352, loss: 0.00000094\n",
      "Epoch: [41] [  13/  20] time: 222.1761, loss: 0.00001646\n",
      "Epoch: [41] [  14/  20] time: 222.4180, loss: 0.00000719\n",
      "Epoch: [41] [  15/  20] time: 222.6577, loss: 0.00000214\n",
      "Epoch: [41] [  16/  20] time: 222.8987, loss: 0.00000588\n",
      "Epoch: [41] [  17/  20] time: 223.1402, loss: 0.00000672\n",
      "Epoch: [41] [  18/  20] time: 223.3826, loss: 0.00000623\n",
      "Epoch: [41] [  19/  20] time: 223.6250, loss: 0.00000273\n",
      "[41/50] - ptime: 5.0808 loss: 0.00001066 acc: 0.67000 lr: 0.00065610\n",
      "Epoch: [42] [   0/  20] time: 224.4943, loss: 0.00000270\n",
      "Epoch: [42] [   1/  20] time: 224.7318, loss: 0.00002010\n",
      "Epoch: [42] [   2/  20] time: 224.9707, loss: 0.00042700\n",
      "Epoch: [42] [   3/  20] time: 225.2098, loss: 0.00000062\n",
      "Epoch: [42] [   4/  20] time: 225.4508, loss: 0.00003131\n",
      "Epoch: [42] [   5/  20] time: 225.6909, loss: 0.00000262\n",
      "Epoch: [42] [   6/  20] time: 225.9300, loss: 0.00001594\n",
      "Epoch: [42] [   7/  20] time: 226.1699, loss: 0.00000298\n",
      "Epoch: [42] [   8/  20] time: 226.4120, loss: 0.00002169\n",
      "Epoch: [42] [   9/  20] time: 226.6540, loss: 0.00000250\n",
      "Epoch: [42] [  10/  20] time: 226.8942, loss: 0.00005346\n",
      "Epoch: [42] [  11/  20] time: 227.1336, loss: 0.00000961\n",
      "Epoch: [42] [  12/  20] time: 227.3747, loss: 0.00004017\n",
      "Epoch: [42] [  13/  20] time: 227.6150, loss: 0.00000728\n",
      "Epoch: [42] [  14/  20] time: 227.8549, loss: 0.00000514\n",
      "Epoch: [42] [  15/  20] time: 228.0957, loss: 0.00001783\n",
      "Epoch: [42] [  16/  20] time: 228.3361, loss: 0.00000689\n",
      "Epoch: [42] [  17/  20] time: 228.5783, loss: 0.00001963\n",
      "Epoch: [42] [  18/  20] time: 228.8183, loss: 0.00009469\n",
      "Epoch: [42] [  19/  20] time: 229.0585, loss: 0.00000428\n",
      "[42/50] - ptime: 5.0792 loss: 0.00003932 acc: 0.67000 lr: 0.00065610\n",
      "Epoch: [43] [   0/  20] time: 229.9590, loss: 0.00000768\n",
      "Epoch: [43] [   1/  20] time: 230.1974, loss: 0.00000383\n",
      "Epoch: [43] [   2/  20] time: 230.4407, loss: 0.00000210\n",
      "Epoch: [43] [   3/  20] time: 230.6814, loss: 0.00000261\n",
      "Epoch: [43] [   4/  20] time: 230.9218, loss: 0.00000457\n",
      "Epoch: [43] [   5/  20] time: 231.1614, loss: 0.00000378\n",
      "Epoch: [43] [   6/  20] time: 231.4035, loss: 0.00000250\n",
      "Epoch: [43] [   7/  20] time: 231.6450, loss: 0.00000285\n",
      "Epoch: [43] [   8/  20] time: 231.8855, loss: 0.00000166\n",
      "Epoch: [43] [   9/  20] time: 232.1264, loss: 0.00000667\n",
      "Epoch: [43] [  10/  20] time: 232.3667, loss: 0.00000670\n",
      "Epoch: [43] [  11/  20] time: 232.6105, loss: 0.00005591\n",
      "Epoch: [43] [  12/  20] time: 232.8555, loss: 0.00004534\n",
      "Epoch: [43] [  13/  20] time: 233.0944, loss: 0.00002382\n",
      "Epoch: [43] [  14/  20] time: 233.3339, loss: 0.00003473\n",
      "Epoch: [43] [  15/  20] time: 233.5757, loss: 0.00002820\n",
      "Epoch: [43] [  16/  20] time: 233.8150, loss: 0.00026832\n",
      "Epoch: [43] [  17/  20] time: 234.0548, loss: 0.00003123\n",
      "Epoch: [43] [  18/  20] time: 234.2948, loss: 0.00000123\n",
      "Epoch: [43] [  19/  20] time: 234.5358, loss: 0.00001780\n",
      "[43/50] - ptime: 5.0962 loss: 0.00002758 acc: 0.66000 lr: 0.00065610\n",
      "Epoch: [44] [   0/  20] time: 235.3949, loss: 0.00000560\n",
      "Epoch: [44] [   1/  20] time: 235.6357, loss: 0.00000674\n",
      "Epoch: [44] [   2/  20] time: 235.8754, loss: 0.00000707\n",
      "Epoch: [44] [   3/  20] time: 236.1168, loss: 0.00000737\n",
      "Epoch: [44] [   4/  20] time: 236.3570, loss: 0.00000765\n",
      "Epoch: [44] [   5/  20] time: 236.6005, loss: 0.00000418\n",
      "Epoch: [44] [   6/  20] time: 236.8404, loss: 0.00001010\n",
      "Epoch: [44] [   7/  20] time: 237.0808, loss: 0.00002491\n",
      "Epoch: [44] [   8/  20] time: 237.3211, loss: 0.00000637\n",
      "Epoch: [44] [   9/  20] time: 237.5628, loss: 0.00000489\n",
      "Epoch: [44] [  10/  20] time: 237.8038, loss: 0.00000148\n",
      "Epoch: [44] [  11/  20] time: 238.0440, loss: 0.00001864\n",
      "Epoch: [44] [  12/  20] time: 238.2849, loss: 0.00000469\n",
      "Epoch: [44] [  13/  20] time: 238.5295, loss: 0.00000258\n",
      "Epoch: [44] [  14/  20] time: 238.7694, loss: 0.00003276\n",
      "Epoch: [44] [  15/  20] time: 239.0087, loss: 0.00001949\n",
      "Epoch: [44] [  16/  20] time: 239.2491, loss: 0.00000207\n",
      "Epoch: [44] [  17/  20] time: 239.4906, loss: 0.00002645\n",
      "Epoch: [44] [  18/  20] time: 239.7298, loss: 0.00000337\n",
      "Epoch: [44] [  19/  20] time: 239.9708, loss: 0.00004657\n",
      "[44/50] - ptime: 5.0881 loss: 0.00001215 acc: 0.67000 lr: 0.00065610\n",
      "Epoch: [45] [   0/  20] time: 240.8314, loss: 0.00000356\n",
      "Epoch: [45] [   1/  20] time: 241.0691, loss: 0.00000205\n",
      "Epoch: [45] [   2/  20] time: 241.3100, loss: 0.00000334\n",
      "Epoch: [45] [   3/  20] time: 241.5523, loss: 0.00000076\n",
      "Epoch: [45] [   4/  20] time: 241.7921, loss: 0.00000368\n",
      "Epoch: [45] [   5/  20] time: 242.0318, loss: 0.00003314\n",
      "Epoch: [45] [   6/  20] time: 242.2713, loss: 0.00000288\n",
      "Epoch: [45] [   7/  20] time: 242.5132, loss: 0.00000580\n",
      "Epoch: [45] [   8/  20] time: 242.7535, loss: 0.00000463\n",
      "Epoch: [45] [   9/  20] time: 242.9962, loss: 0.00000111\n",
      "Epoch: [45] [  10/  20] time: 243.2361, loss: 0.00000653\n",
      "Epoch: [45] [  11/  20] time: 243.4772, loss: 0.00000120\n",
      "Epoch: [45] [  12/  20] time: 243.7176, loss: 0.00001755\n",
      "Epoch: [45] [  13/  20] time: 243.9582, loss: 0.00008812\n",
      "Epoch: [45] [  14/  20] time: 244.1982, loss: 0.00001459\n",
      "Epoch: [45] [  15/  20] time: 244.4388, loss: 0.00003633\n",
      "Epoch: [45] [  16/  20] time: 244.6789, loss: 0.00000954\n",
      "Epoch: [45] [  17/  20] time: 244.9191, loss: 0.00000268\n",
      "Epoch: [45] [  18/  20] time: 245.1586, loss: 0.00005897\n",
      "Epoch: [45] [  19/  20] time: 245.3990, loss: 0.00000290\n",
      "[45/50] - ptime: 5.0798 loss: 0.00001497 acc: 0.67000 lr: 0.00065610\n",
      "Epoch: [46] [   0/  20] time: 246.2628, loss: 0.00002842\n",
      "Epoch: [46] [   1/  20] time: 246.5042, loss: 0.00000096\n",
      "Epoch: [46] [   2/  20] time: 246.7436, loss: 0.00000048\n",
      "Epoch: [46] [   3/  20] time: 246.9835, loss: 0.00000807\n",
      "Epoch: [46] [   4/  20] time: 247.2239, loss: 0.00000141\n",
      "Epoch: [46] [   5/  20] time: 247.4666, loss: 0.00001708\n",
      "Epoch: [46] [   6/  20] time: 247.7070, loss: 0.00000125\n",
      "Epoch: [46] [   7/  20] time: 247.9469, loss: 0.00001562\n",
      "Epoch: [46] [   8/  20] time: 248.1898, loss: 0.00000884\n",
      "Epoch: [46] [   9/  20] time: 248.4304, loss: 0.00007401\n",
      "Epoch: [46] [  10/  20] time: 248.6730, loss: 0.00000243\n",
      "Epoch: [46] [  11/  20] time: 248.9139, loss: 0.00000070\n",
      "Epoch: [46] [  12/  20] time: 249.1541, loss: 0.00001448\n",
      "Epoch: [46] [  13/  20] time: 249.3954, loss: 0.00000227\n",
      "Epoch: [46] [  14/  20] time: 249.6405, loss: 0.00000114\n",
      "Epoch: [46] [  15/  20] time: 249.8810, loss: 0.00000107\n",
      "Epoch: [46] [  16/  20] time: 250.1210, loss: 0.00000505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46] [  17/  20] time: 250.3642, loss: 0.00000649\n",
      "Epoch: [46] [  18/  20] time: 250.6043, loss: 0.00000456\n",
      "Epoch: [46] [  19/  20] time: 250.8443, loss: 0.00001376\n",
      "[46/50] - ptime: 5.0945 loss: 0.00001040 acc: 0.67000 lr: 0.00065610\n",
      "Epoch: [47] [   0/  20] time: 251.7024, loss: 0.00002575\n",
      "Epoch: [47] [   1/  20] time: 251.9400, loss: 0.00001336\n",
      "Epoch: [47] [   2/  20] time: 252.1797, loss: 0.00000260\n",
      "Epoch: [47] [   3/  20] time: 252.4235, loss: 0.00001338\n",
      "Epoch: [47] [   4/  20] time: 252.6647, loss: 0.00006500\n",
      "Epoch: [47] [   5/  20] time: 252.9048, loss: 0.00002754\n",
      "Epoch: [47] [   6/  20] time: 253.1449, loss: 0.00001016\n",
      "Epoch: [47] [   7/  20] time: 253.3863, loss: 0.00016731\n",
      "Epoch: [47] [   8/  20] time: 253.6266, loss: 0.00004619\n",
      "Epoch: [47] [   9/  20] time: 253.8676, loss: 0.00004071\n",
      "Epoch: [47] [  10/  20] time: 254.1072, loss: 0.00000938\n",
      "Epoch: [47] [  11/  20] time: 254.3479, loss: 0.00000397\n",
      "Epoch: [47] [  12/  20] time: 254.5885, loss: 0.00000328\n",
      "Epoch: [47] [  13/  20] time: 254.8290, loss: 0.00001076\n",
      "Epoch: [47] [  14/  20] time: 255.0694, loss: 0.00000476\n",
      "Epoch: [47] [  15/  20] time: 255.3095, loss: 0.00000118\n",
      "Epoch: [47] [  16/  20] time: 255.5553, loss: 0.00000290\n",
      "Epoch: [47] [  17/  20] time: 255.7956, loss: 0.00000821\n",
      "Epoch: [47] [  18/  20] time: 256.0366, loss: 0.00000835\n",
      "Epoch: [47] [  19/  20] time: 256.2765, loss: 0.00002897\n",
      "[47/50] - ptime: 5.0874 loss: 0.00002469 acc: 0.67000 lr: 0.00065610\n",
      "Epoch: [48] [   0/  20] time: 257.2425, loss: 0.00001429\n",
      "Epoch: [48] [   1/  20] time: 257.4813, loss: 0.00000181\n",
      "Epoch: [48] [   2/  20] time: 257.7231, loss: 0.00000822\n",
      "Epoch: [48] [   3/  20] time: 257.9634, loss: 0.00004499\n",
      "Epoch: [48] [   4/  20] time: 258.2049, loss: 0.00000221\n",
      "Epoch: [48] [   5/  20] time: 258.4463, loss: 0.00000692\n",
      "Epoch: [48] [   6/  20] time: 258.6882, loss: 0.00001955\n",
      "Epoch: [48] [   7/  20] time: 258.9282, loss: 0.00000139\n",
      "Epoch: [48] [   8/  20] time: 259.1692, loss: 0.00002958\n",
      "Epoch: [48] [   9/  20] time: 259.4105, loss: 0.00001481\n",
      "Epoch: [48] [  10/  20] time: 259.6516, loss: 0.00005399\n",
      "Epoch: [48] [  11/  20] time: 259.8912, loss: 0.00000117\n",
      "Epoch: [48] [  12/  20] time: 260.1315, loss: 0.00000481\n",
      "Epoch: [48] [  13/  20] time: 260.3723, loss: 0.00000620\n",
      "Epoch: [48] [  14/  20] time: 260.6131, loss: 0.00000756\n",
      "Epoch: [48] [  15/  20] time: 260.8539, loss: 0.00000742\n",
      "Epoch: [48] [  16/  20] time: 261.0949, loss: 0.00000615\n",
      "Epoch: [48] [  17/  20] time: 261.3351, loss: 0.00000955\n",
      "Epoch: [48] [  18/  20] time: 261.5784, loss: 0.00004318\n",
      "Epoch: [48] [  19/  20] time: 261.8186, loss: 0.00000533\n",
      "[48/50] - ptime: 5.0876 loss: 0.00001446 acc: 0.67000 lr: 0.00065610\n",
      "Epoch: [49] [   0/  20] time: 262.6742, loss: 0.00003046\n",
      "Epoch: [49] [   1/  20] time: 262.9130, loss: 0.00000167\n",
      "Epoch: [49] [   2/  20] time: 263.1528, loss: 0.00000195\n",
      "Epoch: [49] [   3/  20] time: 263.3933, loss: 0.00000436\n",
      "Epoch: [49] [   4/  20] time: 263.6347, loss: 0.00002272\n",
      "Epoch: [49] [   5/  20] time: 263.8742, loss: 0.00000088\n",
      "Epoch: [49] [   6/  20] time: 264.1138, loss: 0.00000094\n",
      "Epoch: [49] [   7/  20] time: 264.3534, loss: 0.00000234\n",
      "Epoch: [49] [   8/  20] time: 264.5992, loss: 0.00004452\n",
      "Epoch: [49] [   9/  20] time: 264.8382, loss: 0.00000305\n",
      "Epoch: [49] [  10/  20] time: 265.0777, loss: 0.00000867\n",
      "Epoch: [49] [  11/  20] time: 265.3177, loss: 0.00000092\n",
      "Epoch: [49] [  12/  20] time: 265.5590, loss: 0.00000647\n",
      "Epoch: [49] [  13/  20] time: 265.7975, loss: 0.00000958\n",
      "Epoch: [49] [  14/  20] time: 266.0381, loss: 0.00000983\n",
      "Epoch: [49] [  15/  20] time: 266.2769, loss: 0.00005340\n",
      "Epoch: [49] [  16/  20] time: 266.5177, loss: 0.00002269\n",
      "Epoch: [49] [  17/  20] time: 266.7575, loss: 0.00002627\n",
      "Epoch: [49] [  18/  20] time: 266.9972, loss: 0.00000435\n",
      "Epoch: [49] [  19/  20] time: 267.2365, loss: 0.00000388\n",
      "[49/50] - ptime: 5.0766 loss: 0.00001295 acc: 0.67000 lr: 0.00065610\n",
      "Epoch: [50] [   0/  20] time: 268.1326, loss: 0.00000608\n",
      "Epoch: [50] [   1/  20] time: 268.3712, loss: 0.00000117\n",
      "Epoch: [50] [   2/  20] time: 268.6123, loss: 0.00000733\n",
      "Epoch: [50] [   3/  20] time: 268.8522, loss: 0.00000950\n",
      "Epoch: [50] [   4/  20] time: 269.0916, loss: 0.00000296\n",
      "Epoch: [50] [   5/  20] time: 269.3316, loss: 0.00000159\n",
      "Epoch: [50] [   6/  20] time: 269.5737, loss: 0.00000241\n",
      "Epoch: [50] [   7/  20] time: 269.8141, loss: 0.00001925\n",
      "Epoch: [50] [   8/  20] time: 270.0551, loss: 0.00000118\n",
      "Epoch: [50] [   9/  20] time: 270.2953, loss: 0.00002134\n",
      "Epoch: [50] [  10/  20] time: 270.5355, loss: 0.00000595\n",
      "Epoch: [50] [  11/  20] time: 270.7746, loss: 0.00008266\n",
      "Epoch: [50] [  12/  20] time: 271.0136, loss: 0.00006488\n",
      "Epoch: [50] [  13/  20] time: 271.2531, loss: 0.00000190\n",
      "Epoch: [50] [  14/  20] time: 271.4935, loss: 0.00001452\n",
      "Epoch: [50] [  15/  20] time: 271.7335, loss: 0.00000276\n",
      "Epoch: [50] [  16/  20] time: 271.9724, loss: 0.00000456\n",
      "Epoch: [50] [  17/  20] time: 272.2123, loss: 0.00003400\n",
      "Epoch: [50] [  18/  20] time: 272.4525, loss: 0.00024960\n",
      "Epoch: [50] [  19/  20] time: 272.6916, loss: 0.00004667\n",
      "[50/50] - ptime: 5.0698 loss: 0.00002902 acc: 0.65000 lr: 0.00059049\n",
      "Avg per epoch ptime: 5.10, total 50 epochs ptime: 273.10\n",
      " [*] Training finished!\n",
      " [*] Best Epoch:  18 , Accuracy:  0.8500000238418579\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay-18\n",
      " [*] Finished testing Best Epoch: 18 , accuracy:  0.8500000238418579 !\n"
     ]
    }
   ],
   "source": [
    "dataset = '4_Flowers_1s'\n",
    "epoch = 50\n",
    "batch_size = 100\n",
    "checkpoint_dir = 'checkpoint'\n",
    "log_dir = 'logs'\n",
    "trainhist_dir = 'trainhist'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "# --log_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# --trainhist_dir\n",
    "if not os.path.exists(trainhist_dir):\n",
    "    os.makedirs(trainhist_dir)\n",
    "\n",
    "# open session\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    \n",
    "    # declare instance for GAN\n",
    "    CNN = CNN(sess, epoch=epoch, batch_size=batch_size, dataset_name=dataset, checkpoint_dir=checkpoint_dir, \n",
    "                log_dir=log_dir, trainhist_dir=trainhist_dir)\n",
    "\n",
    "    # build graph\n",
    "    CNN.build_model()\n",
    "\n",
    "    # show network architecture\n",
    "    CNN.show_all_variables()\n",
    "\n",
    "    # launch the graph in a session\n",
    "    CNN.train()\n",
    "    \n",
    "#     CNN.test(epoch)\n",
    "        \n",
    "sess.close()\n",
    "        \n",
    "# lrdecay\n",
    "#  [*] Best Epoch:  28 , Accuracy:  0.7999999523162842\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay-28\n",
    "#  [*] Finished testing Best Epoch: 28 , accuracy:  0.7999999523162842 !\n",
    "\n",
    "# Avg per epoch ptime: 5.10, total 50 epochs ptime: 273.10\n",
    "#  [*] Training finished!\n",
    "#  [*] Best Epoch:  18 , Accuracy:  0.8500000238418579\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay-18\n",
    "#  [*] Finished testing Best Epoch: 18 , accuracy:  0.8500000238418579 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "file='/home/huiqy/Music/CloudMusic/All_Time_Low.mp3' #文件名是完整路径名\n",
    "pygame.mixer.init() #初始化音频\n",
    "track = pygame.mixer.music.load(file)#载入音乐文件\n",
    "pygame.mixer.music.play()#开始播放\n",
    "time.sleep(60)#播放10秒\n",
    "pygame.mixer.music.stop()#停止播放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
