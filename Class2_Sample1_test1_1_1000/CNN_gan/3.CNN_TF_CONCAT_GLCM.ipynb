{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imutils import paths\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import argparse\n",
    "import imutils,sklearn\n",
    "import os, cv2, re, random, shutil, imageio, pickle\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #定义最大灰度级数\n",
    "# gray_level = 128\n",
    "\n",
    "# def maxGrayLevel(img):\n",
    "#     max_gray_level=0\n",
    "#     (height,width)=img.shape\n",
    "#     for y in range(height):\n",
    "#         for x in range(width):\n",
    "#             if img[y][x] > max_gray_level:\n",
    "#                 max_gray_level = img[y][x]\n",
    "#     return max_gray_level+1\n",
    "\n",
    "# def getGlcm(input,d_x,d_y):\n",
    "#     srcdata=input.copy()\n",
    "#     ret=[[0.0 for i in range(gray_level)] for j in range(gray_level)]\n",
    "#     (height,width) = input.shape\n",
    "\n",
    "#     max_gray_level=maxGrayLevel(input)\n",
    "\n",
    "#     #若灰度级数大于gray_level，则将图像的灰度级缩小至gray_level，减小灰度共生矩阵的大小\n",
    "#     if max_gray_level > gray_level:\n",
    "#         for j in range(height):\n",
    "#             for i in range(width):\n",
    "#                 srcdata[j][i] = srcdata[j][i]*gray_level / max_gray_level\n",
    "\n",
    "#     for j in range(height-d_y):\n",
    "#         for i in range(width-d_x):\n",
    "#             rows = srcdata[j][i]\n",
    "#             cols = srcdata[j + d_y][i+d_x]\n",
    "#             ret[rows][cols]+=1.0\n",
    "\n",
    "#     for i in range(gray_level):\n",
    "#         for j in range(gray_level):\n",
    "#             ret[i][j]/=float(height*width)\n",
    "\n",
    "#     return np.array(ret)\n",
    "    \n",
    "# def batch_GLCM(images):\n",
    "#     greycomatrix_list = []\n",
    "#     for i in tqdm(range(len(images))):\n",
    "#         img = (images.astype(np.float32)* 255)[i, :, :, :].astype(np.uint8)\n",
    "#         gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         glcm_list = []\n",
    "        \n",
    "#         glcm_0=getGlcm(gray, 1,0)\n",
    "#         glcm_0=cv2.normalize(glcm_0, glcm_0, 0, 1, cv2.NORM_MINMAX) \n",
    "#         glcm_list.append(glcm_0)\n",
    "        \n",
    "#         glcm_1=getGlcm(gray, 0,1)\n",
    "#         glcm_1=cv2.normalize(glcm_1, glcm_1, 0, 1, cv2.NORM_MINMAX) \n",
    "#         glcm_list.append(glcm_1)\n",
    "        \n",
    "#         glcm_2=getGlcm(gray, 1,1)\n",
    "#         glcm_2=cv2.normalize(glcm_2, glcm_2, 0, 1, cv2.NORM_MINMAX) \n",
    "#         glcm_list.append(glcm_2)\n",
    "\n",
    "#         glcm_list=np.array(glcm_list,dtype = float32)\n",
    "#         greycomatrix_list.append(glcm_list)\n",
    "        \n",
    "#     greycomatrix_list = np.array(greycomatrix_list,dtype = float32).reshape([images.shape[0],images.shape[1],images.shape[2],3)\n",
    "    \n",
    "#     print (greycomatrix_list.shape)\n",
    "#     return greycomatrix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def atoi(text):\n",
    "#     return int(text) if text.isdigit() else text\n",
    "\n",
    "# def natural_keys(text):\n",
    "#     return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# def load_flower_data():\n",
    "#     # grab the list of images that we'll be describing\n",
    "#     print(\"[INFO] handling images...\")\n",
    "#     TRAIN_ORIGINAL_DIR = '../train/'\n",
    "#     TRAIN_SUB_DIR = '../subsample/'\n",
    "#     TRAIN_GAN = '../../image_gan/'\n",
    "#     TEST_DIR = '../../test/'\n",
    "\n",
    "#     # use this for full dataset\n",
    "#     train_images_gan = [TRAIN_GAN + i for i in os.listdir(TRAIN_GAN)]\n",
    "#     test_images = [TEST_DIR + i for i in os.listdir(TEST_DIR)]\n",
    "    \n",
    "#     train_images = train_images_gan\n",
    "    \n",
    "#     train_images.sort(key=natural_keys)\n",
    "#     test_images.sort(key=natural_keys)\n",
    "\n",
    "#     # initialize the features matrix and labels list\n",
    "#     trainImage = []\n",
    "#     trainLabels = []\n",
    "#     testImage = []\n",
    "#     testLabels = []\n",
    "\n",
    "#     # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(train_images):\n",
    "#         # extract the class label\n",
    "#         # get the labels from the name of the images by extract the string before \"_\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # read and resize image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         trainImage.append(img)\n",
    "#         trainLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(train_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(train_images)))\n",
    "            \n",
    "#       # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(test_images):\n",
    "#         # extract the class label\n",
    "#         # our images were named as labels.image_number.format\n",
    "#         # get the labels from the name of the images by extract the string before \".\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # extract CNN features in the image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         testImage.append(img)\n",
    "#         testLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(test_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(test_images)))\n",
    "\n",
    "\n",
    "#     trainImage = np.array(trainImage,dtype = float32)\n",
    "#     trainLabels = np.array(trainLabels)\n",
    "#     testImage = np.array(testImage,dtype = float32)\n",
    "#     testLabels = np.array(testLabels)\n",
    "#     print (trainImage.shape)\n",
    "    \n",
    "#     trainImage = trainImage.astype(np.float32) / 255\n",
    "#     testImage = testImage.astype(np.float32) / 255\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(trainLabels)\n",
    "#     list(le.classes_)\n",
    "#     trainLabels = le.transform(trainLabels) \n",
    "#     testLabels = le.transform(testLabels) \n",
    "    \n",
    "#     return trainImage, trainLabels, testImage, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainImage, trainLabels, testImage, testLabels = load_flower_data()\n",
    "\n",
    "# trainImage_GLCM = batch_GLCM(trainImage)\n",
    "# testImage_GLCM  = batch_GLCM(testImage)\n",
    "# nb_classes = 2\n",
    "\n",
    "# # Convert class vectors to binary class matrices.\n",
    "# trainLabels = keras.utils.to_categorical(trainLabels, nb_classes)\n",
    "# print (trainLabels)\n",
    "# testLabels = keras.utils.to_categorical(testLabels, nb_classes)\n",
    "# print (testLabels)\n",
    "# print (testLabels.shape)\n",
    "\n",
    "# np.save('../trainImage.npy', trainImage)\n",
    "# np.save('../trainLabels.npy', trainLabels)\n",
    "# np.save('../testImage.npy', testImage)\n",
    "# np.save('../testLabels.npy', testLabels)\n",
    "# np.save('../trainImage_GLCM.npy', trainImage_GLCM)\n",
    "# np.save('../testImage_GLCM.npy', testImage_GLCM)\n",
    "\n",
    "# print(\"[INFO] trainImage matrix: {:.2f}MB\".format(\n",
    "#     (trainImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainLabels matrix: {:.4f}MB\".format(\n",
    "#     (trainLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage matrix: {:.2f}MB\".format(\n",
    "#     (testImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testLabels matrix: {:.4f}MB\".format(\n",
    "#     (testLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainImage_GLCM matrix: {:.2f}MB\".format(\n",
    "#     (trainImage_GLCM.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage_GLCM matrix: {:.4f}MB\".format(\n",
    "#     (testImage_GLCM.nbytes) / (1024 * 1000.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self, sess, epoch, batch_size, dataset_name, checkpoint_dir, log_dir, trainhist_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.trainhist_dir = trainhist_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.classname = ['Iris', 'Pansy']\n",
    "\n",
    "        # parameters\n",
    "        self.input_height = 128\n",
    "        self.input_width = 128\n",
    "        self.c_dim = 3  # color dimension\n",
    "        self.nb_class = 2\n",
    "        \n",
    "        # number of convolutional filters to use  \n",
    "        self.nb_CNN = [32, 64, 64, 64, 128]  \n",
    "        # number of dense filters to use  \n",
    "        self.nb_Dense = [256] \n",
    "        # size of pooling area for max pooling  \n",
    "        self.pool_size = (2, 2)  \n",
    "        # convolution kernel size  \n",
    "        self.kernel_size = (3, 3)\n",
    "        self.batch_normalization_control = True\n",
    "        \n",
    "        # name for checkpoint\n",
    "        self.model_name = 'CNN_GLCM_C%d_D%d_Kernel(%d,%d)_%d_lrdecay' % (len(self.nb_CNN), len(self.nb_Dense),\n",
    "                                                          self.kernel_size[0], self.kernel_size[1], max(self.nb_CNN))\n",
    "\n",
    "        # train\n",
    "        #设置一个全局的计数器\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(0.001, \n",
    "                                             global_step=self.global_step, \n",
    "                                             decay_steps=10, \n",
    "                                             decay_rate=0.9, \n",
    "                                             staircase=True)\n",
    "        self.beta1 = 0.5\n",
    "        #max model to keep saving\n",
    "        self.max_to_keep = 300\n",
    "        \n",
    "        # test\n",
    "\n",
    "        #load_flower_data\n",
    "        self.train_x = np.load('../trainImage.npy')\n",
    "        self.train_y = np.load('../trainLabels.npy')\n",
    "        self.test_x = np.load('../testImage.npy')\n",
    "        self.test_y = np.load('../testLabels.npy')\n",
    "        self.train_x_glcm = np.load('../trainImage_GLCM.npy')\n",
    "        self.test_x_glcm = np.load('../testImage_GLCM.npy')\n",
    "        \n",
    "        #记录\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['losses'] = []\n",
    "        self.train_hist['accuracy'] = []\n",
    "        self.train_hist['learning_rate'] = []\n",
    "        self.train_hist['per_epoch_ptimes'] = []\n",
    "        self.train_hist['total_ptime'] = []\n",
    "        \n",
    "        # get number of batches for a single epoch\n",
    "        self.num_batches_train = len(self.train_x) // self.batch_size\n",
    "        self.num_batches_test= len(self.test_x) // self.batch_size\n",
    "\n",
    "    def cnn_model(self, x, x_GLCM, keep_prob, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(\"cnn\", reuse=reuse):\n",
    "             \n",
    "            #初始化参数\n",
    "            W = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "            B = tf.constant_initializer(0.0)\n",
    "        \n",
    "            print(\"CNN:x\",x.get_shape()) # 128, 128, 3 \n",
    "            print(\"CNN:x_GLCM\",x_GLCM.get_shape()) # 128, 128, 3 \n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_1 = tf.layers.conv2d(inputs = x,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_1'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_1.get_shape())\n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_2 = tf.layers.conv2d(inputs = x_GLCM,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_2'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_2.get_shape())\n",
    "\n",
    "            #把数据和边缘进行连接\n",
    "            net = tf.concat([net1_1, net1_2], 3)\n",
    "            net = tf.layers.batch_normalization(net, training=is_training)\n",
    "            net = tf.nn.relu(net, name = 'relu_conv_1')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_1'\n",
    "                                             )\n",
    "            \n",
    "            for i in range(2,len(self.nb_CNN)+1):\n",
    "                net = tf.layers.conv2d(inputs = net,                 # 输入,\n",
    "                                       filters = self.nb_CNN[i-1],      # 卷积核个数,\n",
    "                                       kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                       strides = (1, 1),\n",
    "                                       padding = 'same',              # padding方法\n",
    "                                       kernel_initializer = W,\n",
    "                                       bias_initializer = B,\n",
    "                                       kernel_regularizer = None,\n",
    "                                       bias_regularizer = None,\n",
    "                                       activity_regularizer = None,\n",
    "                                       name = 'conv_'+ str(i)        # 命名用于获取变量\n",
    "                                       )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "                if self.batch_normalization_control:\n",
    "                    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_conv_' + str(i))\n",
    "                net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_' + str(i)\n",
    "                                             )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #flatten\n",
    "            net = tf.reshape(net, [-1, int(net.get_shape()[1]*net.get_shape()[2]*net.get_shape()[3])],name='flatten')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #dense layer\n",
    "            for i in range(1,len(self.nb_Dense)+1):\n",
    "                net = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_Dense[i-1],\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_' + str(i)\n",
    "                                    )\n",
    "#                 net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_dense_' + str(i))\n",
    "                net = tf.layers.dropout(inputs = net,\n",
    "                                        rate=keep_prob,\n",
    "                                        noise_shape=None,\n",
    "                                        seed=None,\n",
    "                                        training = is_training,\n",
    "                                        name= 'dropout_dense_' + str(i)\n",
    "                                        )\n",
    "            #output\n",
    "            logit = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_class,\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_output'\n",
    "                                    )\n",
    "            out_logit = tf.nn.softmax(logit, name=\"softmax\")\n",
    "            print(\"CNN:out_logit\",out_logit.get_shape())\n",
    "            print(\"------------------------\")    \n",
    "\n",
    "            return out_logit, logit\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.x = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.c_dim], \n",
    "                                name='x_image')\n",
    "        \n",
    "        self.x_GLCM = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.c_dim], \n",
    "                                name='x_GLCM')\n",
    "\n",
    "        self.y = tf.placeholder(tf.float32, shape=[self.batch_size, self.nb_class], name='y_label')\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.add_global = self.global_step.assign_add(1)\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "\n",
    "        # output of cnn_model\n",
    "        self.out_logit, self.logit = self.cnn_model(self.x, self.x_GLCM, self.keep_prob, is_training=True, reuse=False)\n",
    "        \n",
    "        self.loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y,\n",
    "                                                                                         logits =self.logit))\n",
    "        \n",
    "        \"\"\" Training \"\"\"\n",
    "        # trainable variables into a group\n",
    "        tf_vars = tf.trainable_variables()\n",
    "        cnn_vars = [var for var in tf_vars if var.name.startswith('cnn')]\n",
    "\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.cnn_optim = tf.train.AdamOptimizer(self.lr, beta1=self.beta1).minimize(self.loss_cross_entropy,\n",
    "                                                                                        var_list=cnn_vars)\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        # output of cnn_model\n",
    "        self.out_logit_test, self.logit_test = self.cnn_model(self.x, self.x_GLCM, self.keep_prob, is_training=False, reuse=True)\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit_test, 1), tf.argmax(self.y, 1))\n",
    "        self.predict = tf.argmax(self.logit_test, 1)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        self.loss_sum = tf.summary.scalar(\"loss\", self.loss_cross_entropy)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver(max_to_keep = self.max_to_keep)\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_epoch = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_epoch) + 1\n",
    "            counter = 1\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name+'.pkl', 'rb') \n",
    "            self.train_hist = pickle.load(f)\n",
    "            f.close()\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "            print(\" [!] START_EPOCH is \", start_epoch)\n",
    "        else:\n",
    "            start_epoch = 1\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        for epoch_loop in range(start_epoch, self.epoch + 1):\n",
    "\n",
    "            CNN_losses = []\n",
    "  \n",
    "            epoch_start_time = time.time()\n",
    "            shuffle_idxs = random.sample(range(0, self.train_x.shape[0]), self.train_x.shape[0])\n",
    "            shuffled_set = self.train_x[shuffle_idxs]\n",
    "            shuffled_set_glcm = self.train_x_glcm[shuffle_idxs]\n",
    "            shuffled_label = self.train_y[shuffle_idxs]\n",
    "    \n",
    "            # get batch data\n",
    "            for idx in range(self.num_batches_train):\n",
    "                batch_x = shuffled_set[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "#                 batch_x_GLCM = self.batch_GLCM(batch_x)\n",
    "                batch_x_GLCM = shuffled_set_glcm[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_y = shuffled_label[idx*self.batch_size:(idx+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                \n",
    "\n",
    "                # update D network\n",
    "                _, summary_str, cnn_loss = self.sess.run([self.cnn_optim, self.loss_sum, self.loss_cross_entropy],\n",
    "                                               feed_dict={self.x: batch_x,\n",
    "                                                          self.x_GLCM: batch_x_GLCM,\n",
    "                                                          self.y: batch_y,\n",
    "                                                          self.keep_prob: 0.5}\n",
    "                                                      )\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                CNN_losses.append(cnn_loss)\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f\" % (epoch_loop, idx, self.num_batches_train, \n",
    "                                                                          time.time() - start_time, cnn_loss))\n",
    "\n",
    "            # After an epoch\n",
    "            # Evaluates accuracy on test set\n",
    "            test_accuracy_list = []\n",
    "            for idx_test in range(self.num_batches_test):\n",
    "                batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "                batch_x_GLCM_test =self.test_x_glcm[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "                batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                    self.x_GLCM: batch_x_GLCM_test,\n",
    "                                                                    self.y: batch_y_tes,\n",
    "                                                                    self.keep_prob: 1.0})\n",
    "                test_accuracy_list.append(accuracy)\n",
    "            test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        \n",
    "            #update learning rate\n",
    "            _, rate = sess.run([self.add_global, self.lr])\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            print('[%d/%d] - ptime: %.4f loss: %.8f acc: %.5f lr: %.8f'% (epoch_loop, self.epoch, per_epoch_ptime, \n",
    "                                                                    np.mean(CNN_losses), test_accuracy, rate))\n",
    "            \n",
    "            self.train_hist['losses'].append(np.mean(CNN_losses))\n",
    "            self.train_hist['accuracy'].append( test_accuracy)\n",
    "            self.train_hist['learning_rate'].append(rate)\n",
    "            self.train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "            \n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, epoch_loop)\n",
    "            \n",
    "            # save trainhist for train\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name + '.pkl', 'wb') \n",
    "            pickle.dump(self.train_hist, f)\n",
    "            f.close()\n",
    "            self.show_train_hist(self.train_hist, save=True, path= self.trainhist_dir + '/' \n",
    "                                 + self.model_name + '.png')\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_ptime = end_time - start_time\n",
    "        self.train_hist['total_ptime'].append(total_ptime)\n",
    "        print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(self.train_hist['per_epoch_ptimes']), \n",
    "                                                                          self.epoch, total_ptime))\n",
    "        print(\" [*] Training finished!\")\n",
    "        \n",
    "        \"\"\"test after train\"\"\"\n",
    "        best_acc = max(self.train_hist['accuracy'])\n",
    "        beat_epoch = self.train_hist['accuracy'].index(best_acc) + 1\n",
    "        print (\" [*] Best Epoch: \", beat_epoch, \", Accuracy: \", best_acc)\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(beat_epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_GLCM_test =self.test_x_glcm[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_GLCM: batch_x_GLCM_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Best Epoch:\", beat_epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "\n",
    "    def test(self, epoch):\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_GLCM_test =self.test_x_glcm[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_GLCM: batch_x_GLCM_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Epoch:\", epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "        \n",
    "    def show_all_variables(self):\n",
    "        model_vars = tf.trainable_variables()\n",
    "        tf.contrib.slim.model_analyzer.analyze_vars(model_vars, print_info=True) \n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            epoch = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read [{}], epoch [{}]\".format(ckpt_name,epoch))\n",
    "            return True, epoch\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "        \n",
    "    def show_train_hist(self, hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "        x = range(1, len(hist['losses'])+1)\n",
    "\n",
    "        y1 = hist['losses']\n",
    "        y2 = hist['accuracy']\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "                            \n",
    "        ax2 = ax1.twinx()  \n",
    "\n",
    "        ax1.plot(x, y1, 'b')\n",
    "        ax2.plot(x, y2, 'r')\n",
    "                            \n",
    "        ax1.set_xlabel('Epoch')\n",
    "                            \n",
    "        ax1.set_ylabel('CNN_loss')    \n",
    "        ax2.set_ylabel('accuracy')\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(path, dpi = 400)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_GLCM (100, 128, 128, 3)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_GLCM (100, 128, 128, 3)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "cnn/conv_1_1/kernel:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "cnn/conv_1_1/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/conv_1_2/kernel:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "cnn/conv_1_2/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/batch_normalization/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_2/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_2/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_3/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_3/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_4/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_4/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_5/kernel:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
      "cnn/conv_5/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/dense_1/kernel:0 (float32_ref 2048x256) [524288, bytes: 2097152]\n",
      "cnn/dense_1/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "cnn/dense_output/kernel:0 (float32_ref 256x2) [512, bytes: 2048]\n",
      "cnn/dense_output/bias:0 (float32_ref 2) [2, bytes: 8]\n",
      "Total size of variables: 712258\n",
      "Total bytes of variables: 2849032\n",
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load failed...\n",
      "Epoch: [ 1] [   0/  20] time: 1.6129, loss: 0.68355715\n",
      "Epoch: [ 1] [   1/  20] time: 1.8556, loss: 0.32849205\n",
      "Epoch: [ 1] [   2/  20] time: 2.0965, loss: 0.11542796\n",
      "Epoch: [ 1] [   3/  20] time: 2.3370, loss: 0.10611991\n",
      "Epoch: [ 1] [   4/  20] time: 2.5780, loss: 0.12237713\n",
      "Epoch: [ 1] [   5/  20] time: 2.8218, loss: 0.06463742\n",
      "Epoch: [ 1] [   6/  20] time: 3.0622, loss: 0.05170728\n",
      "Epoch: [ 1] [   7/  20] time: 3.3028, loss: 0.07217710\n",
      "Epoch: [ 1] [   8/  20] time: 3.5430, loss: 0.05850030\n",
      "Epoch: [ 1] [   9/  20] time: 3.7844, loss: 0.03295911\n",
      "Epoch: [ 1] [  10/  20] time: 4.0266, loss: 0.05685390\n",
      "Epoch: [ 1] [  11/  20] time: 4.2670, loss: 0.03824202\n",
      "Epoch: [ 1] [  12/  20] time: 4.5069, loss: 0.02042487\n",
      "Epoch: [ 1] [  13/  20] time: 4.7488, loss: 0.03464924\n",
      "Epoch: [ 1] [  14/  20] time: 4.9889, loss: 0.05392769\n",
      "Epoch: [ 1] [  15/  20] time: 5.2288, loss: 0.05285078\n",
      "Epoch: [ 1] [  16/  20] time: 5.4690, loss: 0.01897217\n",
      "Epoch: [ 1] [  17/  20] time: 5.7098, loss: 0.05016525\n",
      "Epoch: [ 1] [  18/  20] time: 5.9523, loss: 0.01553925\n",
      "Epoch: [ 1] [  19/  20] time: 6.1919, loss: 0.05007621\n",
      "[1/50] - ptime: 6.2581 loss: 0.10138284 acc: 0.80000 lr: 0.00100000\n",
      "Epoch: [ 2] [   0/  20] time: 7.1245, loss: 0.03692690\n",
      "Epoch: [ 2] [   1/  20] time: 7.3624, loss: 0.04253440\n",
      "Epoch: [ 2] [   2/  20] time: 7.6020, loss: 0.02348244\n",
      "Epoch: [ 2] [   3/  20] time: 7.8452, loss: 0.05228204\n",
      "Epoch: [ 2] [   4/  20] time: 8.0850, loss: 0.02981604\n",
      "Epoch: [ 2] [   5/  20] time: 8.3248, loss: 0.01170623\n",
      "Epoch: [ 2] [   6/  20] time: 8.5642, loss: 0.05123684\n",
      "Epoch: [ 2] [   7/  20] time: 8.8055, loss: 0.00431979\n",
      "Epoch: [ 2] [   8/  20] time: 9.0452, loss: 0.03007654\n",
      "Epoch: [ 2] [   9/  20] time: 9.2850, loss: 0.03358176\n",
      "Epoch: [ 2] [  10/  20] time: 9.5248, loss: 0.04312307\n",
      "Epoch: [ 2] [  11/  20] time: 9.7668, loss: 0.01484628\n",
      "Epoch: [ 2] [  12/  20] time: 10.0068, loss: 0.03461018\n",
      "Epoch: [ 2] [  13/  20] time: 10.2461, loss: 0.02606688\n",
      "Epoch: [ 2] [  14/  20] time: 10.4855, loss: 0.02244374\n",
      "Epoch: [ 2] [  15/  20] time: 10.7257, loss: 0.04997480\n",
      "Epoch: [ 2] [  16/  20] time: 10.9669, loss: 0.04110651\n",
      "Epoch: [ 2] [  17/  20] time: 11.2071, loss: 0.05141203\n",
      "Epoch: [ 2] [  18/  20] time: 11.4470, loss: 0.03256831\n",
      "Epoch: [ 2] [  19/  20] time: 11.6870, loss: 0.04206572\n",
      "[2/50] - ptime: 5.1241 loss: 0.03370903 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 3] [   0/  20] time: 12.6453, loss: 0.00937792\n",
      "Epoch: [ 3] [   1/  20] time: 12.8863, loss: 0.12956704\n",
      "Epoch: [ 3] [   2/  20] time: 13.1271, loss: 0.17293803\n",
      "Epoch: [ 3] [   3/  20] time: 13.3677, loss: 0.02333037\n",
      "Epoch: [ 3] [   4/  20] time: 13.6083, loss: 0.03045638\n",
      "Epoch: [ 3] [   5/  20] time: 13.8533, loss: 0.06196653\n",
      "Epoch: [ 3] [   6/  20] time: 14.0938, loss: 0.00246636\n",
      "Epoch: [ 3] [   7/  20] time: 14.3343, loss: 0.06382779\n",
      "Epoch: [ 3] [   8/  20] time: 14.5745, loss: 0.02934215\n",
      "Epoch: [ 3] [   9/  20] time: 14.8149, loss: 0.22202085\n",
      "Epoch: [ 3] [  10/  20] time: 15.0573, loss: 0.11568486\n",
      "Epoch: [ 3] [  11/  20] time: 15.2976, loss: 0.03759324\n",
      "Epoch: [ 3] [  12/  20] time: 15.5372, loss: 0.03040973\n",
      "Epoch: [ 3] [  13/  20] time: 15.7788, loss: 0.03594828\n",
      "Epoch: [ 3] [  14/  20] time: 16.0203, loss: 0.04094539\n",
      "Epoch: [ 3] [  15/  20] time: 16.2605, loss: 0.02640999\n",
      "Epoch: [ 3] [  16/  20] time: 16.5012, loss: 0.02773352\n",
      "Epoch: [ 3] [  17/  20] time: 16.7419, loss: 0.05141211\n",
      "Epoch: [ 3] [  18/  20] time: 16.9819, loss: 0.09037247\n",
      "Epoch: [ 3] [  19/  20] time: 17.2215, loss: 0.07784103\n",
      "[3/50] - ptime: 5.1341 loss: 0.06398220 acc: 0.77000 lr: 0.00100000\n",
      "Epoch: [ 4] [   0/  20] time: 18.1303, loss: 0.01061410\n",
      "Epoch: [ 4] [   1/  20] time: 18.3683, loss: 0.03921088\n",
      "Epoch: [ 4] [   2/  20] time: 18.6061, loss: 0.01293531\n",
      "Epoch: [ 4] [   3/  20] time: 18.8489, loss: 0.02614294\n",
      "Epoch: [ 4] [   4/  20] time: 19.0904, loss: 0.03307453\n",
      "Epoch: [ 4] [   5/  20] time: 19.3309, loss: 0.05352099\n",
      "Epoch: [ 4] [   6/  20] time: 19.5714, loss: 0.01872138\n",
      "Epoch: [ 4] [   7/  20] time: 19.8131, loss: 0.00128919\n",
      "Epoch: [ 4] [   8/  20] time: 20.0524, loss: 0.02868037\n",
      "Epoch: [ 4] [   9/  20] time: 20.2926, loss: 0.05544552\n",
      "Epoch: [ 4] [  10/  20] time: 20.5332, loss: 0.05744486\n",
      "Epoch: [ 4] [  11/  20] time: 20.7753, loss: 0.03981215\n",
      "Epoch: [ 4] [  12/  20] time: 21.0160, loss: 0.01384677\n",
      "Epoch: [ 4] [  13/  20] time: 21.2557, loss: 0.02807031\n",
      "Epoch: [ 4] [  14/  20] time: 21.4963, loss: 0.01449619\n",
      "Epoch: [ 4] [  15/  20] time: 21.7388, loss: 0.03277812\n",
      "Epoch: [ 4] [  16/  20] time: 21.9783, loss: 0.06898431\n",
      "Epoch: [ 4] [  17/  20] time: 22.2184, loss: 0.01333417\n",
      "Epoch: [ 4] [  18/  20] time: 22.4588, loss: 0.01919244\n",
      "Epoch: [ 4] [  19/  20] time: 22.7001, loss: 0.01684226\n",
      "[4/50] - ptime: 5.1289 loss: 0.02922184 acc: 0.80000 lr: 0.00100000\n",
      "Epoch: [ 5] [   0/  20] time: 23.6133, loss: 0.01409707\n",
      "Epoch: [ 5] [   1/  20] time: 23.8557, loss: 0.02979193\n",
      "Epoch: [ 5] [   2/  20] time: 24.0961, loss: 0.00984493\n",
      "Epoch: [ 5] [   3/  20] time: 24.3362, loss: 0.02747998\n",
      "Epoch: [ 5] [   4/  20] time: 24.5765, loss: 0.02196268\n",
      "Epoch: [ 5] [   5/  20] time: 24.8196, loss: 0.01902195\n",
      "Epoch: [ 5] [   6/  20] time: 25.0605, loss: 0.00516186\n",
      "Epoch: [ 5] [   7/  20] time: 25.3033, loss: 0.13692120\n",
      "Epoch: [ 5] [   8/  20] time: 25.5456, loss: 0.04927112\n",
      "Epoch: [ 5] [   9/  20] time: 25.7873, loss: 0.02756952\n",
      "Epoch: [ 5] [  10/  20] time: 26.0295, loss: 0.02211261\n",
      "Epoch: [ 5] [  11/  20] time: 26.2700, loss: 0.05133902\n",
      "Epoch: [ 5] [  12/  20] time: 26.5104, loss: 0.04697995\n",
      "Epoch: [ 5] [  13/  20] time: 26.7517, loss: 0.06137349\n",
      "Epoch: [ 5] [  14/  20] time: 26.9940, loss: 0.04065396\n",
      "Epoch: [ 5] [  15/  20] time: 27.2345, loss: 0.01149598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 5] [  16/  20] time: 27.4746, loss: 0.04339203\n",
      "Epoch: [ 5] [  17/  20] time: 27.7169, loss: 0.01382364\n",
      "Epoch: [ 5] [  18/  20] time: 27.9605, loss: 0.07291513\n",
      "Epoch: [ 5] [  19/  20] time: 28.2012, loss: 0.02691477\n",
      "[5/50] - ptime: 5.1461 loss: 0.03660614 acc: 0.73000 lr: 0.00100000\n",
      "Epoch: [ 6] [   0/  20] time: 29.1237, loss: 0.03946025\n",
      "Epoch: [ 6] [   1/  20] time: 29.3623, loss: 0.00837838\n",
      "Epoch: [ 6] [   2/  20] time: 29.6025, loss: 0.03398918\n",
      "Epoch: [ 6] [   3/  20] time: 29.8443, loss: 0.00447670\n",
      "Epoch: [ 6] [   4/  20] time: 30.0850, loss: 0.01058676\n",
      "Epoch: [ 6] [   5/  20] time: 30.3250, loss: 0.02505489\n",
      "Epoch: [ 6] [   6/  20] time: 30.5653, loss: 0.00188113\n",
      "Epoch: [ 6] [   7/  20] time: 30.8067, loss: 0.02658002\n",
      "Epoch: [ 6] [   8/  20] time: 31.0481, loss: 0.01845208\n",
      "Epoch: [ 6] [   9/  20] time: 31.2884, loss: 0.03521707\n",
      "Epoch: [ 6] [  10/  20] time: 31.5289, loss: 0.01270238\n",
      "Epoch: [ 6] [  11/  20] time: 31.7706, loss: 0.02620232\n",
      "Epoch: [ 6] [  12/  20] time: 32.0113, loss: 0.04133435\n",
      "Epoch: [ 6] [  13/  20] time: 32.2520, loss: 0.06140082\n",
      "Epoch: [ 6] [  14/  20] time: 32.4924, loss: 0.01108272\n",
      "Epoch: [ 6] [  15/  20] time: 32.7341, loss: 0.01091871\n",
      "Epoch: [ 6] [  16/  20] time: 32.9782, loss: 0.01340216\n",
      "Epoch: [ 6] [  17/  20] time: 33.2197, loss: 0.03734646\n",
      "Epoch: [ 6] [  18/  20] time: 33.4602, loss: 0.02223506\n",
      "Epoch: [ 6] [  19/  20] time: 33.7018, loss: 0.04713061\n",
      "[6/50] - ptime: 5.1380 loss: 0.02439160 acc: 0.56000 lr: 0.00100000\n",
      "Epoch: [ 7] [   0/  20] time: 34.6271, loss: 0.02453655\n",
      "Epoch: [ 7] [   1/  20] time: 34.8692, loss: 0.01492017\n",
      "Epoch: [ 7] [   2/  20] time: 35.1108, loss: 0.00753054\n",
      "Epoch: [ 7] [   3/  20] time: 35.3528, loss: 0.04434012\n",
      "Epoch: [ 7] [   4/  20] time: 35.5941, loss: 0.02021570\n",
      "Epoch: [ 7] [   5/  20] time: 35.8390, loss: 0.02789846\n",
      "Epoch: [ 7] [   6/  20] time: 36.0795, loss: 0.01587173\n",
      "Epoch: [ 7] [   7/  20] time: 36.3194, loss: 0.00519510\n",
      "Epoch: [ 7] [   8/  20] time: 36.5601, loss: 0.02096419\n",
      "Epoch: [ 7] [   9/  20] time: 36.8024, loss: 0.02824219\n",
      "Epoch: [ 7] [  10/  20] time: 37.0431, loss: 0.02942677\n",
      "Epoch: [ 7] [  11/  20] time: 37.2839, loss: 0.01456597\n",
      "Epoch: [ 7] [  12/  20] time: 37.5244, loss: 0.02294683\n",
      "Epoch: [ 7] [  13/  20] time: 37.7659, loss: 0.01985533\n",
      "Epoch: [ 7] [  14/  20] time: 38.0060, loss: 0.01450142\n",
      "Epoch: [ 7] [  15/  20] time: 38.2464, loss: 0.02418345\n",
      "Epoch: [ 7] [  16/  20] time: 38.4866, loss: 0.01176029\n",
      "Epoch: [ 7] [  17/  20] time: 38.7271, loss: 0.03948727\n",
      "Epoch: [ 7] [  18/  20] time: 38.9695, loss: 0.01511873\n",
      "Epoch: [ 7] [  19/  20] time: 39.2134, loss: 0.02751690\n",
      "[7/50] - ptime: 5.1446 loss: 0.02145388 acc: 0.74000 lr: 0.00100000\n",
      "Epoch: [ 8] [   0/  20] time: 40.1236, loss: 0.01333186\n",
      "Epoch: [ 8] [   1/  20] time: 40.3623, loss: 0.01088719\n",
      "Epoch: [ 8] [   2/  20] time: 40.6040, loss: 0.02482791\n",
      "Epoch: [ 8] [   3/  20] time: 40.8462, loss: 0.04427174\n",
      "Epoch: [ 8] [   4/  20] time: 41.0879, loss: 0.03300041\n",
      "Epoch: [ 8] [   5/  20] time: 41.3291, loss: 0.01965376\n",
      "Epoch: [ 8] [   6/  20] time: 41.5706, loss: 0.01776939\n",
      "Epoch: [ 8] [   7/  20] time: 41.8130, loss: 0.01103442\n",
      "Epoch: [ 8] [   8/  20] time: 42.0544, loss: 0.06117995\n",
      "Epoch: [ 8] [   9/  20] time: 42.2950, loss: 0.01484470\n",
      "Epoch: [ 8] [  10/  20] time: 42.5361, loss: 0.04508173\n",
      "Epoch: [ 8] [  11/  20] time: 42.7789, loss: 0.01061082\n",
      "Epoch: [ 8] [  12/  20] time: 43.0206, loss: 0.04196340\n",
      "Epoch: [ 8] [  13/  20] time: 43.2611, loss: 0.00705069\n",
      "Epoch: [ 8] [  14/  20] time: 43.5021, loss: 0.00700566\n",
      "Epoch: [ 8] [  15/  20] time: 43.7438, loss: 0.01973924\n",
      "Epoch: [ 8] [  16/  20] time: 43.9868, loss: 0.02348743\n",
      "Epoch: [ 8] [  17/  20] time: 44.2279, loss: 0.02735775\n",
      "Epoch: [ 8] [  18/  20] time: 44.4705, loss: 0.01662138\n",
      "Epoch: [ 8] [  19/  20] time: 44.7110, loss: 0.02657778\n",
      "[8/50] - ptime: 5.1470 loss: 0.02381486 acc: 0.60000 lr: 0.00100000\n",
      "Epoch: [ 9] [   0/  20] time: 45.6676, loss: 0.01860853\n",
      "Epoch: [ 9] [   1/  20] time: 45.9089, loss: 0.02991533\n",
      "Epoch: [ 9] [   2/  20] time: 46.1494, loss: 0.00966683\n",
      "Epoch: [ 9] [   3/  20] time: 46.3898, loss: 0.00039275\n",
      "Epoch: [ 9] [   4/  20] time: 46.6302, loss: 0.02043659\n",
      "Epoch: [ 9] [   5/  20] time: 46.8729, loss: 0.03156791\n",
      "Epoch: [ 9] [   6/  20] time: 47.1151, loss: 0.00784814\n",
      "Epoch: [ 9] [   7/  20] time: 47.3562, loss: 0.01944757\n",
      "Epoch: [ 9] [   8/  20] time: 47.5976, loss: 0.01553660\n",
      "Epoch: [ 9] [   9/  20] time: 47.8418, loss: 0.01667104\n",
      "Epoch: [ 9] [  10/  20] time: 48.0837, loss: 0.00038867\n",
      "Epoch: [ 9] [  11/  20] time: 48.3253, loss: 0.01495197\n",
      "Epoch: [ 9] [  12/  20] time: 48.5662, loss: 0.04972723\n",
      "Epoch: [ 9] [  13/  20] time: 48.8096, loss: 0.01532860\n",
      "Epoch: [ 9] [  14/  20] time: 49.0534, loss: 0.01052019\n",
      "Epoch: [ 9] [  15/  20] time: 49.2941, loss: 0.02247288\n",
      "Epoch: [ 9] [  16/  20] time: 49.5351, loss: 0.02633670\n",
      "Epoch: [ 9] [  17/  20] time: 49.7768, loss: 0.01821908\n",
      "Epoch: [ 9] [  18/  20] time: 50.0191, loss: 0.01820793\n",
      "Epoch: [ 9] [  19/  20] time: 50.2597, loss: 0.00430948\n",
      "[9/50] - ptime: 5.1500 loss: 0.01752770 acc: 0.57000 lr: 0.00100000\n",
      "Epoch: [10] [   0/  20] time: 51.1606, loss: 0.00047272\n",
      "Epoch: [10] [   1/  20] time: 51.4000, loss: 0.01214681\n",
      "Epoch: [10] [   2/  20] time: 51.6404, loss: 0.02754395\n",
      "Epoch: [10] [   3/  20] time: 51.8831, loss: 0.01088506\n",
      "Epoch: [10] [   4/  20] time: 52.1232, loss: 0.00996735\n",
      "Epoch: [10] [   5/  20] time: 52.3638, loss: 0.03509948\n",
      "Epoch: [10] [   6/  20] time: 52.6043, loss: 0.00401951\n",
      "Epoch: [10] [   7/  20] time: 52.8461, loss: 0.01807686\n",
      "Epoch: [10] [   8/  20] time: 53.0869, loss: 0.01256202\n",
      "Epoch: [10] [   9/  20] time: 53.3277, loss: 0.00011415\n",
      "Epoch: [10] [  10/  20] time: 53.5693, loss: 0.02016787\n",
      "Epoch: [10] [  11/  20] time: 53.8126, loss: 0.07650869\n",
      "Epoch: [10] [  12/  20] time: 54.0536, loss: 0.03084010\n",
      "Epoch: [10] [  13/  20] time: 54.2947, loss: 0.03450366\n",
      "Epoch: [10] [  14/  20] time: 54.5360, loss: 0.01884008\n",
      "Epoch: [10] [  15/  20] time: 54.7781, loss: 0.02871809\n",
      "Epoch: [10] [  16/  20] time: 55.0231, loss: 0.03110456\n",
      "Epoch: [10] [  17/  20] time: 55.2643, loss: 0.02042514\n",
      "Epoch: [10] [  18/  20] time: 55.5057, loss: 0.01230725\n",
      "Epoch: [10] [  19/  20] time: 55.7484, loss: 0.01288889\n",
      "[10/50] - ptime: 5.1461 loss: 0.02085961 acc: 0.71000 lr: 0.00090000\n",
      "Epoch: [11] [   0/  20] time: 56.6444, loss: 0.01531713\n",
      "Epoch: [11] [   1/  20] time: 56.8855, loss: 0.01840872\n",
      "Epoch: [11] [   2/  20] time: 57.1265, loss: 0.00720291\n",
      "Epoch: [11] [   3/  20] time: 57.3686, loss: 0.03186231\n",
      "Epoch: [11] [   4/  20] time: 57.6091, loss: 0.00729575\n",
      "Epoch: [11] [   5/  20] time: 57.8523, loss: 0.02272288\n",
      "Epoch: [11] [   6/  20] time: 58.0914, loss: 0.02011880\n",
      "Epoch: [11] [   7/  20] time: 58.3332, loss: 0.01401308\n",
      "Epoch: [11] [   8/  20] time: 58.5754, loss: 0.03079387\n",
      "Epoch: [11] [   9/  20] time: 58.8186, loss: 0.02429031\n",
      "Epoch: [11] [  10/  20] time: 59.0605, loss: 0.01804005\n",
      "Epoch: [11] [  11/  20] time: 59.3020, loss: 0.01325437\n",
      "Epoch: [11] [  12/  20] time: 59.5430, loss: 0.01465263\n",
      "Epoch: [11] [  13/  20] time: 59.7853, loss: 0.01596426\n",
      "Epoch: [11] [  14/  20] time: 60.0279, loss: 0.03180929\n",
      "Epoch: [11] [  15/  20] time: 60.2687, loss: 0.02730188\n",
      "Epoch: [11] [  16/  20] time: 60.5091, loss: 0.00824075\n",
      "Epoch: [11] [  17/  20] time: 60.7512, loss: 0.03738967\n",
      "Epoch: [11] [  18/  20] time: 60.9943, loss: 0.00163270\n",
      "Epoch: [11] [  19/  20] time: 61.2352, loss: 0.00983547\n",
      "[11/50] - ptime: 5.1492 loss: 0.01850734 acc: 0.29000 lr: 0.00090000\n",
      "Epoch: [12] [   0/  20] time: 62.1614, loss: 0.01575721\n",
      "Epoch: [12] [   1/  20] time: 62.4010, loss: 0.00145654\n",
      "Epoch: [12] [   2/  20] time: 62.6419, loss: 0.01039740\n",
      "Epoch: [12] [   3/  20] time: 62.8854, loss: 0.01178046\n",
      "Epoch: [12] [   4/  20] time: 63.1263, loss: 0.00004168\n",
      "Epoch: [12] [   5/  20] time: 63.3678, loss: 0.01625484\n",
      "Epoch: [12] [   6/  20] time: 63.6082, loss: 0.04826319\n",
      "Epoch: [12] [   7/  20] time: 63.8510, loss: 0.01374289\n",
      "Epoch: [12] [   8/  20] time: 64.0931, loss: 0.00645212\n",
      "Epoch: [12] [   9/  20] time: 64.3353, loss: 0.02014548\n",
      "Epoch: [12] [  10/  20] time: 64.5763, loss: 0.00107663\n",
      "Epoch: [12] [  11/  20] time: 64.8200, loss: 0.01713220\n",
      "Epoch: [12] [  12/  20] time: 65.0614, loss: 0.02189493\n",
      "Epoch: [12] [  13/  20] time: 65.3021, loss: 0.00648098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12] [  14/  20] time: 65.5422, loss: 0.02104071\n",
      "Epoch: [12] [  15/  20] time: 65.7847, loss: 0.02581180\n",
      "Epoch: [12] [  16/  20] time: 66.0267, loss: 0.00380945\n",
      "Epoch: [12] [  17/  20] time: 66.2683, loss: 0.01164910\n",
      "Epoch: [12] [  18/  20] time: 66.5088, loss: 0.03605047\n",
      "Epoch: [12] [  19/  20] time: 66.7517, loss: 0.02180663\n",
      "[12/50] - ptime: 5.1489 loss: 0.01555224 acc: 0.43000 lr: 0.00090000\n",
      "Epoch: [13] [   0/  20] time: 67.6481, loss: 0.03406337\n",
      "Epoch: [13] [   1/  20] time: 67.8897, loss: 0.02058636\n",
      "Epoch: [13] [   2/  20] time: 68.1305, loss: 0.00695443\n",
      "Epoch: [13] [   3/  20] time: 68.3712, loss: 0.00817413\n",
      "Epoch: [13] [   4/  20] time: 68.6124, loss: 0.01947048\n",
      "Epoch: [13] [   5/  20] time: 68.8552, loss: 0.05369007\n",
      "Epoch: [13] [   6/  20] time: 69.0977, loss: 0.00159705\n",
      "Epoch: [13] [   7/  20] time: 69.3394, loss: 0.00715176\n",
      "Epoch: [13] [   8/  20] time: 69.5813, loss: 0.03193051\n",
      "Epoch: [13] [   9/  20] time: 69.8243, loss: 0.02742330\n",
      "Epoch: [13] [  10/  20] time: 70.0680, loss: 0.00860068\n",
      "Epoch: [13] [  11/  20] time: 70.3092, loss: 0.00134476\n",
      "Epoch: [13] [  12/  20] time: 70.5507, loss: 0.07779571\n",
      "Epoch: [13] [  13/  20] time: 70.7939, loss: 0.00498563\n",
      "Epoch: [13] [  14/  20] time: 71.0370, loss: 0.00142175\n",
      "Epoch: [13] [  15/  20] time: 71.2778, loss: 0.01450820\n",
      "Epoch: [13] [  16/  20] time: 71.5189, loss: 0.05117784\n",
      "Epoch: [13] [  17/  20] time: 71.7607, loss: 0.01929650\n",
      "Epoch: [13] [  18/  20] time: 72.0040, loss: 0.07527114\n",
      "Epoch: [13] [  19/  20] time: 72.2450, loss: 0.00308318\n",
      "[13/50] - ptime: 5.1543 loss: 0.02342634 acc: 0.74000 lr: 0.00090000\n",
      "Epoch: [14] [   0/  20] time: 73.1447, loss: 0.00095023\n",
      "Epoch: [14] [   1/  20] time: 73.3843, loss: 0.01228635\n",
      "Epoch: [14] [   2/  20] time: 73.6255, loss: 0.03424801\n",
      "Epoch: [14] [   3/  20] time: 73.8696, loss: 0.00561217\n",
      "Epoch: [14] [   4/  20] time: 74.1144, loss: 0.01419815\n",
      "Epoch: [14] [   5/  20] time: 74.3563, loss: 0.05951514\n",
      "Epoch: [14] [   6/  20] time: 74.5989, loss: 0.00986889\n",
      "Epoch: [14] [   7/  20] time: 74.8408, loss: 0.05200236\n",
      "Epoch: [14] [   8/  20] time: 75.0839, loss: 0.06902075\n",
      "Epoch: [14] [   9/  20] time: 75.3255, loss: 0.00385277\n",
      "Epoch: [14] [  10/  20] time: 75.5668, loss: 0.02235946\n",
      "Epoch: [14] [  11/  20] time: 75.8092, loss: 0.02522832\n",
      "Epoch: [14] [  12/  20] time: 76.0519, loss: 0.02553459\n",
      "Epoch: [14] [  13/  20] time: 76.2930, loss: 0.01951435\n",
      "Epoch: [14] [  14/  20] time: 76.5340, loss: 0.03491276\n",
      "Epoch: [14] [  15/  20] time: 76.7757, loss: 0.04174072\n",
      "Epoch: [14] [  16/  20] time: 77.0190, loss: 0.00268186\n",
      "Epoch: [14] [  17/  20] time: 77.2600, loss: 0.01551319\n",
      "Epoch: [14] [  18/  20] time: 77.5015, loss: 0.01873489\n",
      "Epoch: [14] [  19/  20] time: 77.7436, loss: 0.02904757\n",
      "[14/50] - ptime: 5.1583 loss: 0.02484113 acc: 0.44000 lr: 0.00090000\n",
      "Epoch: [15] [   0/  20] time: 78.6417, loss: 0.01126489\n",
      "Epoch: [15] [   1/  20] time: 78.8826, loss: 0.00620714\n",
      "Epoch: [15] [   2/  20] time: 79.1250, loss: 0.03666521\n",
      "Epoch: [15] [   3/  20] time: 79.3664, loss: 0.01721359\n",
      "Epoch: [15] [   4/  20] time: 79.6076, loss: 0.00246420\n",
      "Epoch: [15] [   5/  20] time: 79.8513, loss: 0.03788280\n",
      "Epoch: [15] [   6/  20] time: 80.0943, loss: 0.02298191\n",
      "Epoch: [15] [   7/  20] time: 80.3370, loss: 0.01234895\n",
      "Epoch: [15] [   8/  20] time: 80.5785, loss: 0.01213107\n",
      "Epoch: [15] [   9/  20] time: 80.8215, loss: 0.04582376\n",
      "Epoch: [15] [  10/  20] time: 81.0636, loss: 0.01891883\n",
      "Epoch: [15] [  11/  20] time: 81.3045, loss: 0.01507823\n",
      "Epoch: [15] [  12/  20] time: 81.5459, loss: 0.00549055\n",
      "Epoch: [15] [  13/  20] time: 81.7879, loss: 0.01077022\n",
      "Epoch: [15] [  14/  20] time: 82.0315, loss: 0.01279910\n",
      "Epoch: [15] [  15/  20] time: 82.2724, loss: 0.02750326\n",
      "Epoch: [15] [  16/  20] time: 82.5145, loss: 0.00589258\n",
      "Epoch: [15] [  17/  20] time: 82.7566, loss: 0.00619708\n",
      "Epoch: [15] [  18/  20] time: 82.9980, loss: 0.01664604\n",
      "Epoch: [15] [  19/  20] time: 83.2401, loss: 0.02537209\n",
      "[15/50] - ptime: 5.1592 loss: 0.01748258 acc: 0.70000 lr: 0.00090000\n",
      "Epoch: [16] [   0/  20] time: 84.2103, loss: 0.01052530\n",
      "Epoch: [16] [   1/  20] time: 84.4505, loss: 0.00801043\n",
      "Epoch: [16] [   2/  20] time: 84.6936, loss: 0.00906552\n",
      "Epoch: [16] [   3/  20] time: 84.9360, loss: 0.00152564\n",
      "Epoch: [16] [   4/  20] time: 85.1765, loss: 0.02948878\n",
      "Epoch: [16] [   5/  20] time: 85.4171, loss: 0.01247868\n",
      "Epoch: [16] [   6/  20] time: 85.6581, loss: 0.02918920\n",
      "Epoch: [16] [   7/  20] time: 85.9015, loss: 0.00793008\n",
      "Epoch: [16] [   8/  20] time: 86.1433, loss: 0.00017153\n",
      "Epoch: [16] [   9/  20] time: 86.3855, loss: 0.02895690\n",
      "Epoch: [16] [  10/  20] time: 86.6270, loss: 0.05551285\n",
      "Epoch: [16] [  11/  20] time: 86.8696, loss: 0.01900567\n",
      "Epoch: [16] [  12/  20] time: 87.1129, loss: 0.01247332\n",
      "Epoch: [16] [  13/  20] time: 87.3547, loss: 0.03351456\n",
      "Epoch: [16] [  14/  20] time: 87.5965, loss: 0.01512994\n",
      "Epoch: [16] [  15/  20] time: 87.8396, loss: 0.01071446\n",
      "Epoch: [16] [  16/  20] time: 88.0839, loss: 0.00957246\n",
      "Epoch: [16] [  17/  20] time: 88.3252, loss: 0.00031359\n",
      "Epoch: [16] [  18/  20] time: 88.5660, loss: 0.00305248\n",
      "Epoch: [16] [  19/  20] time: 88.8078, loss: 0.01908776\n",
      "[16/50] - ptime: 5.1557 loss: 0.01578596 acc: 0.55000 lr: 0.00090000\n",
      "Epoch: [17] [   0/  20] time: 89.7136, loss: 0.00002156\n",
      "Epoch: [17] [   1/  20] time: 89.9542, loss: 0.00536608\n",
      "Epoch: [17] [   2/  20] time: 90.1939, loss: 0.03043915\n",
      "Epoch: [17] [   3/  20] time: 90.4345, loss: 0.01370772\n",
      "Epoch: [17] [   4/  20] time: 90.6750, loss: 0.01373503\n",
      "Epoch: [17] [   5/  20] time: 90.9173, loss: 0.01621096\n",
      "Epoch: [17] [   6/  20] time: 91.1578, loss: 0.00972016\n",
      "Epoch: [17] [   7/  20] time: 91.3996, loss: 0.03854781\n",
      "Epoch: [17] [   8/  20] time: 91.6408, loss: 0.00981601\n",
      "Epoch: [17] [   9/  20] time: 91.8836, loss: 0.00935709\n",
      "Epoch: [17] [  10/  20] time: 92.1257, loss: 0.04848284\n",
      "Epoch: [17] [  11/  20] time: 92.3669, loss: 0.02341725\n",
      "Epoch: [17] [  12/  20] time: 92.6082, loss: 0.00396748\n",
      "Epoch: [17] [  13/  20] time: 92.8520, loss: 0.02299234\n",
      "Epoch: [17] [  14/  20] time: 93.0957, loss: 0.03625924\n",
      "Epoch: [17] [  15/  20] time: 93.3381, loss: 0.01396198\n",
      "Epoch: [17] [  16/  20] time: 93.5794, loss: 0.00786669\n",
      "Epoch: [17] [  17/  20] time: 93.8235, loss: 0.04661885\n",
      "Epoch: [17] [  18/  20] time: 94.0659, loss: 0.01412948\n",
      "Epoch: [17] [  19/  20] time: 94.3061, loss: 0.01350240\n",
      "[17/50] - ptime: 5.1506 loss: 0.01890600 acc: 0.48000 lr: 0.00090000\n",
      "Epoch: [18] [   0/  20] time: 95.2078, loss: 0.01884486\n",
      "Epoch: [18] [   1/  20] time: 95.4472, loss: 0.01243037\n",
      "Epoch: [18] [   2/  20] time: 95.6878, loss: 0.01118563\n",
      "Epoch: [18] [   3/  20] time: 95.9292, loss: 0.01376402\n",
      "Epoch: [18] [   4/  20] time: 96.1697, loss: 0.00469053\n",
      "Epoch: [18] [   5/  20] time: 96.4098, loss: 0.01255095\n",
      "Epoch: [18] [   6/  20] time: 96.6509, loss: 0.00939577\n",
      "Epoch: [18] [   7/  20] time: 96.8924, loss: 0.00528911\n",
      "Epoch: [18] [   8/  20] time: 97.1342, loss: 0.00233612\n",
      "Epoch: [18] [   9/  20] time: 97.3749, loss: 0.00730300\n",
      "Epoch: [18] [  10/  20] time: 97.6176, loss: 0.03340736\n",
      "Epoch: [18] [  11/  20] time: 97.8606, loss: 0.02800142\n",
      "Epoch: [18] [  12/  20] time: 98.1044, loss: 0.01844601\n",
      "Epoch: [18] [  13/  20] time: 98.3464, loss: 0.01225597\n",
      "Epoch: [18] [  14/  20] time: 98.5879, loss: 0.00483704\n",
      "Epoch: [18] [  15/  20] time: 98.8306, loss: 0.00816981\n",
      "Epoch: [18] [  16/  20] time: 99.0772, loss: 0.01187510\n",
      "Epoch: [18] [  17/  20] time: 99.3176, loss: 0.00409286\n",
      "Epoch: [18] [  18/  20] time: 99.5579, loss: 0.01045550\n",
      "Epoch: [18] [  19/  20] time: 99.8009, loss: 0.01280058\n",
      "[18/50] - ptime: 5.1519 loss: 0.01210660 acc: 0.62000 lr: 0.00090000\n",
      "Epoch: [19] [   0/  20] time: 100.7014, loss: 0.00732397\n",
      "Epoch: [19] [   1/  20] time: 100.9416, loss: 0.00083794\n",
      "Epoch: [19] [   2/  20] time: 101.1825, loss: 0.00464918\n",
      "Epoch: [19] [   3/  20] time: 101.4228, loss: 0.01033894\n",
      "Epoch: [19] [   4/  20] time: 101.6633, loss: 0.00470856\n",
      "Epoch: [19] [   5/  20] time: 101.9057, loss: 0.00013874\n",
      "Epoch: [19] [   6/  20] time: 102.1475, loss: 0.00110859\n",
      "Epoch: [19] [   7/  20] time: 102.3924, loss: 0.00059839\n",
      "Epoch: [19] [   8/  20] time: 102.6412, loss: 0.01664336\n",
      "Epoch: [19] [   9/  20] time: 102.8987, loss: 0.01041788\n",
      "Epoch: [19] [  10/  20] time: 103.1408, loss: 0.00469532\n",
      "Epoch: [19] [  11/  20] time: 103.3835, loss: 0.00248322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19] [  12/  20] time: 103.6238, loss: 0.01698836\n",
      "Epoch: [19] [  13/  20] time: 103.8666, loss: 0.01064921\n",
      "Epoch: [19] [  14/  20] time: 104.1088, loss: 0.01718762\n",
      "Epoch: [19] [  15/  20] time: 104.3494, loss: 0.00958062\n",
      "Epoch: [19] [  16/  20] time: 104.5928, loss: 0.00865076\n",
      "Epoch: [19] [  17/  20] time: 104.8409, loss: 0.00556731\n",
      "Epoch: [19] [  18/  20] time: 105.0844, loss: 0.01060153\n",
      "Epoch: [19] [  19/  20] time: 105.3263, loss: 0.01180122\n",
      "[19/50] - ptime: 5.1850 loss: 0.00774854 acc: 0.69000 lr: 0.00090000\n",
      "Epoch: [20] [   0/  20] time: 106.2264, loss: 0.00872159\n",
      "Epoch: [20] [   1/  20] time: 106.4676, loss: 0.00530761\n",
      "Epoch: [20] [   2/  20] time: 106.7095, loss: 0.00512472\n",
      "Epoch: [20] [   3/  20] time: 106.9520, loss: 0.01703656\n",
      "Epoch: [20] [   4/  20] time: 107.1939, loss: 0.01217368\n",
      "Epoch: [20] [   5/  20] time: 107.4365, loss: 0.00643317\n",
      "Epoch: [20] [   6/  20] time: 107.6773, loss: 0.00235050\n",
      "Epoch: [20] [   7/  20] time: 107.9189, loss: 0.00014049\n",
      "Epoch: [20] [   8/  20] time: 108.1625, loss: 0.00748990\n",
      "Epoch: [20] [   9/  20] time: 108.4037, loss: 0.00104473\n",
      "Epoch: [20] [  10/  20] time: 108.6448, loss: 0.00453622\n",
      "Epoch: [20] [  11/  20] time: 108.8877, loss: 0.02509252\n",
      "Epoch: [20] [  12/  20] time: 109.1312, loss: 0.05792241\n",
      "Epoch: [20] [  13/  20] time: 109.3732, loss: 0.04697240\n",
      "Epoch: [20] [  14/  20] time: 109.6145, loss: 0.01785919\n",
      "Epoch: [20] [  15/  20] time: 109.8577, loss: 0.00601722\n",
      "Epoch: [20] [  16/  20] time: 110.1009, loss: 0.00691427\n",
      "Epoch: [20] [  17/  20] time: 110.3434, loss: 0.00582263\n",
      "Epoch: [20] [  18/  20] time: 110.5856, loss: 0.01590701\n",
      "Epoch: [20] [  19/  20] time: 110.8287, loss: 0.00908284\n",
      "[20/50] - ptime: 5.1617 loss: 0.01309748 acc: 0.70000 lr: 0.00081000\n",
      "Epoch: [21] [   0/  20] time: 111.7402, loss: 0.00056782\n",
      "Epoch: [21] [   1/  20] time: 111.9807, loss: 0.03431853\n",
      "Epoch: [21] [   2/  20] time: 112.2229, loss: 0.00710032\n",
      "Epoch: [21] [   3/  20] time: 112.4641, loss: 0.02567569\n",
      "Epoch: [21] [   4/  20] time: 112.7061, loss: 0.00399532\n",
      "Epoch: [21] [   5/  20] time: 112.9481, loss: 0.00646184\n",
      "Epoch: [21] [   6/  20] time: 113.1901, loss: 0.00673538\n",
      "Epoch: [21] [   7/  20] time: 113.4324, loss: 0.02442054\n",
      "Epoch: [21] [   8/  20] time: 113.6743, loss: 0.00798423\n",
      "Epoch: [21] [   9/  20] time: 113.9184, loss: 0.02314525\n",
      "Epoch: [21] [  10/  20] time: 114.1605, loss: 0.03267889\n",
      "Epoch: [21] [  11/  20] time: 114.4022, loss: 0.00325025\n",
      "Epoch: [21] [  12/  20] time: 114.6433, loss: 0.00815723\n",
      "Epoch: [21] [  13/  20] time: 114.8851, loss: 0.00825339\n",
      "Epoch: [21] [  14/  20] time: 115.1281, loss: 0.01022832\n",
      "Epoch: [21] [  15/  20] time: 115.3691, loss: 0.00122084\n",
      "Epoch: [21] [  16/  20] time: 115.6101, loss: 0.00020376\n",
      "Epoch: [21] [  17/  20] time: 115.8533, loss: 0.02046274\n",
      "Epoch: [21] [  18/  20] time: 116.0977, loss: 0.00005767\n",
      "Epoch: [21] [  19/  20] time: 116.3389, loss: 0.00708123\n",
      "[21/50] - ptime: 5.1582 loss: 0.01159996 acc: 0.71000 lr: 0.00081000\n",
      "Epoch: [22] [   0/  20] time: 117.2425, loss: 0.00156062\n",
      "Epoch: [22] [   1/  20] time: 117.4828, loss: 0.02277015\n",
      "Epoch: [22] [   2/  20] time: 117.7260, loss: 0.03850386\n",
      "Epoch: [22] [   3/  20] time: 117.9693, loss: 0.00789428\n",
      "Epoch: [22] [   4/  20] time: 118.2104, loss: 0.00135531\n",
      "Epoch: [22] [   5/  20] time: 118.4527, loss: 0.01216258\n",
      "Epoch: [22] [   6/  20] time: 118.6937, loss: 0.00006528\n",
      "Epoch: [22] [   7/  20] time: 118.9358, loss: 0.00075199\n",
      "Epoch: [22] [   8/  20] time: 119.1775, loss: 0.00209172\n",
      "Epoch: [22] [   9/  20] time: 119.4183, loss: 0.00320849\n",
      "Epoch: [22] [  10/  20] time: 119.6595, loss: 0.06983727\n",
      "Epoch: [22] [  11/  20] time: 119.9040, loss: 0.08039098\n",
      "Epoch: [22] [  12/  20] time: 120.1462, loss: 0.04058045\n",
      "Epoch: [22] [  13/  20] time: 120.3881, loss: 0.02032547\n",
      "Epoch: [22] [  14/  20] time: 120.6305, loss: 0.31647012\n",
      "Epoch: [22] [  15/  20] time: 120.8739, loss: 0.00783711\n",
      "Epoch: [22] [  16/  20] time: 121.1175, loss: 0.06339499\n",
      "Epoch: [22] [  17/  20] time: 121.3591, loss: 0.01124576\n",
      "Epoch: [22] [  18/  20] time: 121.6010, loss: 0.10381234\n",
      "Epoch: [22] [  19/  20] time: 121.8432, loss: 0.01444123\n",
      "[22/50] - ptime: 5.1590 loss: 0.04093501 acc: 0.74000 lr: 0.00081000\n",
      "Epoch: [23] [   0/  20] time: 122.7398, loss: 0.02635356\n",
      "Epoch: [23] [   1/  20] time: 122.9828, loss: 0.03056487\n",
      "Epoch: [23] [   2/  20] time: 123.2242, loss: 0.15272148\n",
      "Epoch: [23] [   3/  20] time: 123.4666, loss: 0.00423893\n",
      "Epoch: [23] [   4/  20] time: 123.7070, loss: 0.00018429\n",
      "Epoch: [23] [   5/  20] time: 123.9497, loss: 0.04490274\n",
      "Epoch: [23] [   6/  20] time: 124.1914, loss: 0.03129413\n",
      "Epoch: [23] [   7/  20] time: 124.4327, loss: 0.01732422\n",
      "Epoch: [23] [   8/  20] time: 124.6732, loss: 0.00036612\n",
      "Epoch: [23] [   9/  20] time: 124.9163, loss: 0.01837765\n",
      "Epoch: [23] [  10/  20] time: 125.1580, loss: 0.03211423\n",
      "Epoch: [23] [  11/  20] time: 125.3992, loss: 0.01021607\n",
      "Epoch: [23] [  12/  20] time: 125.6403, loss: 0.00867545\n",
      "Epoch: [23] [  13/  20] time: 125.8824, loss: 0.04241198\n",
      "Epoch: [23] [  14/  20] time: 126.1268, loss: 0.00412968\n",
      "Epoch: [23] [  15/  20] time: 126.3684, loss: 0.03405457\n",
      "Epoch: [23] [  16/  20] time: 126.6099, loss: 0.01177970\n",
      "Epoch: [23] [  17/  20] time: 126.8544, loss: 0.01234091\n",
      "Epoch: [23] [  18/  20] time: 127.0979, loss: 0.01031229\n",
      "Epoch: [23] [  19/  20] time: 127.3404, loss: 0.00443923\n",
      "[23/50] - ptime: 5.1601 loss: 0.02484011 acc: 0.71000 lr: 0.00081000\n",
      "Epoch: [24] [   0/  20] time: 128.2400, loss: 0.00819995\n",
      "Epoch: [24] [   1/  20] time: 128.4809, loss: 0.00723797\n",
      "Epoch: [24] [   2/  20] time: 128.7227, loss: 0.00513770\n",
      "Epoch: [24] [   3/  20] time: 128.9642, loss: 0.01260421\n",
      "Epoch: [24] [   4/  20] time: 129.2064, loss: 0.02053924\n",
      "Epoch: [24] [   5/  20] time: 129.4483, loss: 0.00716077\n",
      "Epoch: [24] [   6/  20] time: 129.6889, loss: 0.03106947\n",
      "Epoch: [24] [   7/  20] time: 129.9317, loss: 0.02914665\n",
      "Epoch: [24] [   8/  20] time: 130.1741, loss: 0.00294068\n",
      "Epoch: [24] [   9/  20] time: 130.4153, loss: 0.00526135\n",
      "Epoch: [24] [  10/  20] time: 130.6565, loss: 0.01962968\n",
      "Epoch: [24] [  11/  20] time: 130.8993, loss: 0.00028528\n",
      "Epoch: [24] [  12/  20] time: 131.1430, loss: 0.00764217\n",
      "Epoch: [24] [  13/  20] time: 131.3848, loss: 0.01008386\n",
      "Epoch: [24] [  14/  20] time: 131.6262, loss: 0.00172802\n",
      "Epoch: [24] [  15/  20] time: 131.8696, loss: 0.00122227\n",
      "Epoch: [24] [  16/  20] time: 132.1141, loss: 0.01088719\n",
      "Epoch: [24] [  17/  20] time: 132.3563, loss: 0.00373326\n",
      "Epoch: [24] [  18/  20] time: 132.5981, loss: 0.01360584\n",
      "Epoch: [24] [  19/  20] time: 132.8411, loss: 0.00816338\n",
      "[24/50] - ptime: 5.1610 loss: 0.01031395 acc: 0.71000 lr: 0.00081000\n",
      "Epoch: [25] [   0/  20] time: 133.7449, loss: 0.00304847\n",
      "Epoch: [25] [   1/  20] time: 133.9863, loss: 0.00204857\n",
      "Epoch: [25] [   2/  20] time: 134.2272, loss: 0.00628657\n",
      "Epoch: [25] [   3/  20] time: 134.4680, loss: 0.00652414\n",
      "Epoch: [25] [   4/  20] time: 134.7090, loss: 0.00737156\n",
      "Epoch: [25] [   5/  20] time: 134.9501, loss: 0.00099050\n",
      "Epoch: [25] [   6/  20] time: 135.1913, loss: 0.00548044\n",
      "Epoch: [25] [   7/  20] time: 135.4324, loss: 0.03856161\n",
      "Epoch: [25] [   8/  20] time: 135.6732, loss: 0.00655353\n",
      "Epoch: [25] [   9/  20] time: 135.9156, loss: 0.01004577\n",
      "Epoch: [25] [  10/  20] time: 136.1595, loss: 0.00631987\n",
      "Epoch: [25] [  11/  20] time: 136.4009, loss: 0.00000297\n",
      "Epoch: [25] [  12/  20] time: 136.6423, loss: 0.00220230\n",
      "Epoch: [25] [  13/  20] time: 136.8866, loss: 0.01196302\n",
      "Epoch: [25] [  14/  20] time: 137.1305, loss: 0.02200770\n",
      "Epoch: [25] [  15/  20] time: 137.3733, loss: 0.00886834\n",
      "Epoch: [25] [  16/  20] time: 137.6139, loss: 0.00511338\n",
      "Epoch: [25] [  17/  20] time: 137.8587, loss: 0.00031790\n",
      "Epoch: [25] [  18/  20] time: 138.1018, loss: 0.00065271\n",
      "Epoch: [25] [  19/  20] time: 138.3426, loss: 0.00030993\n",
      "[25/50] - ptime: 5.1576 loss: 0.00723346 acc: 0.73000 lr: 0.00081000\n",
      "Epoch: [26] [   0/  20] time: 139.2778, loss: 0.00232865\n",
      "Epoch: [26] [   1/  20] time: 139.5178, loss: 0.00000297\n",
      "Epoch: [26] [   2/  20] time: 139.7604, loss: 0.00885553\n",
      "Epoch: [26] [   3/  20] time: 140.0016, loss: 0.00099480\n",
      "Epoch: [26] [   4/  20] time: 140.2418, loss: 0.00030819\n",
      "Epoch: [26] [   5/  20] time: 140.4828, loss: 0.00780620\n",
      "Epoch: [26] [   6/  20] time: 140.7243, loss: 0.00241940\n",
      "Epoch: [26] [   7/  20] time: 140.9651, loss: 0.00189237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26] [   8/  20] time: 141.2073, loss: 0.00405416\n",
      "Epoch: [26] [   9/  20] time: 141.4480, loss: 0.03468641\n",
      "Epoch: [26] [  10/  20] time: 141.6890, loss: 0.01032505\n",
      "Epoch: [26] [  11/  20] time: 141.9302, loss: 0.00619538\n",
      "Epoch: [26] [  12/  20] time: 142.1717, loss: 0.00087243\n",
      "Epoch: [26] [  13/  20] time: 142.4138, loss: 0.01110187\n",
      "Epoch: [26] [  14/  20] time: 142.6552, loss: 0.01058914\n",
      "Epoch: [26] [  15/  20] time: 142.8981, loss: 0.07469320\n",
      "Epoch: [26] [  16/  20] time: 143.1416, loss: 0.02596756\n",
      "Epoch: [26] [  17/  20] time: 143.3843, loss: 0.00183906\n",
      "Epoch: [26] [  18/  20] time: 143.6257, loss: 0.00742525\n",
      "Epoch: [26] [  19/  20] time: 143.8697, loss: 0.02007992\n",
      "[26/50] - ptime: 5.1581 loss: 0.01162188 acc: 0.79000 lr: 0.00081000\n",
      "Epoch: [27] [   0/  20] time: 144.7768, loss: 0.01136899\n",
      "Epoch: [27] [   1/  20] time: 145.0170, loss: 0.01174152\n",
      "Epoch: [27] [   2/  20] time: 145.2576, loss: 0.00396871\n",
      "Epoch: [27] [   3/  20] time: 145.4992, loss: 0.00771452\n",
      "Epoch: [27] [   4/  20] time: 145.7406, loss: 0.01230443\n",
      "Epoch: [27] [   5/  20] time: 145.9812, loss: 0.00357097\n",
      "Epoch: [27] [   6/  20] time: 146.2240, loss: 0.02103278\n",
      "Epoch: [27] [   7/  20] time: 146.4646, loss: 0.00247925\n",
      "Epoch: [27] [   8/  20] time: 146.7059, loss: 0.00997952\n",
      "Epoch: [27] [   9/  20] time: 146.9485, loss: 0.01065705\n",
      "Epoch: [27] [  10/  20] time: 147.1904, loss: 0.00122416\n",
      "Epoch: [27] [  11/  20] time: 147.4333, loss: 0.01165465\n",
      "Epoch: [27] [  12/  20] time: 147.6741, loss: 0.03631958\n",
      "Epoch: [27] [  13/  20] time: 147.9176, loss: 0.00734876\n",
      "Epoch: [27] [  14/  20] time: 148.1619, loss: 0.01348558\n",
      "Epoch: [27] [  15/  20] time: 148.4051, loss: 0.00447767\n",
      "Epoch: [27] [  16/  20] time: 148.6462, loss: 0.00738426\n",
      "Epoch: [27] [  17/  20] time: 148.8891, loss: 0.01637057\n",
      "Epoch: [27] [  18/  20] time: 149.1320, loss: 0.00232232\n",
      "Epoch: [27] [  19/  20] time: 149.3733, loss: 0.03213966\n",
      "[27/50] - ptime: 5.1544 loss: 0.01137725 acc: 0.80000 lr: 0.00081000\n",
      "Epoch: [28] [   0/  20] time: 150.2860, loss: 0.00302067\n",
      "Epoch: [28] [   1/  20] time: 150.5264, loss: 0.00477816\n",
      "Epoch: [28] [   2/  20] time: 150.7717, loss: 0.00318978\n",
      "Epoch: [28] [   3/  20] time: 151.0154, loss: 0.00357461\n",
      "Epoch: [28] [   4/  20] time: 151.2562, loss: 0.00641946\n",
      "Epoch: [28] [   5/  20] time: 151.4970, loss: 0.00290703\n",
      "Epoch: [28] [   6/  20] time: 151.7388, loss: 0.00835952\n",
      "Epoch: [28] [   7/  20] time: 151.9793, loss: 0.00780145\n",
      "Epoch: [28] [   8/  20] time: 152.2211, loss: 0.00123970\n",
      "Epoch: [28] [   9/  20] time: 152.4624, loss: 0.00270457\n",
      "Epoch: [28] [  10/  20] time: 152.7045, loss: 0.00000986\n",
      "Epoch: [28] [  11/  20] time: 152.9472, loss: 0.00324275\n",
      "Epoch: [28] [  12/  20] time: 153.1923, loss: 0.00308625\n",
      "Epoch: [28] [  13/  20] time: 153.4344, loss: 0.01145301\n",
      "Epoch: [28] [  14/  20] time: 153.6760, loss: 0.00449042\n",
      "Epoch: [28] [  15/  20] time: 153.9206, loss: 0.00155961\n",
      "Epoch: [28] [  16/  20] time: 154.1635, loss: 0.00020785\n",
      "Epoch: [28] [  17/  20] time: 154.4047, loss: 0.00380805\n",
      "Epoch: [28] [  18/  20] time: 154.6458, loss: 0.01783754\n",
      "Epoch: [28] [  19/  20] time: 154.8885, loss: 0.00364378\n",
      "[28/50] - ptime: 5.1607 loss: 0.00466670 acc: 0.77000 lr: 0.00081000\n",
      "Epoch: [29] [   0/  20] time: 155.8337, loss: 0.00163933\n",
      "Epoch: [29] [   1/  20] time: 156.0743, loss: 0.00017661\n",
      "Epoch: [29] [   2/  20] time: 156.3161, loss: 0.00245323\n",
      "Epoch: [29] [   3/  20] time: 156.5582, loss: 0.00253624\n",
      "Epoch: [29] [   4/  20] time: 156.8019, loss: 0.00062644\n",
      "Epoch: [29] [   5/  20] time: 157.0438, loss: 0.01297443\n",
      "Epoch: [29] [   6/  20] time: 157.2857, loss: 0.00778020\n",
      "Epoch: [29] [   7/  20] time: 157.5268, loss: 0.00007819\n",
      "Epoch: [29] [   8/  20] time: 157.7708, loss: 0.00088717\n",
      "Epoch: [29] [   9/  20] time: 158.0117, loss: 0.00135480\n",
      "Epoch: [29] [  10/  20] time: 158.2541, loss: 0.00020159\n",
      "Epoch: [29] [  11/  20] time: 158.4954, loss: 0.01646487\n",
      "Epoch: [29] [  12/  20] time: 158.7371, loss: 0.00000108\n",
      "Epoch: [29] [  13/  20] time: 158.9799, loss: 0.00002077\n",
      "Epoch: [29] [  14/  20] time: 159.2263, loss: 0.00166769\n",
      "Epoch: [29] [  15/  20] time: 159.4684, loss: 0.00235836\n",
      "Epoch: [29] [  16/  20] time: 159.7097, loss: 0.00779309\n",
      "Epoch: [29] [  17/  20] time: 159.9519, loss: 0.00003429\n",
      "Epoch: [29] [  18/  20] time: 160.1953, loss: 0.00814406\n",
      "Epoch: [29] [  19/  20] time: 160.4374, loss: 0.00266845\n",
      "[29/50] - ptime: 5.1676 loss: 0.00349304 acc: 0.76000 lr: 0.00081000\n",
      "Epoch: [30] [   0/  20] time: 161.3395, loss: 0.00000002\n",
      "Epoch: [30] [   1/  20] time: 161.5817, loss: 0.00531738\n",
      "Epoch: [30] [   2/  20] time: 161.8261, loss: 0.00166782\n",
      "Epoch: [30] [   3/  20] time: 162.0685, loss: 0.00224442\n",
      "Epoch: [30] [   4/  20] time: 162.3098, loss: 0.01295656\n",
      "Epoch: [30] [   5/  20] time: 162.5515, loss: 0.00096039\n",
      "Epoch: [30] [   6/  20] time: 162.7937, loss: 0.00104136\n",
      "Epoch: [30] [   7/  20] time: 163.0348, loss: 0.00189564\n",
      "Epoch: [30] [   8/  20] time: 163.2717, loss: 0.00081480\n",
      "Epoch: [30] [   9/  20] time: 163.5109, loss: 0.00063954\n",
      "Epoch: [30] [  10/  20] time: 163.7531, loss: 0.00573739\n",
      "Epoch: [30] [  11/  20] time: 163.9957, loss: 0.00035738\n",
      "Epoch: [30] [  12/  20] time: 164.2391, loss: 0.00048455\n",
      "Epoch: [30] [  13/  20] time: 164.4808, loss: 0.01424407\n",
      "Epoch: [30] [  14/  20] time: 164.7229, loss: 0.00275211\n",
      "Epoch: [30] [  15/  20] time: 164.9638, loss: 0.00140998\n",
      "Epoch: [30] [  16/  20] time: 165.2077, loss: 0.00588621\n",
      "Epoch: [30] [  17/  20] time: 165.4503, loss: 0.00093958\n",
      "Epoch: [30] [  18/  20] time: 165.6923, loss: 0.00044943\n",
      "Epoch: [30] [  19/  20] time: 165.9350, loss: 0.01018043\n",
      "[30/50] - ptime: 5.1556 loss: 0.00349895 acc: 0.73000 lr: 0.00081000\n",
      "Epoch: [31] [   0/  20] time: 166.9248, loss: 0.00731466\n",
      "Epoch: [31] [   1/  20] time: 167.1658, loss: 0.00529695\n",
      "Epoch: [31] [   2/  20] time: 167.4068, loss: 0.00010863\n",
      "Epoch: [31] [   3/  20] time: 167.6476, loss: 0.00086341\n",
      "Epoch: [31] [   4/  20] time: 167.8922, loss: 0.00045387\n",
      "Epoch: [31] [   5/  20] time: 168.1352, loss: 0.00350290\n",
      "Epoch: [31] [   6/  20] time: 168.3785, loss: 0.00028400\n",
      "Epoch: [31] [   7/  20] time: 168.6206, loss: 0.00004138\n",
      "Epoch: [31] [   8/  20] time: 168.8647, loss: 0.00556504\n",
      "Epoch: [31] [   9/  20] time: 169.1089, loss: 0.00092763\n",
      "Epoch: [31] [  10/  20] time: 169.3513, loss: 0.00214419\n",
      "Epoch: [31] [  11/  20] time: 169.5921, loss: 0.00021323\n",
      "Epoch: [31] [  12/  20] time: 169.8343, loss: 0.00000187\n",
      "Epoch: [31] [  13/  20] time: 170.0779, loss: 0.00223198\n",
      "Epoch: [31] [  14/  20] time: 170.3207, loss: 0.00299441\n",
      "Epoch: [31] [  15/  20] time: 170.5623, loss: 0.00306686\n",
      "Epoch: [31] [  16/  20] time: 170.8068, loss: 0.00039815\n",
      "Epoch: [31] [  17/  20] time: 171.0501, loss: 0.00024933\n",
      "Epoch: [31] [  18/  20] time: 171.2917, loss: 0.00981307\n",
      "Epoch: [31] [  19/  20] time: 171.5343, loss: 0.00044461\n",
      "[31/50] - ptime: 5.1690 loss: 0.00229581 acc: 0.63000 lr: 0.00072900\n",
      "Epoch: [32] [   0/  20] time: 172.4480, loss: 0.00398501\n",
      "Epoch: [32] [   1/  20] time: 172.6886, loss: 0.04297382\n",
      "Epoch: [32] [   2/  20] time: 172.9307, loss: 0.00797893\n",
      "Epoch: [32] [   3/  20] time: 173.1750, loss: 0.00070776\n",
      "Epoch: [32] [   4/  20] time: 173.4170, loss: 0.01664735\n",
      "Epoch: [32] [   5/  20] time: 173.6606, loss: 0.04114238\n",
      "Epoch: [32] [   6/  20] time: 173.9051, loss: 0.00130796\n",
      "Epoch: [32] [   7/  20] time: 174.1484, loss: 0.00586074\n",
      "Epoch: [32] [   8/  20] time: 174.3901, loss: 0.00336008\n",
      "Epoch: [32] [   9/  20] time: 174.6311, loss: 0.00002868\n",
      "Epoch: [32] [  10/  20] time: 174.8755, loss: 0.00337563\n",
      "Epoch: [32] [  11/  20] time: 175.1172, loss: 0.00009104\n",
      "Epoch: [32] [  12/  20] time: 175.3597, loss: 0.00151700\n",
      "Epoch: [32] [  13/  20] time: 175.6014, loss: 0.00032960\n",
      "Epoch: [32] [  14/  20] time: 175.8449, loss: 0.00000297\n",
      "Epoch: [32] [  15/  20] time: 176.0878, loss: 0.01931798\n",
      "Epoch: [32] [  16/  20] time: 176.3300, loss: 0.00655035\n",
      "Epoch: [32] [  17/  20] time: 176.5725, loss: 0.04211855\n",
      "Epoch: [32] [  18/  20] time: 176.8156, loss: 0.00099296\n",
      "Epoch: [32] [  19/  20] time: 177.0575, loss: 0.01267168\n",
      "[32/50] - ptime: 5.1692 loss: 0.01054802 acc: 0.56000 lr: 0.00072900\n",
      "Epoch: [33] [   0/  20] time: 177.9740, loss: 0.00160397\n",
      "Epoch: [33] [   1/  20] time: 178.2158, loss: 0.00182597\n",
      "Epoch: [33] [   2/  20] time: 178.4570, loss: 0.01128363\n",
      "Epoch: [33] [   3/  20] time: 178.6988, loss: 0.00120078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33] [   4/  20] time: 178.9445, loss: 0.01300298\n",
      "Epoch: [33] [   5/  20] time: 179.1878, loss: 0.00255787\n",
      "Epoch: [33] [   6/  20] time: 179.4291, loss: 0.00457131\n",
      "Epoch: [33] [   7/  20] time: 179.6720, loss: 0.00005373\n",
      "Epoch: [33] [   8/  20] time: 179.9159, loss: 0.00121061\n",
      "Epoch: [33] [   9/  20] time: 180.1588, loss: 0.00709857\n",
      "Epoch: [33] [  10/  20] time: 180.4009, loss: 0.00480068\n",
      "Epoch: [33] [  11/  20] time: 180.6437, loss: 0.00980325\n",
      "Epoch: [33] [  12/  20] time: 180.8869, loss: 0.00164832\n",
      "Epoch: [33] [  13/  20] time: 181.1282, loss: 0.00685318\n",
      "Epoch: [33] [  14/  20] time: 181.3694, loss: 0.00176657\n",
      "Epoch: [33] [  15/  20] time: 181.6103, loss: 0.00129133\n",
      "Epoch: [33] [  16/  20] time: 181.8529, loss: 0.00015370\n",
      "Epoch: [33] [  17/  20] time: 182.0939, loss: 0.00129503\n",
      "Epoch: [33] [  18/  20] time: 182.3359, loss: 0.00011580\n",
      "Epoch: [33] [  19/  20] time: 182.5774, loss: 0.00063242\n",
      "[33/50] - ptime: 5.1615 loss: 0.00363849 acc: 0.60000 lr: 0.00072900\n",
      "Epoch: [34] [   0/  20] time: 183.5114, loss: 0.01131383\n",
      "Epoch: [34] [   1/  20] time: 183.7533, loss: 0.00045756\n",
      "Epoch: [34] [   2/  20] time: 183.9970, loss: 0.00028741\n",
      "Epoch: [34] [   3/  20] time: 184.2398, loss: 0.00034838\n",
      "Epoch: [34] [   4/  20] time: 184.4821, loss: 0.00076304\n",
      "Epoch: [34] [   5/  20] time: 184.7247, loss: 0.00010119\n",
      "Epoch: [34] [   6/  20] time: 184.9658, loss: 0.00002902\n",
      "Epoch: [34] [   7/  20] time: 185.2100, loss: 0.00093105\n",
      "Epoch: [34] [   8/  20] time: 185.4527, loss: 0.00000320\n",
      "Epoch: [34] [   9/  20] time: 185.6958, loss: 0.00344784\n",
      "Epoch: [34] [  10/  20] time: 185.9394, loss: 0.00058838\n",
      "Epoch: [34] [  11/  20] time: 186.1803, loss: 0.00000736\n",
      "Epoch: [34] [  12/  20] time: 186.4219, loss: 0.00073601\n",
      "Epoch: [34] [  13/  20] time: 186.6628, loss: 0.00457673\n",
      "Epoch: [34] [  14/  20] time: 186.9057, loss: 0.00004109\n",
      "Epoch: [34] [  15/  20] time: 187.1475, loss: 0.00000492\n",
      "Epoch: [34] [  16/  20] time: 187.3890, loss: 0.00009398\n",
      "Epoch: [34] [  17/  20] time: 187.6301, loss: 0.00085221\n",
      "Epoch: [34] [  18/  20] time: 187.8737, loss: 0.00685874\n",
      "Epoch: [34] [  19/  20] time: 188.1187, loss: 0.00208826\n",
      "[34/50] - ptime: 5.1654 loss: 0.00167651 acc: 0.60000 lr: 0.00072900\n",
      "Epoch: [35] [   0/  20] time: 189.0231, loss: 0.00000393\n",
      "Epoch: [35] [   1/  20] time: 189.2649, loss: 0.00002907\n",
      "Epoch: [35] [   2/  20] time: 189.5069, loss: 0.00111899\n",
      "Epoch: [35] [   3/  20] time: 189.7506, loss: 0.00028794\n",
      "Epoch: [35] [   4/  20] time: 189.9920, loss: 0.00187979\n",
      "Epoch: [35] [   5/  20] time: 190.2349, loss: 0.00000774\n",
      "Epoch: [35] [   6/  20] time: 190.4776, loss: 0.00194292\n",
      "Epoch: [35] [   7/  20] time: 190.7197, loss: 0.00000838\n",
      "Epoch: [35] [   8/  20] time: 190.9607, loss: 0.00011143\n",
      "Epoch: [35] [   9/  20] time: 191.2050, loss: 0.00003036\n",
      "Epoch: [35] [  10/  20] time: 191.4476, loss: 0.00002791\n",
      "Epoch: [35] [  11/  20] time: 191.6897, loss: 0.00072267\n",
      "Epoch: [35] [  12/  20] time: 191.9325, loss: 0.00001232\n",
      "Epoch: [35] [  13/  20] time: 192.1766, loss: 0.00618998\n",
      "Epoch: [35] [  14/  20] time: 192.4183, loss: 0.00101575\n",
      "Epoch: [35] [  15/  20] time: 192.6596, loss: 0.00139534\n",
      "Epoch: [35] [  16/  20] time: 192.9232, loss: 0.00052033\n",
      "Epoch: [35] [  17/  20] time: 193.1652, loss: 0.00022131\n",
      "Epoch: [35] [  18/  20] time: 193.4077, loss: 0.00940992\n",
      "Epoch: [35] [  19/  20] time: 193.6499, loss: 0.00001527\n",
      "[35/50] - ptime: 5.1876 loss: 0.00124757 acc: 0.58000 lr: 0.00072900\n",
      "Epoch: [36] [   0/  20] time: 194.5990, loss: 0.04701303\n",
      "Epoch: [36] [   1/  20] time: 194.8379, loss: 0.00133468\n",
      "Epoch: [36] [   2/  20] time: 195.0805, loss: 0.00105781\n",
      "Epoch: [36] [   3/  20] time: 195.3243, loss: 0.06770656\n",
      "Epoch: [36] [   4/  20] time: 195.5661, loss: 0.02582002\n",
      "Epoch: [36] [   5/  20] time: 195.8105, loss: 0.00501550\n",
      "Epoch: [36] [   6/  20] time: 196.0526, loss: 0.00332779\n",
      "Epoch: [36] [   7/  20] time: 196.2945, loss: 0.00000568\n",
      "Epoch: [36] [   8/  20] time: 196.5356, loss: 0.00000013\n",
      "Epoch: [36] [   9/  20] time: 196.7781, loss: 0.00013383\n",
      "Epoch: [36] [  10/  20] time: 197.0260, loss: 0.00244030\n",
      "Epoch: [36] [  11/  20] time: 197.2862, loss: 0.00088244\n",
      "Epoch: [36] [  12/  20] time: 197.5439, loss: 0.00404316\n",
      "Epoch: [36] [  13/  20] time: 197.7876, loss: 0.00555585\n",
      "Epoch: [36] [  14/  20] time: 198.0301, loss: 0.00011733\n",
      "Epoch: [36] [  15/  20] time: 198.2924, loss: 0.01237601\n",
      "Epoch: [36] [  16/  20] time: 198.5525, loss: 0.00007362\n",
      "Epoch: [36] [  17/  20] time: 198.8093, loss: 0.00192339\n",
      "Epoch: [36] [  18/  20] time: 199.0652, loss: 0.00172034\n",
      "Epoch: [36] [  19/  20] time: 199.3098, loss: 0.00672439\n",
      "[36/50] - ptime: 5.2741 loss: 0.00936359 acc: 0.57000 lr: 0.00072900\n",
      "Epoch: [37] [   0/  20] time: 200.2388, loss: 0.00069593\n",
      "Epoch: [37] [   1/  20] time: 200.4830, loss: 0.00038211\n",
      "Epoch: [37] [   2/  20] time: 200.7250, loss: 0.00004500\n",
      "Epoch: [37] [   3/  20] time: 200.9800, loss: 0.00001177\n",
      "Epoch: [37] [   4/  20] time: 201.2397, loss: 0.00000077\n",
      "Epoch: [37] [   5/  20] time: 201.4972, loss: 0.00020978\n",
      "Epoch: [37] [   6/  20] time: 201.7564, loss: 0.00359783\n",
      "Epoch: [37] [   7/  20] time: 202.0147, loss: 0.00000749\n",
      "Epoch: [37] [   8/  20] time: 202.2592, loss: 0.00012621\n",
      "Epoch: [37] [   9/  20] time: 202.5012, loss: 0.00000877\n",
      "Epoch: [37] [  10/  20] time: 202.7534, loss: 0.00524275\n",
      "Epoch: [37] [  11/  20] time: 203.0128, loss: 0.00781183\n",
      "Epoch: [37] [  12/  20] time: 203.2564, loss: 0.00629976\n",
      "Epoch: [37] [  13/  20] time: 203.4985, loss: 0.01217347\n",
      "Epoch: [37] [  14/  20] time: 203.7438, loss: 0.00077866\n",
      "Epoch: [37] [  15/  20] time: 203.9963, loss: 0.00449302\n",
      "Epoch: [37] [  16/  20] time: 204.2393, loss: 0.00016830\n",
      "Epoch: [37] [  17/  20] time: 204.4837, loss: 0.00132004\n",
      "Epoch: [37] [  18/  20] time: 204.7260, loss: 0.00450016\n",
      "Epoch: [37] [  19/  20] time: 204.9677, loss: 0.00359198\n",
      "[37/50] - ptime: 5.2913 loss: 0.00257328 acc: 0.58000 lr: 0.00072900\n",
      "Epoch: [38] [   0/  20] time: 205.8757, loss: 0.00009462\n",
      "Epoch: [38] [   1/  20] time: 206.1158, loss: 0.00005116\n",
      "Epoch: [38] [   2/  20] time: 206.3580, loss: 0.00425419\n",
      "Epoch: [38] [   3/  20] time: 206.6005, loss: 0.00232739\n",
      "Epoch: [38] [   4/  20] time: 206.8439, loss: 0.00449035\n",
      "Epoch: [38] [   5/  20] time: 207.0958, loss: 0.00012143\n",
      "Epoch: [38] [   6/  20] time: 207.3391, loss: 0.00023080\n",
      "Epoch: [38] [   7/  20] time: 207.5844, loss: 0.00312210\n",
      "Epoch: [38] [   8/  20] time: 207.8279, loss: 0.00029672\n",
      "Epoch: [38] [   9/  20] time: 208.0708, loss: 0.00000259\n",
      "Epoch: [38] [  10/  20] time: 208.3133, loss: 0.00020540\n",
      "Epoch: [38] [  11/  20] time: 208.5591, loss: 0.00476266\n",
      "Epoch: [38] [  12/  20] time: 208.8033, loss: 0.00000708\n",
      "Epoch: [38] [  13/  20] time: 209.0448, loss: 0.00015971\n",
      "Epoch: [38] [  14/  20] time: 209.2880, loss: 0.00006904\n",
      "Epoch: [38] [  15/  20] time: 209.5306, loss: 0.00037944\n",
      "Epoch: [38] [  16/  20] time: 209.7755, loss: 0.00008282\n",
      "Epoch: [38] [  17/  20] time: 210.0180, loss: 0.00014414\n",
      "Epoch: [38] [  18/  20] time: 210.2616, loss: 0.00269772\n",
      "Epoch: [38] [  19/  20] time: 210.5057, loss: 0.00000104\n",
      "[38/50] - ptime: 5.1906 loss: 0.00117502 acc: 0.57000 lr: 0.00072900\n",
      "Epoch: [39] [   0/  20] time: 211.4088, loss: 0.00012723\n",
      "Epoch: [39] [   1/  20] time: 211.6502, loss: 0.00002157\n",
      "Epoch: [39] [   2/  20] time: 211.8948, loss: 0.00000443\n",
      "Epoch: [39] [   3/  20] time: 212.1371, loss: 0.00008356\n",
      "Epoch: [39] [   4/  20] time: 212.3778, loss: 0.00003136\n",
      "Epoch: [39] [   5/  20] time: 212.6193, loss: 0.00208411\n",
      "Epoch: [39] [   6/  20] time: 212.8630, loss: 0.00066187\n",
      "Epoch: [39] [   7/  20] time: 213.1042, loss: 0.00005801\n",
      "Epoch: [39] [   8/  20] time: 213.3476, loss: 0.00005210\n",
      "Epoch: [39] [   9/  20] time: 213.5897, loss: 0.00024610\n",
      "Epoch: [39] [  10/  20] time: 213.8343, loss: 0.00425404\n",
      "Epoch: [39] [  11/  20] time: 214.0773, loss: 0.00000196\n",
      "Epoch: [39] [  12/  20] time: 214.3197, loss: 0.00093099\n",
      "Epoch: [39] [  13/  20] time: 214.5626, loss: 0.00001577\n",
      "Epoch: [39] [  14/  20] time: 214.8085, loss: 0.00001711\n",
      "Epoch: [39] [  15/  20] time: 215.0584, loss: 0.00005475\n",
      "Epoch: [39] [  16/  20] time: 215.3014, loss: 0.00001526\n",
      "Epoch: [39] [  17/  20] time: 215.5463, loss: 0.00000116\n",
      "Epoch: [39] [  18/  20] time: 215.7912, loss: 0.00002101\n",
      "Epoch: [39] [  19/  20] time: 216.0476, loss: 0.00084590\n",
      "[39/50] - ptime: 5.2025 loss: 0.00047641 acc: 0.57000 lr: 0.00072900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40] [   0/  20] time: 216.9644, loss: 0.00000253\n",
      "Epoch: [40] [   1/  20] time: 217.2086, loss: 0.00007616\n",
      "Epoch: [40] [   2/  20] time: 217.4506, loss: 0.00055342\n",
      "Epoch: [40] [   3/  20] time: 217.6928, loss: 0.00048442\n",
      "Epoch: [40] [   4/  20] time: 217.9351, loss: 0.00010700\n",
      "Epoch: [40] [   5/  20] time: 218.1778, loss: 0.00000299\n",
      "Epoch: [40] [   6/  20] time: 218.4222, loss: 0.00003661\n",
      "Epoch: [40] [   7/  20] time: 218.6642, loss: 0.00002212\n",
      "Epoch: [40] [   8/  20] time: 218.9082, loss: 0.00070025\n",
      "Epoch: [40] [   9/  20] time: 219.1546, loss: 0.00177304\n",
      "Epoch: [40] [  10/  20] time: 219.3967, loss: 0.00008652\n",
      "Epoch: [40] [  11/  20] time: 219.6402, loss: 0.00012571\n",
      "Epoch: [40] [  12/  20] time: 219.8833, loss: 0.00000245\n",
      "Epoch: [40] [  13/  20] time: 220.1261, loss: 0.00002671\n",
      "Epoch: [40] [  14/  20] time: 220.3687, loss: 0.00458602\n",
      "Epoch: [40] [  15/  20] time: 220.6119, loss: 0.00399687\n",
      "Epoch: [40] [  16/  20] time: 220.8567, loss: 0.00000008\n",
      "Epoch: [40] [  17/  20] time: 221.1000, loss: 0.00020521\n",
      "Epoch: [40] [  18/  20] time: 221.3413, loss: 0.00000198\n",
      "Epoch: [40] [  19/  20] time: 221.5830, loss: 0.00551821\n",
      "[40/50] - ptime: 5.1781 loss: 0.00091542 acc: 0.58000 lr: 0.00065610\n",
      "Epoch: [41] [   0/  20] time: 222.4971, loss: 0.00076669\n",
      "Epoch: [41] [   1/  20] time: 222.7460, loss: 0.00004296\n",
      "Epoch: [41] [   2/  20] time: 222.9904, loss: 0.00015063\n",
      "Epoch: [41] [   3/  20] time: 223.2335, loss: 0.00070283\n",
      "Epoch: [41] [   4/  20] time: 223.4755, loss: 0.00036180\n",
      "Epoch: [41] [   5/  20] time: 223.7174, loss: 0.00295638\n",
      "Epoch: [41] [   6/  20] time: 223.9592, loss: 0.00001119\n",
      "Epoch: [41] [   7/  20] time: 224.2029, loss: 0.00007825\n",
      "Epoch: [41] [   8/  20] time: 224.4458, loss: 0.00000240\n",
      "Epoch: [41] [   9/  20] time: 224.6972, loss: 0.00000081\n",
      "Epoch: [41] [  10/  20] time: 224.9390, loss: 0.00028460\n",
      "Epoch: [41] [  11/  20] time: 225.1814, loss: 0.00007400\n",
      "Epoch: [41] [  12/  20] time: 225.4236, loss: 0.00000039\n",
      "Epoch: [41] [  13/  20] time: 225.6659, loss: 0.00000025\n",
      "Epoch: [41] [  14/  20] time: 225.9093, loss: 0.00010812\n",
      "Epoch: [41] [  15/  20] time: 226.1514, loss: 0.00001356\n",
      "Epoch: [41] [  16/  20] time: 226.3942, loss: 0.00014990\n",
      "Epoch: [41] [  17/  20] time: 226.6378, loss: 0.00001504\n",
      "Epoch: [41] [  18/  20] time: 226.8798, loss: 0.00041216\n",
      "Epoch: [41] [  19/  20] time: 227.1215, loss: 0.00003190\n",
      "[41/50] - ptime: 5.1932 loss: 0.00030819 acc: 0.58000 lr: 0.00065610\n",
      "Epoch: [42] [   0/  20] time: 228.0267, loss: 0.00000198\n",
      "Epoch: [42] [   1/  20] time: 228.2678, loss: 0.00052425\n",
      "Epoch: [42] [   2/  20] time: 228.5097, loss: 0.00000366\n",
      "Epoch: [42] [   3/  20] time: 228.7541, loss: 0.00000585\n",
      "Epoch: [42] [   4/  20] time: 228.9958, loss: 0.00000017\n",
      "Epoch: [42] [   5/  20] time: 229.2423, loss: 0.00001859\n",
      "Epoch: [42] [   6/  20] time: 229.4843, loss: 0.00172768\n",
      "Epoch: [42] [   7/  20] time: 229.7268, loss: 0.00000395\n",
      "Epoch: [42] [   8/  20] time: 229.9686, loss: 0.00153597\n",
      "Epoch: [42] [   9/  20] time: 230.2111, loss: 0.00001087\n",
      "Epoch: [42] [  10/  20] time: 230.4531, loss: 0.00000260\n",
      "Epoch: [42] [  11/  20] time: 230.6958, loss: 0.00018655\n",
      "Epoch: [42] [  12/  20] time: 230.9384, loss: 0.00110904\n",
      "Epoch: [42] [  13/  20] time: 231.1800, loss: 0.00005589\n",
      "Epoch: [42] [  14/  20] time: 231.4219, loss: 0.00000871\n",
      "Epoch: [42] [  15/  20] time: 231.6641, loss: 0.00016880\n",
      "Epoch: [42] [  16/  20] time: 231.9084, loss: 0.00042687\n",
      "Epoch: [42] [  17/  20] time: 232.1518, loss: 0.00011182\n",
      "Epoch: [42] [  18/  20] time: 232.3935, loss: 0.00016133\n",
      "Epoch: [42] [  19/  20] time: 232.6350, loss: 0.00095755\n",
      "[42/50] - ptime: 5.1668 loss: 0.00035111 acc: 0.58000 lr: 0.00065610\n",
      "Epoch: [43] [   0/  20] time: 233.5465, loss: 0.00000054\n",
      "Epoch: [43] [   1/  20] time: 233.7883, loss: 0.00002546\n",
      "Epoch: [43] [   2/  20] time: 234.0327, loss: 0.00014926\n",
      "Epoch: [43] [   3/  20] time: 234.2764, loss: 0.00011936\n",
      "Epoch: [43] [   4/  20] time: 234.5181, loss: 0.00003479\n",
      "Epoch: [43] [   5/  20] time: 234.7613, loss: 0.00010187\n",
      "Epoch: [43] [   6/  20] time: 235.0033, loss: 0.00000038\n",
      "Epoch: [43] [   7/  20] time: 235.2454, loss: 0.00047798\n",
      "Epoch: [43] [   8/  20] time: 235.4867, loss: 0.00228573\n",
      "Epoch: [43] [   9/  20] time: 235.7283, loss: 0.00006189\n",
      "Epoch: [43] [  10/  20] time: 235.9697, loss: 0.00000893\n",
      "Epoch: [43] [  11/  20] time: 236.2122, loss: 0.00176479\n",
      "Epoch: [43] [  12/  20] time: 236.4543, loss: 0.00000113\n",
      "Epoch: [43] [  13/  20] time: 236.6959, loss: 0.00000377\n",
      "Epoch: [43] [  14/  20] time: 236.9410, loss: 0.00001243\n",
      "Epoch: [43] [  15/  20] time: 237.1854, loss: 0.00004726\n",
      "Epoch: [43] [  16/  20] time: 237.4269, loss: 0.00013345\n",
      "Epoch: [43] [  17/  20] time: 237.6687, loss: 0.00057423\n",
      "Epoch: [43] [  18/  20] time: 237.9113, loss: 0.00001058\n",
      "Epoch: [43] [  19/  20] time: 238.1494, loss: 0.00001011\n",
      "[43/50] - ptime: 5.1614 loss: 0.00029120 acc: 0.57000 lr: 0.00065610\n",
      "Epoch: [44] [   0/  20] time: 239.0401, loss: 0.00036844\n",
      "Epoch: [44] [   1/  20] time: 239.2811, loss: 0.00097801\n",
      "Epoch: [44] [   2/  20] time: 239.5238, loss: 0.00147285\n",
      "Epoch: [44] [   3/  20] time: 239.7675, loss: 0.00000153\n",
      "Epoch: [44] [   4/  20] time: 240.0105, loss: 0.00000717\n",
      "Epoch: [44] [   5/  20] time: 240.2564, loss: 0.00001494\n",
      "Epoch: [44] [   6/  20] time: 240.4980, loss: 0.00172130\n",
      "Epoch: [44] [   7/  20] time: 240.7404, loss: 0.00000602\n",
      "Epoch: [44] [   8/  20] time: 240.9809, loss: 0.00009585\n",
      "Epoch: [44] [   9/  20] time: 241.2241, loss: 0.00031395\n",
      "Epoch: [44] [  10/  20] time: 241.4651, loss: 0.00002111\n",
      "Epoch: [44] [  11/  20] time: 241.7068, loss: 0.00004903\n",
      "Epoch: [44] [  12/  20] time: 241.9498, loss: 0.00002840\n",
      "Epoch: [44] [  13/  20] time: 242.1929, loss: 0.00000315\n",
      "Epoch: [44] [  14/  20] time: 242.4339, loss: 0.00002413\n",
      "Epoch: [44] [  15/  20] time: 242.6772, loss: 0.00006213\n",
      "Epoch: [44] [  16/  20] time: 242.9194, loss: 0.00000001\n",
      "Epoch: [44] [  17/  20] time: 243.1626, loss: 0.00069691\n",
      "Epoch: [44] [  18/  20] time: 243.4058, loss: 0.00002292\n",
      "Epoch: [44] [  19/  20] time: 243.6467, loss: 0.00000446\n",
      "[44/50] - ptime: 5.1635 loss: 0.00029462 acc: 0.57000 lr: 0.00065610\n",
      "Epoch: [45] [   0/  20] time: 244.5729, loss: 0.00009553\n",
      "Epoch: [45] [   1/  20] time: 244.8144, loss: 0.00013046\n",
      "Epoch: [45] [   2/  20] time: 245.0558, loss: 0.00055097\n",
      "Epoch: [45] [   3/  20] time: 245.2988, loss: 0.00001324\n",
      "Epoch: [45] [   4/  20] time: 245.5413, loss: 0.00002913\n",
      "Epoch: [45] [   5/  20] time: 245.7828, loss: 0.00000020\n",
      "Epoch: [45] [   6/  20] time: 246.0253, loss: 0.00149683\n",
      "Epoch: [45] [   7/  20] time: 246.2691, loss: 0.00020432\n",
      "Epoch: [45] [   8/  20] time: 246.5115, loss: 0.00003783\n",
      "Epoch: [45] [   9/  20] time: 246.7568, loss: 0.00001095\n",
      "Epoch: [45] [  10/  20] time: 246.9999, loss: 0.00000029\n",
      "Epoch: [45] [  11/  20] time: 247.2413, loss: 0.00021327\n",
      "Epoch: [45] [  12/  20] time: 247.4822, loss: 0.00037990\n",
      "Epoch: [45] [  13/  20] time: 247.7254, loss: 0.00000793\n",
      "Epoch: [45] [  14/  20] time: 247.9672, loss: 0.00002376\n",
      "Epoch: [45] [  15/  20] time: 248.2097, loss: 0.00002236\n",
      "Epoch: [45] [  16/  20] time: 248.4512, loss: 0.00003527\n",
      "Epoch: [45] [  17/  20] time: 248.6935, loss: 0.00000398\n",
      "Epoch: [45] [  18/  20] time: 248.9363, loss: 0.00014240\n",
      "Epoch: [45] [  19/  20] time: 249.1794, loss: 0.00000046\n",
      "[45/50] - ptime: 5.1650 loss: 0.00016995 acc: 0.55000 lr: 0.00065610\n",
      "Epoch: [46] [   0/  20] time: 250.0849, loss: 0.00001551\n",
      "Epoch: [46] [   1/  20] time: 250.3261, loss: 0.00002769\n",
      "Epoch: [46] [   2/  20] time: 250.5682, loss: 0.00009632\n",
      "Epoch: [46] [   3/  20] time: 250.8108, loss: 0.00000553\n",
      "Epoch: [46] [   4/  20] time: 251.0514, loss: 0.00014532\n",
      "Epoch: [46] [   5/  20] time: 251.2949, loss: 0.00035022\n",
      "Epoch: [46] [   6/  20] time: 251.5368, loss: 0.00000008\n",
      "Epoch: [46] [   7/  20] time: 251.7801, loss: 0.00007246\n",
      "Epoch: [46] [   8/  20] time: 252.0222, loss: 0.00000181\n",
      "Epoch: [46] [   9/  20] time: 252.2650, loss: 0.00043904\n",
      "Epoch: [46] [  10/  20] time: 252.5083, loss: 0.00005159\n",
      "Epoch: [46] [  11/  20] time: 252.7514, loss: 0.00000364\n",
      "Epoch: [46] [  12/  20] time: 252.9925, loss: 0.00000161\n",
      "Epoch: [46] [  13/  20] time: 253.2335, loss: 0.00142192\n",
      "Epoch: [46] [  14/  20] time: 253.4748, loss: 0.00002519\n",
      "Epoch: [46] [  15/  20] time: 253.7160, loss: 0.00034904\n",
      "Epoch: [46] [  16/  20] time: 253.9577, loss: 0.00000390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46] [  17/  20] time: 254.1982, loss: 0.00009304\n",
      "Epoch: [46] [  18/  20] time: 254.4393, loss: 0.00000051\n",
      "Epoch: [46] [  19/  20] time: 254.6806, loss: 0.00006424\n",
      "[46/50] - ptime: 5.1627 loss: 0.00015843 acc: 0.55000 lr: 0.00065610\n",
      "Epoch: [47] [   0/  20] time: 255.5739, loss: 0.00000152\n",
      "Epoch: [47] [   1/  20] time: 255.8159, loss: 0.00003849\n",
      "Epoch: [47] [   2/  20] time: 256.0586, loss: 0.00031726\n",
      "Epoch: [47] [   3/  20] time: 256.3058, loss: 0.00003464\n",
      "Epoch: [47] [   4/  20] time: 256.5476, loss: 0.00074978\n",
      "Epoch: [47] [   5/  20] time: 256.7903, loss: 0.00004019\n",
      "Epoch: [47] [   6/  20] time: 257.0327, loss: 0.00000534\n",
      "Epoch: [47] [   7/  20] time: 257.2775, loss: 0.00000691\n",
      "Epoch: [47] [   8/  20] time: 257.5196, loss: 0.00000030\n",
      "Epoch: [47] [   9/  20] time: 257.7656, loss: 0.00000194\n",
      "Epoch: [47] [  10/  20] time: 258.0077, loss: 0.00001302\n",
      "Epoch: [47] [  11/  20] time: 258.2506, loss: 0.00000024\n",
      "Epoch: [47] [  12/  20] time: 258.4919, loss: 0.00019083\n",
      "Epoch: [47] [  13/  20] time: 258.7334, loss: 0.00004372\n",
      "Epoch: [47] [  14/  20] time: 258.9756, loss: 0.00001062\n",
      "Epoch: [47] [  15/  20] time: 259.2182, loss: 0.00022142\n",
      "Epoch: [47] [  16/  20] time: 259.4600, loss: 0.00000717\n",
      "Epoch: [47] [  17/  20] time: 259.7026, loss: 0.00000014\n",
      "Epoch: [47] [  18/  20] time: 259.9455, loss: 0.00004743\n",
      "Epoch: [47] [  19/  20] time: 260.1875, loss: 0.00008801\n",
      "[47/50] - ptime: 5.1719 loss: 0.00009095 acc: 0.55000 lr: 0.00065610\n",
      "Epoch: [48] [   0/  20] time: 261.1853, loss: 0.00000012\n",
      "Epoch: [48] [   1/  20] time: 261.4262, loss: 0.00001234\n",
      "Epoch: [48] [   2/  20] time: 261.6678, loss: 0.00001965\n",
      "Epoch: [48] [   3/  20] time: 261.9109, loss: 0.00002748\n",
      "Epoch: [48] [   4/  20] time: 262.1527, loss: 0.00003993\n",
      "Epoch: [48] [   5/  20] time: 262.3960, loss: 0.00000681\n",
      "Epoch: [48] [   6/  20] time: 262.6387, loss: 0.00000088\n",
      "Epoch: [48] [   7/  20] time: 262.8818, loss: 0.00033852\n",
      "Epoch: [48] [   8/  20] time: 263.1236, loss: 0.00005349\n",
      "Epoch: [48] [   9/  20] time: 263.3670, loss: 0.00001794\n",
      "Epoch: [48] [  10/  20] time: 263.6086, loss: 0.00000918\n",
      "Epoch: [48] [  11/  20] time: 263.8502, loss: 0.00085645\n",
      "Epoch: [48] [  12/  20] time: 264.0922, loss: 0.00000004\n",
      "Epoch: [48] [  13/  20] time: 264.3384, loss: 0.00001621\n",
      "Epoch: [48] [  14/  20] time: 264.5809, loss: 0.00000075\n",
      "Epoch: [48] [  15/  20] time: 264.8241, loss: 0.00006400\n",
      "Epoch: [48] [  16/  20] time: 265.0665, loss: 0.00002640\n",
      "Epoch: [48] [  17/  20] time: 265.3098, loss: 0.00001453\n",
      "Epoch: [48] [  18/  20] time: 265.5509, loss: 0.00006278\n",
      "Epoch: [48] [  19/  20] time: 265.7928, loss: 0.00019740\n",
      "[48/50] - ptime: 5.1663 loss: 0.00008825 acc: 0.55000 lr: 0.00065610\n",
      "Epoch: [49] [   0/  20] time: 266.6933, loss: 0.00000038\n",
      "Epoch: [49] [   1/  20] time: 266.9345, loss: 0.00008850\n",
      "Epoch: [49] [   2/  20] time: 267.1764, loss: 0.00000431\n",
      "Epoch: [49] [   3/  20] time: 267.4192, loss: 0.00000030\n",
      "Epoch: [49] [   4/  20] time: 267.6608, loss: 0.00000042\n",
      "Epoch: [49] [   5/  20] time: 267.9045, loss: 0.00001426\n",
      "Epoch: [49] [   6/  20] time: 268.1462, loss: 0.00000003\n",
      "Epoch: [49] [   7/  20] time: 268.3888, loss: 0.00035329\n",
      "Epoch: [49] [   8/  20] time: 268.6319, loss: 0.00017916\n",
      "Epoch: [49] [   9/  20] time: 268.8741, loss: 0.00050165\n",
      "Epoch: [49] [  10/  20] time: 269.1168, loss: 0.00000360\n",
      "Epoch: [49] [  11/  20] time: 269.3641, loss: 0.00000078\n",
      "Epoch: [49] [  12/  20] time: 269.6061, loss: 0.00003522\n",
      "Epoch: [49] [  13/  20] time: 269.8482, loss: 0.00000209\n",
      "Epoch: [49] [  14/  20] time: 270.0913, loss: 0.00003413\n",
      "Epoch: [49] [  15/  20] time: 270.3353, loss: 0.00003445\n",
      "Epoch: [49] [  16/  20] time: 270.5779, loss: 0.00000013\n",
      "Epoch: [49] [  17/  20] time: 270.8235, loss: 0.00000031\n",
      "Epoch: [49] [  18/  20] time: 271.0612, loss: 0.00000450\n",
      "Epoch: [49] [  19/  20] time: 271.3050, loss: 0.00000169\n",
      "[49/50] - ptime: 5.1708 loss: 0.00006296 acc: 0.56000 lr: 0.00065610\n",
      "Epoch: [50] [   0/  20] time: 272.2322, loss: 0.00006341\n",
      "Epoch: [50] [   1/  20] time: 272.4719, loss: 0.00000026\n",
      "Epoch: [50] [   2/  20] time: 272.7129, loss: 0.00000010\n",
      "Epoch: [50] [   3/  20] time: 272.9536, loss: 0.00003705\n",
      "Epoch: [50] [   4/  20] time: 273.1950, loss: 0.00000353\n",
      "Epoch: [50] [   5/  20] time: 273.4371, loss: 0.00007377\n",
      "Epoch: [50] [   6/  20] time: 273.6800, loss: 0.00001652\n",
      "Epoch: [50] [   7/  20] time: 273.9229, loss: 0.00000009\n",
      "Epoch: [50] [   8/  20] time: 274.1662, loss: 0.00010814\n",
      "Epoch: [50] [   9/  20] time: 274.4090, loss: 0.00008167\n",
      "Epoch: [50] [  10/  20] time: 274.6516, loss: 0.00000192\n",
      "Epoch: [50] [  11/  20] time: 274.8954, loss: 0.00000105\n",
      "Epoch: [50] [  12/  20] time: 275.1369, loss: 0.00184547\n",
      "Epoch: [50] [  13/  20] time: 275.3805, loss: 0.00001740\n",
      "Epoch: [50] [  14/  20] time: 275.6223, loss: 0.00297279\n",
      "Epoch: [50] [  15/  20] time: 275.8645, loss: 0.00000026\n",
      "Epoch: [50] [  16/  20] time: 276.1069, loss: 0.00000340\n",
      "Epoch: [50] [  17/  20] time: 276.3514, loss: 0.00036228\n",
      "Epoch: [50] [  18/  20] time: 276.5948, loss: 0.00003492\n",
      "Epoch: [50] [  19/  20] time: 276.8378, loss: 0.00002067\n",
      "[50/50] - ptime: 5.1735 loss: 0.00028223 acc: 0.59000 lr: 0.00059049\n",
      "Avg per epoch ptime: 5.19, total 50 epochs ptime: 277.23\n",
      " [*] Training finished!\n",
      " [*] Best Epoch:  4 , Accuracy:  0.800000011920929\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/CNN_GLCM_C5_D1_Kernel(3,3)_128_lrdecay/CNN_GLCM_C5_D1_Kernel(3,3)_128_lrdecay-4\n",
      " [*] Finished testing Best Epoch: 4 , accuracy:  0.7999999523162842 !\n"
     ]
    }
   ],
   "source": [
    "dataset = '4_Flowers_1s'\n",
    "epoch = 50\n",
    "batch_size = 100\n",
    "checkpoint_dir = 'checkpoint'\n",
    "log_dir = 'logs'\n",
    "trainhist_dir = 'trainhist'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "# --log_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# --trainhist_dir\n",
    "if not os.path.exists(trainhist_dir):\n",
    "    os.makedirs(trainhist_dir)\n",
    "\n",
    "# open session\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    \n",
    "    # declare instance for GAN\n",
    "    CNN = CNN(sess, epoch=epoch, batch_size=batch_size, dataset_name=dataset, checkpoint_dir=checkpoint_dir, \n",
    "                log_dir=log_dir, trainhist_dir=trainhist_dir)\n",
    "\n",
    "    # build graph\n",
    "    CNN.build_model()\n",
    "\n",
    "    # show network architecture\n",
    "    CNN.show_all_variables()\n",
    "\n",
    "    # launch the graph in a session\n",
    "    CNN.train()\n",
    "    \n",
    "#     CNN.test(epoch)\n",
    "        \n",
    "sess.close()\n",
    "        \n",
    "# lrdecay\n",
    "# [*] Best Epoch:  6 , Accuracy:  0.8399999737739563\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_GLCM_C5_D1_Kernel(3,3)_128_lrdecay/CNN_GLCM_C5_D1_Kernel(3,3)_128_lrdecay-6\n",
    "#  [*] Finished testing Best Epoch: 6 , accuracy:  0.8400000333786011 !\n",
    "\n",
    "#  [*] Training finished!\n",
    "#  [*] Best Epoch:  4 , Accuracy:  0.800000011920929\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_GLCM_C5_D1_Kernel(3,3)_128_lrdecay/CNN_GLCM_C5_D1_Kernel(3,3)_128_lrdecay-4\n",
    "#  [*] Finished testing Best Epoch: 4 , accuracy:  0.7999999523162842 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "file='/home/huiqy/Music/CloudMusic/All_Time_Low.mp3' #文件名是完整路径名\n",
    "pygame.mixer.init() #初始化音频\n",
    "track = pygame.mixer.music.load(file)#载入音乐文件\n",
    "pygame.mixer.music.play()#开始播放\n",
    "time.sleep(60)#播放10秒\n",
    "pygame.mixer.music.stop()#停止播放"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
