{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imutils import paths\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import argparse\n",
    "import imutils,sklearn\n",
    "import os, cv2, re, random, shutil, imageio, pickle\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pyramid\n",
    "# def batch_pyramid(images):\n",
    "#     pyramid_list = []\n",
    "#     for i in tqdm(range(len(images))):\n",
    "#         img = (images.astype(np.float32)* 255)[i, :, :, :].astype(np.uint8)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         G = img.copy()\n",
    "#         gp = [G]\n",
    "#         for i in range(4):\n",
    "#             G = cv2.pyrDown(G)\n",
    "#             gp.append(G)\n",
    "#         lp = [gp[4]]\n",
    "#         for i in range(4,0,-1):\n",
    "#             GE = cv2.pyrUp(gp[i])\n",
    "#             (x,y) = gp[i-1].shape\n",
    "#             L = cv2.subtract(gp[i-1],GE[:x,:y])\n",
    "#             lp.append(L)\n",
    "#         lp_1 = cv2.normalize(lp[1], lp[1],0,255,cv2.NORM_MINMAX)\n",
    "#         lp_2 = cv2.normalize(lp[2], lp[2],0,255,cv2.NORM_MINMAX)\n",
    "#         lp_out = [cv2.resize(lp_1, (128,128)),cv2.resize(lp_2, (128,128))]\n",
    "#         pyramid_list.append((np.array(lp_out)).astype(np.uint8))\n",
    "#     pyramid_list=np.array(pyramid_list,dtype = float32).reshape([images.shape[0],images.shape[1],images.shape[2],2])\n",
    "    \n",
    "#     print (pyramid_list.shape)\n",
    "#     return pyramid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def atoi(text):\n",
    "#     return int(text) if text.isdigit() else text\n",
    "\n",
    "# def natural_keys(text):\n",
    "#     return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# def load_flower_data():\n",
    "#     # grab the list of images that we'll be describing\n",
    "#     print(\"[INFO] handling images...\")\n",
    "#     TRAIN_ORIGINAL_DIR = '../train/'\n",
    "#     TRAIN_SUB_DIR = '../subsample/'\n",
    "#     TRAIN_GAN = '../../image_gan/'\n",
    "#     TEST_DIR = '../../test/'\n",
    "\n",
    "#     # use this for full dataset\n",
    "#     train_images_gan = [TRAIN_GAN + i for i in os.listdir(TRAIN_GAN)]\n",
    "#     test_images = [TEST_DIR + i for i in os.listdir(TEST_DIR)]\n",
    "    \n",
    "#     train_images = train_images_gan\n",
    "    \n",
    "#     train_images.sort(key=natural_keys)\n",
    "#     test_images.sort(key=natural_keys)\n",
    "\n",
    "#     # initialize the features matrix and labels list\n",
    "#     trainImage = []\n",
    "#     trainLabels = []\n",
    "#     testImage = []\n",
    "#     testLabels = []\n",
    "\n",
    "#     # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(train_images):\n",
    "#         # extract the class label\n",
    "#         # get the labels from the name of the images by extract the string before \"_\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # read and resize image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         trainImage.append(img)\n",
    "#         trainLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(train_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(train_images)))\n",
    "            \n",
    "#       # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(test_images):\n",
    "#         # extract the class label\n",
    "#         # our images were named as labels.image_number.format\n",
    "#         # get the labels from the name of the images by extract the string before \".\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # extract CNN features in the image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         testImage.append(img)\n",
    "#         testLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(test_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(test_images)))\n",
    "\n",
    "\n",
    "#     trainImage = np.array(trainImage,dtype = float32)\n",
    "#     trainLabels = np.array(trainLabels)\n",
    "#     testImage = np.array(testImage,dtype = float32)\n",
    "#     testLabels = np.array(testLabels)\n",
    "#     print (trainImage.shape)\n",
    "    \n",
    "#     trainImage = trainImage.astype(np.float32) / 255\n",
    "#     testImage = testImage.astype(np.float32) / 255\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(trainLabels)\n",
    "#     list(le.classes_)\n",
    "#     trainLabels = le.transform(trainLabels) \n",
    "#     testLabels = le.transform(testLabels) \n",
    "    \n",
    "#     return trainImage, trainLabels, testImage, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainImage, trainLabels, testImage, testLabels = load_flower_data()\n",
    "\n",
    "# trainImage_pyramid = batch_pyramid(trainImage)\n",
    "# testImage_pyramid = batch_pyramid(testImage)\n",
    "# nb_classes = 2\n",
    "\n",
    "# # Convert class vectors to binary class matrices.\n",
    "# trainLabels = keras.utils.to_categorical(trainLabels, nb_classes)\n",
    "# print (trainLabels)\n",
    "# testLabels = keras.utils.to_categorical(testLabels, nb_classes)\n",
    "# print (testLabels)\n",
    "# print (testLabels.shape)\n",
    "\n",
    "# np.save('../trainImage.npy', trainImage)\n",
    "# np.save('../trainLabels.npy', trainLabels)\n",
    "# np.save('../testImage.npy', testImage)\n",
    "# np.save('../testLabels.npy', testLabels)\n",
    "# np.save('../trainImage_pyramid_L.npy', trainImage_pyramid)\n",
    "# np.save('../testImage_pyramid_L.npy', testImage_pyramid)\n",
    "\n",
    "# print(\"[INFO] trainImage matrix: {:.2f}MB\".format(\n",
    "#     (trainImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainLabels matrix: {:.4f}MB\".format(\n",
    "#     (trainLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage matrix: {:.2f}MB\".format(\n",
    "#     (testImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testLabels matrix: {:.4f}MB\".format(\n",
    "#     (testLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainImage_pyramid matrix: {:.2f}MB\".format(\n",
    "#     (trainImage_pyramid.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage_pyramid matrix: {:.4f}MB\".format(\n",
    "#     (testImage_pyramid.nbytes) / (1024 * 1000.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self, sess, epoch, batch_size, dataset_name, checkpoint_dir, log_dir, trainhist_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.trainhist_dir = trainhist_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.classname = ['Iris', 'Pansy']\n",
    "\n",
    "        # parameters\n",
    "        self.input_height = 128\n",
    "        self.input_width = 128\n",
    "        self.c_dim = 3  # color dimension\n",
    "        self.pyramid_dim = 2  # color dimension\n",
    "        self.nb_class = 2\n",
    "        \n",
    "        # number of convolutional filters to use  \n",
    "        self.nb_CNN = [32, 64, 64, 64, 128]  \n",
    "        # number of dense filters to use  \n",
    "        self.nb_Dense = [256] \n",
    "        # size of pooling area for max pooling  \n",
    "        self.pool_size = (2, 2)  \n",
    "        # convolution kernel size  \n",
    "        self.kernel_size = (3, 3)\n",
    "        self.batch_normalization_control = True\n",
    "        \n",
    "        # name for checkpoint\n",
    "        self.model_name = 'CNN_Pyramid_L_C%d_D%d_Kernel(%d,%d)_%d_lrdecay' % (len(self.nb_CNN), len(self.nb_Dense),\n",
    "                                                          self.kernel_size[0], self.kernel_size[1], max(self.nb_CNN))\n",
    "\n",
    "        # train\n",
    "        #设置一个全局的计数器\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(0.001, \n",
    "                                             global_step=self.global_step, \n",
    "                                             decay_steps=10, \n",
    "                                             decay_rate=0.9, \n",
    "                                             staircase=True)\n",
    "        self.beta1 = 0.5\n",
    "        #max model to keep saving\n",
    "        self.max_to_keep = 300\n",
    "        \n",
    "        # test\n",
    "\n",
    "        #load_flower_data\n",
    "        self.train_x = np.load('../trainImage.npy')\n",
    "        self.train_y = np.load('../trainLabels.npy')\n",
    "        self.test_x = np.load('../testImage.npy')\n",
    "        self.test_y = np.load('../testLabels.npy')\n",
    "        self.train_x_pyramid = np.load('../trainImage_pyramid_L.npy')\n",
    "        self.test_x_pyramid = np.load('../testImage_pyramid_L.npy')\n",
    "        \n",
    "        #记录\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['losses'] = []\n",
    "        self.train_hist['accuracy'] = []\n",
    "        self.train_hist['learning_rate'] = []\n",
    "        self.train_hist['per_epoch_ptimes'] = []\n",
    "        self.train_hist['total_ptime'] = []\n",
    "        \n",
    "        # get number of batches for a single epoch\n",
    "        self.num_batches_train = len(self.train_x) // self.batch_size\n",
    "        self.num_batches_test= len(self.test_x) // self.batch_size\n",
    "\n",
    "    def cnn_model(self, x, x_pyramid, keep_prob, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(\"cnn\", reuse=reuse):\n",
    "             \n",
    "            #初始化参数\n",
    "            W = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "            B = tf.constant_initializer(0.0)\n",
    "        \n",
    "            print(\"CNN:x\",x.get_shape()) # 128, 128, 3 \n",
    "            print(\"CNN:x_pyramid\",x_pyramid.get_shape()) # 128, 128, 3 \n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_1 = tf.layers.conv2d(inputs = x,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_1'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_1.get_shape())\n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_2 = tf.layers.conv2d(inputs = x_pyramid,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_2'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_2.get_shape())\n",
    "\n",
    "            #把数据和边缘进行连接\n",
    "            net = tf.concat([net1_1, net1_2], 3)\n",
    "            net = tf.layers.batch_normalization(net, training=is_training)\n",
    "            net = tf.nn.relu(net, name = 'relu_conv_1')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_1'\n",
    "                                             )\n",
    "            \n",
    "            for i in range(2,len(self.nb_CNN)+1):\n",
    "                net = tf.layers.conv2d(inputs = net,                 # 输入,\n",
    "                                       filters = self.nb_CNN[i-1],      # 卷积核个数,\n",
    "                                       kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                       strides = (1, 1),\n",
    "                                       padding = 'same',              # padding方法\n",
    "                                       kernel_initializer = W,\n",
    "                                       bias_initializer = B,\n",
    "                                       kernel_regularizer = None,\n",
    "                                       bias_regularizer = None,\n",
    "                                       activity_regularizer = None,\n",
    "                                       name = 'conv_'+ str(i)        # 命名用于获取变量\n",
    "                                       )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "                if self.batch_normalization_control:\n",
    "                    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_conv_' + str(i))\n",
    "                net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_' + str(i)\n",
    "                                             )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #flatten\n",
    "            net = tf.reshape(net, [-1, int(net.get_shape()[1]*net.get_shape()[2]*net.get_shape()[3])],name='flatten')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #dense layer\n",
    "            for i in range(1,len(self.nb_Dense)+1):\n",
    "                net = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_Dense[i-1],\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_' + str(i)\n",
    "                                    )\n",
    "#                 net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_dense_' + str(i))\n",
    "                net = tf.layers.dropout(inputs = net,\n",
    "                                        rate=keep_prob,\n",
    "                                        noise_shape=None,\n",
    "                                        seed=None,\n",
    "                                        training = is_training,\n",
    "                                        name= 'dropout_dense_' + str(i)\n",
    "                                        )\n",
    "            #output\n",
    "            logit = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_class,\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_output'\n",
    "                                    )\n",
    "            out_logit = tf.nn.softmax(logit, name=\"softmax\")\n",
    "            print(\"CNN:out_logit\",out_logit.get_shape())\n",
    "            print(\"------------------------\")    \n",
    "\n",
    "            return out_logit, logit\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.x = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.c_dim], \n",
    "                                name='x_image')\n",
    "        \n",
    "        self.x_pyramid = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.pyramid_dim], \n",
    "                                name='x_pyramid')\n",
    "\n",
    "        self.y = tf.placeholder(tf.float32, shape=[self.batch_size, self.nb_class], name='y_label')\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.add_global = self.global_step.assign_add(1)\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "\n",
    "        # output of cnn_model\n",
    "        self.out_logit, self.logit = self.cnn_model(self.x, self.x_pyramid, self.keep_prob, is_training=True, reuse=False)\n",
    "        \n",
    "        self.loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y,\n",
    "                                                                                         logits =self.logit))\n",
    "        \n",
    "        \"\"\" Training \"\"\"\n",
    "        # trainable variables into a group\n",
    "        tf_vars = tf.trainable_variables()\n",
    "        cnn_vars = [var for var in tf_vars if var.name.startswith('cnn')]\n",
    "\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.cnn_optim = tf.train.AdamOptimizer(self.lr, beta1=self.beta1).minimize(self.loss_cross_entropy,\n",
    "                                                                                        var_list=cnn_vars)\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        # output of cnn_model\n",
    "        self.out_logit_test, self.logit_test = self.cnn_model(self.x, self.x_pyramid, self.keep_prob, is_training=False, reuse=True)\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit_test, 1), tf.argmax(self.y, 1))\n",
    "        self.predict = tf.argmax(self.logit_test, 1)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        self.loss_sum = tf.summary.scalar(\"loss\", self.loss_cross_entropy)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver(max_to_keep = self.max_to_keep)\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_epoch = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_epoch) + 1\n",
    "            counter = 1\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name+'.pkl', 'rb') \n",
    "            self.train_hist = pickle.load(f)\n",
    "            f.close()\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "            print(\" [!] START_EPOCH is \", start_epoch)\n",
    "        else:\n",
    "            start_epoch = 1\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        for epoch_loop in range(start_epoch, self.epoch + 1):\n",
    "\n",
    "            CNN_losses = []\n",
    "  \n",
    "            epoch_start_time = time.time()\n",
    "            shuffle_idxs = random.sample(range(0, self.train_x.shape[0]), self.train_x.shape[0])\n",
    "            shuffled_set = self.train_x[shuffle_idxs]\n",
    "            shuffled_set_pyramid = self.train_x_pyramid[shuffle_idxs]\n",
    "            shuffled_label = self.train_y[shuffle_idxs]\n",
    "    \n",
    "            # get batch data\n",
    "            for idx in range(self.num_batches_train):\n",
    "                batch_x = shuffled_set[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_x_pyramid= shuffled_set_pyramid[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_y = shuffled_label[idx*self.batch_size:(idx+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                \n",
    "\n",
    "                # update D network\n",
    "                _, summary_str, cnn_loss = self.sess.run([self.cnn_optim, self.loss_sum, self.loss_cross_entropy],\n",
    "                                               feed_dict={self.x: batch_x,\n",
    "                                                          self.x_pyramid: batch_x_pyramid,\n",
    "                                                          self.y: batch_y,\n",
    "                                                          self.keep_prob: 0.5}\n",
    "                                                      )\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                CNN_losses.append(cnn_loss)\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f\" % (epoch_loop, idx, self.num_batches_train, \n",
    "                                                                          time.time() - start_time, cnn_loss))\n",
    "\n",
    "            # After an epoch\n",
    "            # Evaluates accuracy on test set\n",
    "            test_accuracy_list = []\n",
    "            for idx_test in range(self.num_batches_test):\n",
    "                batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "                batch_x_pyramid_test =self.test_x_pyramid[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "                batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                    self.x_pyramid: batch_x_pyramid_test,\n",
    "                                                                    self.y: batch_y_tes,\n",
    "                                                                    self.keep_prob: 1.0})\n",
    "                test_accuracy_list.append(accuracy)\n",
    "            test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        \n",
    "            #update learning rate\n",
    "            _, rate = sess.run([self.add_global, self.lr])\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            print('[%d/%d] - ptime: %.4f loss: %.8f acc: %.5f lr: %.8f'% (epoch_loop, self.epoch, per_epoch_ptime, \n",
    "                                                                    np.mean(CNN_losses), test_accuracy, rate))\n",
    "            \n",
    "            self.train_hist['losses'].append(np.mean(CNN_losses))\n",
    "            self.train_hist['accuracy'].append( test_accuracy)\n",
    "            self.train_hist['learning_rate'].append(rate)\n",
    "            self.train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "            \n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, epoch_loop)\n",
    "            \n",
    "            # save trainhist for train\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name + '.pkl', 'wb') \n",
    "            pickle.dump(self.train_hist, f)\n",
    "            f.close()\n",
    "            self.show_train_hist(self.train_hist, save=True, path= self.trainhist_dir + '/' \n",
    "                                 + self.model_name + '.png')\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_ptime = end_time - start_time\n",
    "        self.train_hist['total_ptime'].append(total_ptime)\n",
    "        print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(self.train_hist['per_epoch_ptimes']), \n",
    "                                                                          self.epoch, total_ptime))\n",
    "        print(\" [*] Training finished!\")\n",
    "        \n",
    "        \"\"\"test after train\"\"\"\n",
    "        best_acc = max(self.train_hist['accuracy'])\n",
    "        beat_epoch = self.train_hist['accuracy'].index(best_acc) + 1\n",
    "        print (\" [*] Best Epoch: \", beat_epoch, \", Accuracy: \", best_acc)\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(beat_epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_pyramid_test =self.test_x_pyramid[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_pyramid: batch_x_pyramid_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Best Epoch:\", beat_epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "\n",
    "    def test(self, epoch):\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_pyramidtest =self.test_x_pyramid[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_pyramid: batch_x_pyramid_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Epoch:\", epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "        \n",
    "    def show_all_variables(self):\n",
    "        model_vars = tf.trainable_variables()\n",
    "        tf.contrib.slim.model_analyzer.analyze_vars(model_vars, print_info=True) \n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            epoch = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read [{}], epoch [{}]\".format(ckpt_name,epoch))\n",
    "            return True, epoch\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "        \n",
    "    def show_train_hist(self, hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "        x = range(1, len(hist['losses'])+1)\n",
    "\n",
    "        y1 = hist['losses']\n",
    "        y2 = hist['accuracy']\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "                            \n",
    "        ax2 = ax1.twinx()  \n",
    "\n",
    "        ax1.plot(x, y1, 'b')\n",
    "        ax2.plot(x, y2, 'r')\n",
    "                            \n",
    "        ax1.set_xlabel('Epoch')\n",
    "                            \n",
    "        ax1.set_ylabel('CNN_loss')    \n",
    "        ax2.set_ylabel('accuracy')\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(path, dpi = 400)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_pyramid (100, 128, 128, 2)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_pyramid (100, 128, 128, 2)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "cnn/conv_1_1/kernel:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "cnn/conv_1_1/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/conv_1_2/kernel:0 (float32_ref 3x3x2x32) [576, bytes: 2304]\n",
      "cnn/conv_1_2/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/batch_normalization/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_2/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_2/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_3/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_3/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_4/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_4/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_5/kernel:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
      "cnn/conv_5/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/dense_1/kernel:0 (float32_ref 2048x256) [524288, bytes: 2097152]\n",
      "cnn/dense_1/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "cnn/dense_output/kernel:0 (float32_ref 256x2) [512, bytes: 2048]\n",
      "cnn/dense_output/bias:0 (float32_ref 2) [2, bytes: 8]\n",
      "Total size of variables: 711970\n",
      "Total bytes of variables: 2847880\n",
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load failed...\n",
      "Epoch: [ 1] [   0/  20] time: 1.7685, loss: 0.67163789\n",
      "Epoch: [ 1] [   1/  20] time: 2.0187, loss: 0.32870910\n",
      "Epoch: [ 1] [   2/  20] time: 2.2754, loss: 0.30997643\n",
      "Epoch: [ 1] [   3/  20] time: 2.5150, loss: 0.10509555\n",
      "Epoch: [ 1] [   4/  20] time: 2.7543, loss: 0.06888263\n",
      "Epoch: [ 1] [   5/  20] time: 2.9935, loss: 0.00933050\n",
      "Epoch: [ 1] [   6/  20] time: 3.2323, loss: 0.05059861\n",
      "Epoch: [ 1] [   7/  20] time: 3.4872, loss: 0.06022267\n",
      "Epoch: [ 1] [   8/  20] time: 3.7440, loss: 0.07514434\n",
      "Epoch: [ 1] [   9/  20] time: 3.9974, loss: 0.10619386\n",
      "Epoch: [ 1] [  10/  20] time: 4.2491, loss: 0.08178532\n",
      "Epoch: [ 1] [  11/  20] time: 4.4877, loss: 0.04569075\n",
      "Epoch: [ 1] [  12/  20] time: 4.7260, loss: 0.02251173\n",
      "Epoch: [ 1] [  13/  20] time: 4.9652, loss: 0.07498529\n",
      "Epoch: [ 1] [  14/  20] time: 5.2056, loss: 0.01861329\n",
      "Epoch: [ 1] [  15/  20] time: 5.4450, loss: 0.03687458\n",
      "Epoch: [ 1] [  16/  20] time: 5.6833, loss: 0.02665488\n",
      "Epoch: [ 1] [  17/  20] time: 5.9230, loss: 0.01412065\n",
      "Epoch: [ 1] [  18/  20] time: 6.1611, loss: 0.12926278\n",
      "Epoch: [ 1] [  19/  20] time: 6.3987, loss: 0.01381952\n",
      "[1/50] - ptime: 6.4643 loss: 0.11250552 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 2] [   0/  20] time: 7.2914, loss: 0.11585923\n",
      "Epoch: [ 2] [   1/  20] time: 7.5403, loss: 0.13256130\n",
      "Epoch: [ 2] [   2/  20] time: 7.7942, loss: 0.04913461\n",
      "Epoch: [ 2] [   3/  20] time: 8.0526, loss: 0.02190615\n",
      "Epoch: [ 2] [   4/  20] time: 8.3063, loss: 0.03329057\n",
      "Epoch: [ 2] [   5/  20] time: 8.5632, loss: 0.00879893\n",
      "Epoch: [ 2] [   6/  20] time: 8.8190, loss: 0.03767242\n",
      "Epoch: [ 2] [   7/  20] time: 9.0712, loss: 0.04787113\n",
      "Epoch: [ 2] [   8/  20] time: 9.3108, loss: 0.03450225\n",
      "Epoch: [ 2] [   9/  20] time: 9.5659, loss: 0.10982012\n",
      "Epoch: [ 2] [  10/  20] time: 9.8216, loss: 0.10874722\n",
      "Epoch: [ 2] [  11/  20] time: 10.0756, loss: 0.06216695\n",
      "Epoch: [ 2] [  12/  20] time: 10.3300, loss: 0.01571423\n",
      "Epoch: [ 2] [  13/  20] time: 10.5673, loss: 0.05883779\n",
      "Epoch: [ 2] [  14/  20] time: 10.8063, loss: 0.01179162\n",
      "Epoch: [ 2] [  15/  20] time: 11.0422, loss: 0.04132001\n",
      "Epoch: [ 2] [  16/  20] time: 11.2822, loss: 0.03673657\n",
      "Epoch: [ 2] [  17/  20] time: 11.5210, loss: 0.01987603\n",
      "Epoch: [ 2] [  18/  20] time: 11.7596, loss: 0.01599592\n",
      "Epoch: [ 2] [  19/  20] time: 11.9985, loss: 0.01933656\n",
      "[2/50] - ptime: 5.2254 loss: 0.04909698 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 3] [   0/  20] time: 12.8744, loss: 0.00159414\n",
      "Epoch: [ 3] [   1/  20] time: 13.1106, loss: 0.02866082\n",
      "Epoch: [ 3] [   2/  20] time: 13.3488, loss: 0.03772342\n",
      "Epoch: [ 3] [   3/  20] time: 13.5865, loss: 0.03582161\n",
      "Epoch: [ 3] [   4/  20] time: 13.8257, loss: 0.01549710\n",
      "Epoch: [ 3] [   5/  20] time: 14.0629, loss: 0.00880934\n",
      "Epoch: [ 3] [   6/  20] time: 14.3018, loss: 0.01111720\n",
      "Epoch: [ 3] [   7/  20] time: 14.5390, loss: 0.03921222\n",
      "Epoch: [ 3] [   8/  20] time: 14.7776, loss: 0.01204539\n",
      "Epoch: [ 3] [   9/  20] time: 15.0145, loss: 0.04517946\n",
      "Epoch: [ 3] [  10/  20] time: 15.2541, loss: 0.03389623\n",
      "Epoch: [ 3] [  11/  20] time: 15.4925, loss: 0.03471130\n",
      "Epoch: [ 3] [  12/  20] time: 15.7329, loss: 0.01843695\n",
      "Epoch: [ 3] [  13/  20] time: 15.9729, loss: 0.04269718\n",
      "Epoch: [ 3] [  14/  20] time: 16.2111, loss: 0.01362938\n",
      "Epoch: [ 3] [  15/  20] time: 16.4494, loss: 0.09600860\n",
      "Epoch: [ 3] [  16/  20] time: 16.6896, loss: 0.02351352\n",
      "Epoch: [ 3] [  17/  20] time: 16.9283, loss: 0.03653487\n",
      "Epoch: [ 3] [  18/  20] time: 17.1654, loss: 0.03425807\n",
      "Epoch: [ 3] [  19/  20] time: 17.4033, loss: 0.02156023\n",
      "[3/50] - ptime: 5.0448 loss: 0.02954535 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 4] [   0/  20] time: 18.3247, loss: 0.07761779\n",
      "Epoch: [ 4] [   1/  20] time: 18.5609, loss: 0.04192772\n",
      "Epoch: [ 4] [   2/  20] time: 18.8006, loss: 0.03277575\n",
      "Epoch: [ 4] [   3/  20] time: 19.0396, loss: 0.05873975\n",
      "Epoch: [ 4] [   4/  20] time: 19.2788, loss: 0.01250554\n",
      "Epoch: [ 4] [   5/  20] time: 19.5201, loss: 0.09859073\n",
      "Epoch: [ 4] [   6/  20] time: 19.7593, loss: 0.02334005\n",
      "Epoch: [ 4] [   7/  20] time: 19.9976, loss: 0.08898184\n",
      "Epoch: [ 4] [   8/  20] time: 20.2364, loss: 0.01598752\n",
      "Epoch: [ 4] [   9/  20] time: 20.4730, loss: 0.06911062\n",
      "Epoch: [ 4] [  10/  20] time: 20.7105, loss: 0.00715189\n",
      "Epoch: [ 4] [  11/  20] time: 20.9500, loss: 0.01806786\n",
      "Epoch: [ 4] [  12/  20] time: 21.1894, loss: 0.05992033\n",
      "Epoch: [ 4] [  13/  20] time: 21.4268, loss: 0.00713147\n",
      "Epoch: [ 4] [  14/  20] time: 21.6643, loss: 0.02741199\n",
      "Epoch: [ 4] [  15/  20] time: 21.9043, loss: 0.00098694\n",
      "Epoch: [ 4] [  16/  20] time: 22.1401, loss: 0.04121742\n",
      "Epoch: [ 4] [  17/  20] time: 22.3773, loss: 0.09397408\n",
      "Epoch: [ 4] [  18/  20] time: 22.6146, loss: 0.02088230\n",
      "Epoch: [ 4] [  19/  20] time: 22.8561, loss: 0.02236250\n",
      "[4/50] - ptime: 5.0460 loss: 0.04093421 acc: 0.21000 lr: 0.00100000\n",
      "Epoch: [ 5] [   0/  20] time: 23.7150, loss: 0.03436882\n",
      "Epoch: [ 5] [   1/  20] time: 23.9524, loss: 0.03337833\n",
      "Epoch: [ 5] [   2/  20] time: 24.1908, loss: 0.01426021\n",
      "Epoch: [ 5] [   3/  20] time: 24.4282, loss: 0.02759265\n",
      "Epoch: [ 5] [   4/  20] time: 24.6648, loss: 0.00991922\n",
      "Epoch: [ 5] [   5/  20] time: 24.9041, loss: 0.08631852\n",
      "Epoch: [ 5] [   6/  20] time: 25.1422, loss: 0.00808429\n",
      "Epoch: [ 5] [   7/  20] time: 25.3813, loss: 0.02299455\n",
      "Epoch: [ 5] [   8/  20] time: 25.6191, loss: 0.01445561\n",
      "Epoch: [ 5] [   9/  20] time: 25.8576, loss: 0.01303794\n",
      "Epoch: [ 5] [  10/  20] time: 26.0955, loss: 0.04118470\n",
      "Epoch: [ 5] [  11/  20] time: 26.3353, loss: 0.02048001\n",
      "Epoch: [ 5] [  12/  20] time: 26.5733, loss: 0.03389466\n",
      "Epoch: [ 5] [  13/  20] time: 26.8112, loss: 0.01262217\n",
      "Epoch: [ 5] [  14/  20] time: 27.0499, loss: 0.01005418\n",
      "Epoch: [ 5] [  15/  20] time: 27.2946, loss: 0.03822825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 5] [  16/  20] time: 27.5318, loss: 0.06214521\n",
      "Epoch: [ 5] [  17/  20] time: 27.7730, loss: 0.03354827\n",
      "Epoch: [ 5] [  18/  20] time: 28.0114, loss: 0.01791204\n",
      "Epoch: [ 5] [  19/  20] time: 28.2517, loss: 0.02191235\n",
      "[5/50] - ptime: 5.0508 loss: 0.02781960 acc: 0.84000 lr: 0.00100000\n",
      "Epoch: [ 6] [   0/  20] time: 29.1101, loss: 0.01232774\n",
      "Epoch: [ 6] [   1/  20] time: 29.3489, loss: 0.00910412\n",
      "Epoch: [ 6] [   2/  20] time: 29.5911, loss: 0.00948165\n",
      "Epoch: [ 6] [   3/  20] time: 29.8301, loss: 0.11682923\n",
      "Epoch: [ 6] [   4/  20] time: 30.0699, loss: 0.02174560\n",
      "Epoch: [ 6] [   5/  20] time: 30.3128, loss: 0.02387386\n",
      "Epoch: [ 6] [   6/  20] time: 30.5507, loss: 0.08088167\n",
      "Epoch: [ 6] [   7/  20] time: 30.7959, loss: 0.03182695\n",
      "Epoch: [ 6] [   8/  20] time: 31.0342, loss: 0.04398748\n",
      "Epoch: [ 6] [   9/  20] time: 31.2746, loss: 0.02519491\n",
      "Epoch: [ 6] [  10/  20] time: 31.5129, loss: 0.00915665\n",
      "Epoch: [ 6] [  11/  20] time: 31.7514, loss: 0.02837698\n",
      "Epoch: [ 6] [  12/  20] time: 31.9903, loss: 0.00976517\n",
      "Epoch: [ 6] [  13/  20] time: 32.2282, loss: 0.01855570\n",
      "Epoch: [ 6] [  14/  20] time: 32.4671, loss: 0.18480963\n",
      "Epoch: [ 6] [  15/  20] time: 32.7064, loss: 0.09337505\n",
      "Epoch: [ 6] [  16/  20] time: 32.9481, loss: 0.05528462\n",
      "Epoch: [ 6] [  17/  20] time: 33.1871, loss: 0.04780418\n",
      "Epoch: [ 6] [  18/  20] time: 33.4259, loss: 0.02891972\n",
      "Epoch: [ 6] [  19/  20] time: 33.6638, loss: 0.03528242\n",
      "[6/50] - ptime: 5.0683 loss: 0.04432917 acc: 0.82000 lr: 0.00100000\n",
      "Epoch: [ 7] [   0/  20] time: 34.5161, loss: 0.00814743\n",
      "Epoch: [ 7] [   1/  20] time: 34.7532, loss: 0.03316461\n",
      "Epoch: [ 7] [   2/  20] time: 34.9909, loss: 0.01340753\n",
      "Epoch: [ 7] [   3/  20] time: 35.2305, loss: 0.03013212\n",
      "Epoch: [ 7] [   4/  20] time: 35.4690, loss: 0.01956184\n",
      "Epoch: [ 7] [   5/  20] time: 35.7077, loss: 0.00671285\n",
      "Epoch: [ 7] [   6/  20] time: 35.9484, loss: 0.03242552\n",
      "Epoch: [ 7] [   7/  20] time: 36.1872, loss: 0.03838546\n",
      "Epoch: [ 7] [   8/  20] time: 36.4248, loss: 0.10680567\n",
      "Epoch: [ 7] [   9/  20] time: 36.6625, loss: 0.02228244\n",
      "Epoch: [ 7] [  10/  20] time: 36.9016, loss: 0.04894902\n",
      "Epoch: [ 7] [  11/  20] time: 37.1396, loss: 0.00289078\n",
      "Epoch: [ 7] [  12/  20] time: 37.3796, loss: 0.02811435\n",
      "Epoch: [ 7] [  13/  20] time: 37.6199, loss: 0.03474487\n",
      "Epoch: [ 7] [  14/  20] time: 37.8600, loss: 0.01427114\n",
      "Epoch: [ 7] [  15/  20] time: 38.0989, loss: 0.05261112\n",
      "Epoch: [ 7] [  16/  20] time: 38.3367, loss: 0.02717917\n",
      "Epoch: [ 7] [  17/  20] time: 38.5752, loss: 0.02315384\n",
      "Epoch: [ 7] [  18/  20] time: 38.8144, loss: 0.02002372\n",
      "Epoch: [ 7] [  19/  20] time: 39.0524, loss: 0.02625016\n",
      "[7/50] - ptime: 5.0497 loss: 0.02946068 acc: 0.80000 lr: 0.00100000\n",
      "Epoch: [ 8] [   0/  20] time: 39.9029, loss: 0.02574440\n",
      "Epoch: [ 8] [   1/  20] time: 40.1405, loss: 0.02595829\n",
      "Epoch: [ 8] [   2/  20] time: 40.3796, loss: 0.01296674\n",
      "Epoch: [ 8] [   3/  20] time: 40.6170, loss: 0.02079827\n",
      "Epoch: [ 8] [   4/  20] time: 40.8567, loss: 0.00350100\n",
      "Epoch: [ 8] [   5/  20] time: 41.0951, loss: 0.01682882\n",
      "Epoch: [ 8] [   6/  20] time: 41.3357, loss: 0.01238013\n",
      "Epoch: [ 8] [   7/  20] time: 41.5748, loss: 0.02604436\n",
      "Epoch: [ 8] [   8/  20] time: 41.8188, loss: 0.00745224\n",
      "Epoch: [ 8] [   9/  20] time: 42.0575, loss: 0.01064942\n",
      "Epoch: [ 8] [  10/  20] time: 42.2987, loss: 0.01021441\n",
      "Epoch: [ 8] [  11/  20] time: 42.5377, loss: 0.03507235\n",
      "Epoch: [ 8] [  12/  20] time: 42.7774, loss: 0.00934819\n",
      "Epoch: [ 8] [  13/  20] time: 43.0159, loss: 0.02563676\n",
      "Epoch: [ 8] [  14/  20] time: 43.2563, loss: 0.00954167\n",
      "Epoch: [ 8] [  15/  20] time: 43.4959, loss: 0.00794912\n",
      "Epoch: [ 8] [  16/  20] time: 43.7345, loss: 0.06294422\n",
      "Epoch: [ 8] [  17/  20] time: 43.9758, loss: 0.00601876\n",
      "Epoch: [ 8] [  18/  20] time: 44.2157, loss: 0.00732776\n",
      "Epoch: [ 8] [  19/  20] time: 44.4543, loss: 0.04000556\n",
      "[8/50] - ptime: 5.0653 loss: 0.01881913 acc: 0.84000 lr: 0.00100000\n",
      "Epoch: [ 9] [   0/  20] time: 45.3233, loss: 0.00219739\n",
      "Epoch: [ 9] [   1/  20] time: 45.5602, loss: 0.01101247\n",
      "Epoch: [ 9] [   2/  20] time: 45.7996, loss: 0.00626832\n",
      "Epoch: [ 9] [   3/  20] time: 46.0390, loss: 0.02290772\n",
      "Epoch: [ 9] [   4/  20] time: 46.2785, loss: 0.02130804\n",
      "Epoch: [ 9] [   5/  20] time: 46.5161, loss: 0.02639050\n",
      "Epoch: [ 9] [   6/  20] time: 46.7546, loss: 0.03002934\n",
      "Epoch: [ 9] [   7/  20] time: 46.9928, loss: 0.02843143\n",
      "Epoch: [ 9] [   8/  20] time: 47.2318, loss: 0.00760165\n",
      "Epoch: [ 9] [   9/  20] time: 47.4718, loss: 0.02205833\n",
      "Epoch: [ 9] [  10/  20] time: 47.7107, loss: 0.00908528\n",
      "Epoch: [ 9] [  11/  20] time: 47.9503, loss: 0.03326365\n",
      "Epoch: [ 9] [  12/  20] time: 48.1894, loss: 0.00761704\n",
      "Epoch: [ 9] [  13/  20] time: 48.4297, loss: 0.04100762\n",
      "Epoch: [ 9] [  14/  20] time: 48.6704, loss: 0.06141213\n",
      "Epoch: [ 9] [  15/  20] time: 48.9110, loss: 0.01490985\n",
      "Epoch: [ 9] [  16/  20] time: 49.1492, loss: 0.02337257\n",
      "Epoch: [ 9] [  17/  20] time: 49.3886, loss: 0.00427085\n",
      "Epoch: [ 9] [  18/  20] time: 49.6327, loss: 0.02875608\n",
      "Epoch: [ 9] [  19/  20] time: 49.8724, loss: 0.06041024\n",
      "[9/50] - ptime: 5.0663 loss: 0.02311553 acc: 0.81000 lr: 0.00100000\n",
      "Epoch: [10] [   0/  20] time: 50.7706, loss: 0.05093537\n",
      "Epoch: [10] [   1/  20] time: 51.0078, loss: 0.01276632\n",
      "Epoch: [10] [   2/  20] time: 51.2469, loss: 0.02490526\n",
      "Epoch: [10] [   3/  20] time: 51.4860, loss: 0.02313291\n",
      "Epoch: [10] [   4/  20] time: 51.7248, loss: 0.00352451\n",
      "Epoch: [10] [   5/  20] time: 51.9649, loss: 0.00933805\n",
      "Epoch: [10] [   6/  20] time: 52.2043, loss: 0.02191089\n",
      "Epoch: [10] [   7/  20] time: 52.4439, loss: 0.02340998\n",
      "Epoch: [10] [   8/  20] time: 52.6835, loss: 0.00751596\n",
      "Epoch: [10] [   9/  20] time: 52.9227, loss: 0.00767610\n",
      "Epoch: [10] [  10/  20] time: 53.1621, loss: 0.01201540\n",
      "Epoch: [10] [  11/  20] time: 53.4001, loss: 0.00587371\n",
      "Epoch: [10] [  12/  20] time: 53.6383, loss: 0.02447974\n",
      "Epoch: [10] [  13/  20] time: 53.8776, loss: 0.00663653\n",
      "Epoch: [10] [  14/  20] time: 54.1154, loss: 0.01016539\n",
      "Epoch: [10] [  15/  20] time: 54.3558, loss: 0.01591888\n",
      "Epoch: [10] [  16/  20] time: 54.5946, loss: 0.00799664\n",
      "Epoch: [10] [  17/  20] time: 54.8344, loss: 0.02381882\n",
      "Epoch: [10] [  18/  20] time: 55.0731, loss: 0.01146765\n",
      "Epoch: [10] [  19/  20] time: 55.3131, loss: 0.00808053\n",
      "[10/50] - ptime: 5.0627 loss: 0.01557843 acc: 0.80000 lr: 0.00090000\n",
      "Epoch: [11] [   0/  20] time: 56.2004, loss: 0.02787153\n",
      "Epoch: [11] [   1/  20] time: 56.4391, loss: 0.01700374\n",
      "Epoch: [11] [   2/  20] time: 56.6779, loss: 0.00663001\n",
      "Epoch: [11] [   3/  20] time: 56.9168, loss: 0.00517498\n",
      "Epoch: [11] [   4/  20] time: 57.1551, loss: 0.03919269\n",
      "Epoch: [11] [   5/  20] time: 57.3960, loss: 0.00627364\n",
      "Epoch: [11] [   6/  20] time: 57.6345, loss: 0.00530218\n",
      "Epoch: [11] [   7/  20] time: 57.8781, loss: 0.02341495\n",
      "Epoch: [11] [   8/  20] time: 58.1164, loss: 0.01441555\n",
      "Epoch: [11] [   9/  20] time: 58.3577, loss: 0.00018286\n",
      "Epoch: [11] [  10/  20] time: 58.5975, loss: 0.00328618\n",
      "Epoch: [11] [  11/  20] time: 58.8371, loss: 0.00883032\n",
      "Epoch: [11] [  12/  20] time: 59.0757, loss: 0.00463519\n",
      "Epoch: [11] [  13/  20] time: 59.3163, loss: 0.00345687\n",
      "Epoch: [11] [  14/  20] time: 59.5547, loss: 0.00568307\n",
      "Epoch: [11] [  15/  20] time: 59.7937, loss: 0.02350483\n",
      "Epoch: [11] [  16/  20] time: 60.0318, loss: 0.01642780\n",
      "Epoch: [11] [  17/  20] time: 60.2719, loss: 0.02360598\n",
      "Epoch: [11] [  18/  20] time: 60.5105, loss: 0.01430315\n",
      "Epoch: [11] [  19/  20] time: 60.7500, loss: 0.02597081\n",
      "[11/50] - ptime: 5.0637 loss: 0.01375832 acc: 0.74000 lr: 0.00090000\n",
      "Epoch: [12] [   0/  20] time: 61.6623, loss: 0.00562604\n",
      "Epoch: [12] [   1/  20] time: 61.9001, loss: 0.00847693\n",
      "Epoch: [12] [   2/  20] time: 62.1379, loss: 0.01723571\n",
      "Epoch: [12] [   3/  20] time: 62.3772, loss: 0.00222959\n",
      "Epoch: [12] [   4/  20] time: 62.6158, loss: 0.00014991\n",
      "Epoch: [12] [   5/  20] time: 62.8581, loss: 0.00494027\n",
      "Epoch: [12] [   6/  20] time: 63.0975, loss: 0.00054354\n",
      "Epoch: [12] [   7/  20] time: 63.3382, loss: 0.00671154\n",
      "Epoch: [12] [   8/  20] time: 63.5777, loss: 0.01181606\n",
      "Epoch: [12] [   9/  20] time: 63.8171, loss: 0.03736081\n",
      "Epoch: [12] [  10/  20] time: 64.0574, loss: 0.00367372\n",
      "Epoch: [12] [  11/  20] time: 64.2970, loss: 0.00311384\n",
      "Epoch: [12] [  12/  20] time: 64.5366, loss: 0.00108896\n",
      "Epoch: [12] [  13/  20] time: 64.7775, loss: 0.00878222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12] [  14/  20] time: 65.0155, loss: 0.00459499\n",
      "Epoch: [12] [  15/  20] time: 65.2550, loss: 0.00652505\n",
      "Epoch: [12] [  16/  20] time: 65.4950, loss: 0.00004623\n",
      "Epoch: [12] [  17/  20] time: 65.7354, loss: 0.00036902\n",
      "Epoch: [12] [  18/  20] time: 65.9748, loss: 0.01319629\n",
      "Epoch: [12] [  19/  20] time: 66.2128, loss: 0.00432566\n",
      "[12/50] - ptime: 5.0720 loss: 0.00704032 acc: 0.80000 lr: 0.00090000\n",
      "Epoch: [13] [   0/  20] time: 67.0727, loss: 0.00266548\n",
      "Epoch: [13] [   1/  20] time: 67.3110, loss: 0.00054162\n",
      "Epoch: [13] [   2/  20] time: 67.5498, loss: 0.00757874\n",
      "Epoch: [13] [   3/  20] time: 67.7918, loss: 0.00017282\n",
      "Epoch: [13] [   4/  20] time: 68.0312, loss: 0.02463419\n",
      "Epoch: [13] [   5/  20] time: 68.2725, loss: 0.00401925\n",
      "Epoch: [13] [   6/  20] time: 68.5118, loss: 0.00357641\n",
      "Epoch: [13] [   7/  20] time: 68.7521, loss: 0.03159787\n",
      "Epoch: [13] [   8/  20] time: 68.9919, loss: 0.00889143\n",
      "Epoch: [13] [   9/  20] time: 69.2310, loss: 0.01334186\n",
      "Epoch: [13] [  10/  20] time: 69.4703, loss: 0.00503416\n",
      "Epoch: [13] [  11/  20] time: 69.7091, loss: 0.01316150\n",
      "Epoch: [13] [  12/  20] time: 69.9494, loss: 0.00398855\n",
      "Epoch: [13] [  13/  20] time: 70.1905, loss: 0.00663508\n",
      "Epoch: [13] [  14/  20] time: 70.4333, loss: 0.00688600\n",
      "Epoch: [13] [  15/  20] time: 70.6722, loss: 0.00253923\n",
      "Epoch: [13] [  16/  20] time: 70.9127, loss: 0.00012457\n",
      "Epoch: [13] [  17/  20] time: 71.1525, loss: 0.00800062\n",
      "Epoch: [13] [  18/  20] time: 71.3933, loss: 0.00306001\n",
      "Epoch: [13] [  19/  20] time: 71.6333, loss: 0.00456568\n",
      "[13/50] - ptime: 5.0736 loss: 0.00755075 acc: 0.80000 lr: 0.00090000\n",
      "Epoch: [14] [   0/  20] time: 72.4926, loss: 0.00082403\n",
      "Epoch: [14] [   1/  20] time: 72.7293, loss: 0.00003367\n",
      "Epoch: [14] [   2/  20] time: 72.9689, loss: 0.00101706\n",
      "Epoch: [14] [   3/  20] time: 73.2077, loss: 0.00108562\n",
      "Epoch: [14] [   4/  20] time: 73.4491, loss: 0.00148076\n",
      "Epoch: [14] [   5/  20] time: 73.6884, loss: 0.00094490\n",
      "Epoch: [14] [   6/  20] time: 73.9294, loss: 0.00030145\n",
      "Epoch: [14] [   7/  20] time: 74.1688, loss: 0.00306935\n",
      "Epoch: [14] [   8/  20] time: 74.4110, loss: 0.00209751\n",
      "Epoch: [14] [   9/  20] time: 74.6503, loss: 0.00049749\n",
      "Epoch: [14] [  10/  20] time: 74.8913, loss: 0.00016958\n",
      "Epoch: [14] [  11/  20] time: 75.1317, loss: 0.00000338\n",
      "Epoch: [14] [  12/  20] time: 75.3742, loss: 0.00080708\n",
      "Epoch: [14] [  13/  20] time: 75.6144, loss: 0.01626826\n",
      "Epoch: [14] [  14/  20] time: 75.8565, loss: 0.00562935\n",
      "Epoch: [14] [  15/  20] time: 76.0968, loss: 0.00061915\n",
      "Epoch: [14] [  16/  20] time: 76.3398, loss: 0.00068504\n",
      "Epoch: [14] [  17/  20] time: 76.5784, loss: 0.01144886\n",
      "Epoch: [14] [  18/  20] time: 76.8190, loss: 0.00301160\n",
      "Epoch: [14] [  19/  20] time: 77.0590, loss: 0.00001200\n",
      "[14/50] - ptime: 5.0808 loss: 0.00250031 acc: 0.82000 lr: 0.00090000\n",
      "Epoch: [15] [   0/  20] time: 77.9105, loss: 0.00076243\n",
      "Epoch: [15] [   1/  20] time: 78.1475, loss: 0.00053980\n",
      "Epoch: [15] [   2/  20] time: 78.3871, loss: 0.00167122\n",
      "Epoch: [15] [   3/  20] time: 78.6256, loss: 0.00031615\n",
      "Epoch: [15] [   4/  20] time: 78.8658, loss: 0.00092810\n",
      "Epoch: [15] [   5/  20] time: 79.1054, loss: 0.00402093\n",
      "Epoch: [15] [   6/  20] time: 79.3460, loss: 0.00182803\n",
      "Epoch: [15] [   7/  20] time: 79.5863, loss: 0.00081409\n",
      "Epoch: [15] [   8/  20] time: 79.8278, loss: 0.00663075\n",
      "Epoch: [15] [   9/  20] time: 80.0669, loss: 0.00159742\n",
      "Epoch: [15] [  10/  20] time: 80.3090, loss: 0.00119798\n",
      "Epoch: [15] [  11/  20] time: 80.5484, loss: 0.00307526\n",
      "Epoch: [15] [  12/  20] time: 80.7891, loss: 0.00170423\n",
      "Epoch: [15] [  13/  20] time: 81.0243, loss: 0.00012238\n",
      "Epoch: [15] [  14/  20] time: 81.2630, loss: 0.02074188\n",
      "Epoch: [15] [  15/  20] time: 81.5017, loss: 0.00584979\n",
      "Epoch: [15] [  16/  20] time: 81.7402, loss: 0.00093074\n",
      "Epoch: [15] [  17/  20] time: 81.9799, loss: 0.00091072\n",
      "Epoch: [15] [  18/  20] time: 82.2181, loss: 0.00024657\n",
      "Epoch: [15] [  19/  20] time: 82.4576, loss: 0.00358466\n",
      "[15/50] - ptime: 5.0612 loss: 0.00287366 acc: 0.80000 lr: 0.00090000\n",
      "Epoch: [16] [   0/  20] time: 83.3166, loss: 0.00024592\n",
      "Epoch: [16] [   1/  20] time: 83.5675, loss: 0.02566176\n",
      "Epoch: [16] [   2/  20] time: 83.8068, loss: 0.00020315\n",
      "Epoch: [16] [   3/  20] time: 84.0454, loss: 0.01448123\n",
      "Epoch: [16] [   4/  20] time: 84.2841, loss: 0.00000128\n",
      "Epoch: [16] [   5/  20] time: 84.5229, loss: 0.00213690\n",
      "Epoch: [16] [   6/  20] time: 84.7639, loss: 0.01923351\n",
      "Epoch: [16] [   7/  20] time: 85.0022, loss: 0.00547017\n",
      "Epoch: [16] [   8/  20] time: 85.2417, loss: 0.00282447\n",
      "Epoch: [16] [   9/  20] time: 85.4907, loss: 0.00064425\n",
      "Epoch: [16] [  10/  20] time: 85.7295, loss: 0.01452146\n",
      "Epoch: [16] [  11/  20] time: 85.9690, loss: 0.00092013\n",
      "Epoch: [16] [  12/  20] time: 86.2073, loss: 0.00077461\n",
      "Epoch: [16] [  13/  20] time: 86.4481, loss: 0.00511783\n",
      "Epoch: [16] [  14/  20] time: 86.6874, loss: 0.00251277\n",
      "Epoch: [16] [  15/  20] time: 86.9270, loss: 0.00167345\n",
      "Epoch: [16] [  16/  20] time: 87.1671, loss: 0.00193016\n",
      "Epoch: [16] [  17/  20] time: 87.4089, loss: 0.00115860\n",
      "Epoch: [16] [  18/  20] time: 87.6485, loss: 0.00060438\n",
      "Epoch: [16] [  19/  20] time: 87.8885, loss: 0.00183772\n",
      "[16/50] - ptime: 5.0940 loss: 0.00509769 acc: 0.78000 lr: 0.00090000\n",
      "Epoch: [17] [   0/  20] time: 88.8573, loss: 0.00001988\n",
      "Epoch: [17] [   1/  20] time: 89.0950, loss: 0.00035049\n",
      "Epoch: [17] [   2/  20] time: 89.3360, loss: 0.00025652\n",
      "Epoch: [17] [   3/  20] time: 89.5763, loss: 0.00099254\n",
      "Epoch: [17] [   4/  20] time: 89.8175, loss: 0.00000382\n",
      "Epoch: [17] [   5/  20] time: 90.0567, loss: 0.00028357\n",
      "Epoch: [17] [   6/  20] time: 90.2970, loss: 0.00308722\n",
      "Epoch: [17] [   7/  20] time: 90.5373, loss: 0.00662293\n",
      "Epoch: [17] [   8/  20] time: 90.7770, loss: 0.00001069\n",
      "Epoch: [17] [   9/  20] time: 91.0159, loss: 0.00031144\n",
      "Epoch: [17] [  10/  20] time: 91.2549, loss: 0.00508414\n",
      "Epoch: [17] [  11/  20] time: 91.4966, loss: 0.00060183\n",
      "Epoch: [17] [  12/  20] time: 91.7352, loss: 0.00010738\n",
      "Epoch: [17] [  13/  20] time: 91.9764, loss: 0.00446880\n",
      "Epoch: [17] [  14/  20] time: 92.2152, loss: 0.00051038\n",
      "Epoch: [17] [  15/  20] time: 92.4551, loss: 0.00468539\n",
      "Epoch: [17] [  16/  20] time: 92.6953, loss: 0.00039244\n",
      "Epoch: [17] [  17/  20] time: 92.9373, loss: 0.00326394\n",
      "Epoch: [17] [  18/  20] time: 93.1772, loss: 0.00077614\n",
      "Epoch: [17] [  19/  20] time: 93.4191, loss: 0.00603470\n",
      "[17/50] - ptime: 5.0817 loss: 0.00189321 acc: 0.79000 lr: 0.00090000\n",
      "Epoch: [18] [   0/  20] time: 94.2761, loss: 0.00044456\n",
      "Epoch: [18] [   1/  20] time: 94.5137, loss: 0.00195029\n",
      "Epoch: [18] [   2/  20] time: 94.7520, loss: 0.00191640\n",
      "Epoch: [18] [   3/  20] time: 94.9915, loss: 0.01722199\n",
      "Epoch: [18] [   4/  20] time: 95.2316, loss: 0.00070065\n",
      "Epoch: [18] [   5/  20] time: 95.4727, loss: 0.01578843\n",
      "Epoch: [18] [   6/  20] time: 95.7144, loss: 0.00093225\n",
      "Epoch: [18] [   7/  20] time: 95.9546, loss: 0.00106213\n",
      "Epoch: [18] [   8/  20] time: 96.1943, loss: 0.00009894\n",
      "Epoch: [18] [   9/  20] time: 96.4354, loss: 0.00005297\n",
      "Epoch: [18] [  10/  20] time: 96.6755, loss: 0.00041857\n",
      "Epoch: [18] [  11/  20] time: 96.9149, loss: 0.02548680\n",
      "Epoch: [18] [  12/  20] time: 97.1548, loss: 0.00090530\n",
      "Epoch: [18] [  13/  20] time: 97.3962, loss: 0.02582182\n",
      "Epoch: [18] [  14/  20] time: 97.6354, loss: 0.04896814\n",
      "Epoch: [18] [  15/  20] time: 97.8768, loss: 0.00017586\n",
      "Epoch: [18] [  16/  20] time: 98.1153, loss: 0.00448939\n",
      "Epoch: [18] [  17/  20] time: 98.3553, loss: 0.00008380\n",
      "Epoch: [18] [  18/  20] time: 98.5939, loss: 0.00674625\n",
      "Epoch: [18] [  19/  20] time: 98.8335, loss: 0.00021447\n",
      "[18/50] - ptime: 5.0705 loss: 0.00767395 acc: 0.76000 lr: 0.00090000\n",
      "Epoch: [19] [   0/  20] time: 99.6863, loss: 0.00004866\n",
      "Epoch: [19] [   1/  20] time: 99.9253, loss: 0.00045810\n",
      "Epoch: [19] [   2/  20] time: 100.1650, loss: 0.00012763\n",
      "Epoch: [19] [   3/  20] time: 100.4061, loss: 0.00650557\n",
      "Epoch: [19] [   4/  20] time: 100.6452, loss: 0.00456111\n",
      "Epoch: [19] [   5/  20] time: 100.8860, loss: 0.00074608\n",
      "Epoch: [19] [   6/  20] time: 101.1256, loss: 0.00121944\n",
      "Epoch: [19] [   7/  20] time: 101.3662, loss: 0.00658760\n",
      "Epoch: [19] [   8/  20] time: 101.6053, loss: 0.00532570\n",
      "Epoch: [19] [   9/  20] time: 101.8456, loss: 0.01952671\n",
      "Epoch: [19] [  10/  20] time: 102.0844, loss: 0.03434480\n",
      "Epoch: [19] [  11/  20] time: 102.3236, loss: 0.00100617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19] [  12/  20] time: 102.5628, loss: 0.02546882\n",
      "Epoch: [19] [  13/  20] time: 102.8002, loss: 0.00292908\n",
      "Epoch: [19] [  14/  20] time: 103.0387, loss: 0.06572141\n",
      "Epoch: [19] [  15/  20] time: 103.2779, loss: 0.00539473\n",
      "Epoch: [19] [  16/  20] time: 103.5177, loss: 0.01992316\n",
      "Epoch: [19] [  17/  20] time: 103.7587, loss: 0.00812122\n",
      "Epoch: [19] [  18/  20] time: 103.9987, loss: 0.04116411\n",
      "Epoch: [19] [  19/  20] time: 104.2389, loss: 0.00935482\n",
      "[19/50] - ptime: 5.0679 loss: 0.01292675 acc: 0.77000 lr: 0.00090000\n",
      "Epoch: [20] [   0/  20] time: 105.0924, loss: 0.03384462\n",
      "Epoch: [20] [   1/  20] time: 105.3305, loss: 0.05679308\n",
      "Epoch: [20] [   2/  20] time: 105.5684, loss: 0.00764840\n",
      "Epoch: [20] [   3/  20] time: 105.8086, loss: 0.00262183\n",
      "Epoch: [20] [   4/  20] time: 106.0472, loss: 0.02778685\n",
      "Epoch: [20] [   5/  20] time: 106.2869, loss: 0.04493186\n",
      "Epoch: [20] [   6/  20] time: 106.5268, loss: 0.00665246\n",
      "Epoch: [20] [   7/  20] time: 106.7663, loss: 0.00517163\n",
      "Epoch: [20] [   8/  20] time: 107.0059, loss: 0.00722645\n",
      "Epoch: [20] [   9/  20] time: 107.2464, loss: 0.00261819\n",
      "Epoch: [20] [  10/  20] time: 107.4852, loss: 0.05572373\n",
      "Epoch: [20] [  11/  20] time: 107.7247, loss: 0.01158688\n",
      "Epoch: [20] [  12/  20] time: 107.9653, loss: 0.00927663\n",
      "Epoch: [20] [  13/  20] time: 108.2055, loss: 0.00152335\n",
      "Epoch: [20] [  14/  20] time: 108.4476, loss: 0.01470163\n",
      "Epoch: [20] [  15/  20] time: 108.6865, loss: 0.00692664\n",
      "Epoch: [20] [  16/  20] time: 108.9277, loss: 0.02152759\n",
      "Epoch: [20] [  17/  20] time: 109.1662, loss: 0.02680086\n",
      "Epoch: [20] [  18/  20] time: 109.4070, loss: 0.00072778\n",
      "Epoch: [20] [  19/  20] time: 109.6504, loss: 0.00702636\n",
      "[20/50] - ptime: 5.0714 loss: 0.01755584 acc: 0.78000 lr: 0.00081000\n",
      "Epoch: [21] [   0/  20] time: 110.5110, loss: 0.00015958\n",
      "Epoch: [21] [   1/  20] time: 110.7494, loss: 0.00270825\n",
      "Epoch: [21] [   2/  20] time: 110.9884, loss: 0.01250797\n",
      "Epoch: [21] [   3/  20] time: 111.2272, loss: 0.00021777\n",
      "Epoch: [21] [   4/  20] time: 111.4673, loss: 0.00671330\n",
      "Epoch: [21] [   5/  20] time: 111.7063, loss: 0.05167220\n",
      "Epoch: [21] [   6/  20] time: 111.9467, loss: 0.00017368\n",
      "Epoch: [21] [   7/  20] time: 112.1851, loss: 0.00694459\n",
      "Epoch: [21] [   8/  20] time: 112.4261, loss: 0.01030792\n",
      "Epoch: [21] [   9/  20] time: 112.6650, loss: 0.00516346\n",
      "Epoch: [21] [  10/  20] time: 112.9061, loss: 0.00005666\n",
      "Epoch: [21] [  11/  20] time: 113.1456, loss: 0.00066447\n",
      "Epoch: [21] [  12/  20] time: 113.3859, loss: 0.02206348\n",
      "Epoch: [21] [  13/  20] time: 113.6273, loss: 0.00012568\n",
      "Epoch: [21] [  14/  20] time: 113.8685, loss: 0.00472422\n",
      "Epoch: [21] [  15/  20] time: 114.1086, loss: 0.00093293\n",
      "Epoch: [21] [  16/  20] time: 114.3515, loss: 0.00131381\n",
      "Epoch: [21] [  17/  20] time: 114.5908, loss: 0.00297605\n",
      "Epoch: [21] [  18/  20] time: 114.8311, loss: 0.00577777\n",
      "Epoch: [21] [  19/  20] time: 115.0705, loss: 0.01832396\n",
      "[21/50] - ptime: 5.0778 loss: 0.00767639 acc: 0.77000 lr: 0.00081000\n",
      "Epoch: [22] [   0/  20] time: 115.9343, loss: 0.00752244\n",
      "Epoch: [22] [   1/  20] time: 116.1718, loss: 0.00002802\n",
      "Epoch: [22] [   2/  20] time: 116.4125, loss: 0.00078582\n",
      "Epoch: [22] [   3/  20] time: 116.6533, loss: 0.00536555\n",
      "Epoch: [22] [   4/  20] time: 116.8937, loss: 0.00076109\n",
      "Epoch: [22] [   5/  20] time: 117.1347, loss: 0.00025384\n",
      "Epoch: [22] [   6/  20] time: 117.3780, loss: 0.00077659\n",
      "Epoch: [22] [   7/  20] time: 117.6180, loss: 0.02280524\n",
      "Epoch: [22] [   8/  20] time: 117.8586, loss: 0.00040027\n",
      "Epoch: [22] [   9/  20] time: 118.0987, loss: 0.00187195\n",
      "Epoch: [22] [  10/  20] time: 118.3358, loss: 0.00196618\n",
      "Epoch: [22] [  11/  20] time: 118.5762, loss: 0.01721763\n",
      "Epoch: [22] [  12/  20] time: 118.8207, loss: 0.01052310\n",
      "Epoch: [22] [  13/  20] time: 119.0597, loss: 0.00408853\n",
      "Epoch: [22] [  14/  20] time: 119.2995, loss: 0.00176627\n",
      "Epoch: [22] [  15/  20] time: 119.5388, loss: 0.00038983\n",
      "Epoch: [22] [  16/  20] time: 119.7791, loss: 0.00305047\n",
      "Epoch: [22] [  17/  20] time: 120.0177, loss: 0.00102474\n",
      "Epoch: [22] [  18/  20] time: 120.2574, loss: 0.00023895\n",
      "Epoch: [22] [  19/  20] time: 120.4997, loss: 0.00447078\n",
      "[22/50] - ptime: 5.0787 loss: 0.00426536 acc: 0.78000 lr: 0.00081000\n",
      "Epoch: [23] [   0/  20] time: 121.3453, loss: 0.00014587\n",
      "Epoch: [23] [   1/  20] time: 121.5833, loss: 0.00094349\n",
      "Epoch: [23] [   2/  20] time: 121.8246, loss: 0.00010171\n",
      "Epoch: [23] [   3/  20] time: 122.0634, loss: 0.00007288\n",
      "Epoch: [23] [   4/  20] time: 122.3019, loss: 0.00522378\n",
      "Epoch: [23] [   5/  20] time: 122.5414, loss: 0.00044341\n",
      "Epoch: [23] [   6/  20] time: 122.7803, loss: 0.00016116\n",
      "Epoch: [23] [   7/  20] time: 123.0187, loss: 0.00008272\n",
      "Epoch: [23] [   8/  20] time: 123.2580, loss: 0.00065315\n",
      "Epoch: [23] [   9/  20] time: 123.5036, loss: 0.00080955\n",
      "Epoch: [23] [  10/  20] time: 123.7420, loss: 0.00029717\n",
      "Epoch: [23] [  11/  20] time: 123.9821, loss: 0.00002358\n",
      "Epoch: [23] [  12/  20] time: 124.2210, loss: 0.00000132\n",
      "Epoch: [23] [  13/  20] time: 124.4613, loss: 0.00060496\n",
      "Epoch: [23] [  14/  20] time: 124.7004, loss: 0.00003884\n",
      "Epoch: [23] [  15/  20] time: 124.9411, loss: 0.00005858\n",
      "Epoch: [23] [  16/  20] time: 125.1835, loss: 0.00029388\n",
      "Epoch: [23] [  17/  20] time: 125.4234, loss: 0.00080169\n",
      "Epoch: [23] [  18/  20] time: 125.6637, loss: 0.00009844\n",
      "Epoch: [23] [  19/  20] time: 125.9071, loss: 0.00075275\n",
      "[23/50] - ptime: 5.0758 loss: 0.00058045 acc: 0.79000 lr: 0.00081000\n",
      "Epoch: [24] [   0/  20] time: 126.7622, loss: 0.00000350\n",
      "Epoch: [24] [   1/  20] time: 127.0000, loss: 0.00040831\n",
      "Epoch: [24] [   2/  20] time: 127.2403, loss: 0.00006373\n",
      "Epoch: [24] [   3/  20] time: 127.4805, loss: 0.00007844\n",
      "Epoch: [24] [   4/  20] time: 127.7186, loss: 0.00040941\n",
      "Epoch: [24] [   5/  20] time: 127.9581, loss: 0.00062511\n",
      "Epoch: [24] [   6/  20] time: 128.1971, loss: 0.00186621\n",
      "Epoch: [24] [   7/  20] time: 128.4377, loss: 0.00001096\n",
      "Epoch: [24] [   8/  20] time: 128.6777, loss: 0.00034874\n",
      "Epoch: [24] [   9/  20] time: 128.9174, loss: 0.00002076\n",
      "Epoch: [24] [  10/  20] time: 129.1568, loss: 0.00015445\n",
      "Epoch: [24] [  11/  20] time: 129.3966, loss: 0.00002474\n",
      "Epoch: [24] [  12/  20] time: 129.6366, loss: 0.00063298\n",
      "Epoch: [24] [  13/  20] time: 129.8796, loss: 0.00000389\n",
      "Epoch: [24] [  14/  20] time: 130.1187, loss: 0.00009749\n",
      "Epoch: [24] [  15/  20] time: 130.3594, loss: 0.00037695\n",
      "Epoch: [24] [  16/  20] time: 130.6024, loss: 0.00018476\n",
      "Epoch: [24] [  17/  20] time: 130.8435, loss: 0.00026052\n",
      "Epoch: [24] [  18/  20] time: 131.0830, loss: 0.00043509\n",
      "Epoch: [24] [  19/  20] time: 131.3230, loss: 0.00118262\n",
      "[24/50] - ptime: 5.0755 loss: 0.00035943 acc: 0.79000 lr: 0.00081000\n",
      "Epoch: [25] [   0/  20] time: 132.1783, loss: 0.00019821\n",
      "Epoch: [25] [   1/  20] time: 132.4183, loss: 0.00029008\n",
      "Epoch: [25] [   2/  20] time: 132.6579, loss: 0.00000369\n",
      "Epoch: [25] [   3/  20] time: 132.8987, loss: 0.00001878\n",
      "Epoch: [25] [   4/  20] time: 133.1372, loss: 0.00002479\n",
      "Epoch: [25] [   5/  20] time: 133.3774, loss: 0.00015825\n",
      "Epoch: [25] [   6/  20] time: 133.6157, loss: 0.00035070\n",
      "Epoch: [25] [   7/  20] time: 133.8559, loss: 0.00005600\n",
      "Epoch: [25] [   8/  20] time: 134.0951, loss: 0.00013511\n",
      "Epoch: [25] [   9/  20] time: 134.3343, loss: 0.00018923\n",
      "Epoch: [25] [  10/  20] time: 134.5746, loss: 0.00059109\n",
      "Epoch: [25] [  11/  20] time: 134.8161, loss: 0.00011908\n",
      "Epoch: [25] [  12/  20] time: 135.0562, loss: 0.00001880\n",
      "Epoch: [25] [  13/  20] time: 135.2957, loss: 0.00001496\n",
      "Epoch: [25] [  14/  20] time: 135.5357, loss: 0.00000869\n",
      "Epoch: [25] [  15/  20] time: 135.7757, loss: 0.00012339\n",
      "Epoch: [25] [  16/  20] time: 136.0149, loss: 0.00014981\n",
      "Epoch: [25] [  17/  20] time: 136.2543, loss: 0.00002000\n",
      "Epoch: [25] [  18/  20] time: 136.4961, loss: 0.00007747\n",
      "Epoch: [25] [  19/  20] time: 136.7381, loss: 0.00001510\n",
      "[25/50] - ptime: 5.0740 loss: 0.00012816 acc: 0.79000 lr: 0.00081000\n",
      "Epoch: [26] [   0/  20] time: 137.6158, loss: 0.00000206\n",
      "Epoch: [26] [   1/  20] time: 137.8544, loss: 0.00000068\n",
      "Epoch: [26] [   2/  20] time: 138.0941, loss: 0.00021324\n",
      "Epoch: [26] [   3/  20] time: 138.3332, loss: 0.00002800\n",
      "Epoch: [26] [   4/  20] time: 138.5725, loss: 0.00016412\n",
      "Epoch: [26] [   5/  20] time: 138.8124, loss: 0.00066108\n",
      "Epoch: [26] [   6/  20] time: 139.0511, loss: 0.00006877\n",
      "Epoch: [26] [   7/  20] time: 139.2905, loss: 0.00000795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26] [   8/  20] time: 139.5320, loss: 0.00018190\n",
      "Epoch: [26] [   9/  20] time: 139.7715, loss: 0.00002617\n",
      "Epoch: [26] [  10/  20] time: 140.0103, loss: 0.00011239\n",
      "Epoch: [26] [  11/  20] time: 140.2502, loss: 0.00007026\n",
      "Epoch: [26] [  12/  20] time: 140.4921, loss: 0.00036922\n",
      "Epoch: [26] [  13/  20] time: 140.7313, loss: 0.00008284\n",
      "Epoch: [26] [  14/  20] time: 140.9744, loss: 0.00002314\n",
      "Epoch: [26] [  15/  20] time: 141.2146, loss: 0.00006704\n",
      "Epoch: [26] [  16/  20] time: 141.4553, loss: 0.00000106\n",
      "Epoch: [26] [  17/  20] time: 141.6956, loss: 0.00047487\n",
      "Epoch: [26] [  18/  20] time: 141.9382, loss: 0.00002909\n",
      "Epoch: [26] [  19/  20] time: 142.1771, loss: 0.00000939\n",
      "[26/50] - ptime: 5.0831 loss: 0.00012966 acc: 0.79000 lr: 0.00081000\n",
      "Epoch: [27] [   0/  20] time: 143.0248, loss: 0.00008783\n",
      "Epoch: [27] [   1/  20] time: 143.2641, loss: 0.00010216\n",
      "Epoch: [27] [   2/  20] time: 143.5045, loss: 0.00004757\n",
      "Epoch: [27] [   3/  20] time: 143.7436, loss: 0.00000471\n",
      "Epoch: [27] [   4/  20] time: 143.9830, loss: 0.00025069\n",
      "Epoch: [27] [   5/  20] time: 144.2221, loss: 0.00000694\n",
      "Epoch: [27] [   6/  20] time: 144.4626, loss: 0.00003794\n",
      "Epoch: [27] [   7/  20] time: 144.7019, loss: 0.00026472\n",
      "Epoch: [27] [   8/  20] time: 144.9433, loss: 0.00050266\n",
      "Epoch: [27] [   9/  20] time: 145.1833, loss: 0.00000513\n",
      "Epoch: [27] [  10/  20] time: 145.4237, loss: 0.00001625\n",
      "Epoch: [27] [  11/  20] time: 145.6633, loss: 0.00007192\n",
      "Epoch: [27] [  12/  20] time: 145.9046, loss: 0.00013244\n",
      "Epoch: [27] [  13/  20] time: 146.1447, loss: 0.00000938\n",
      "Epoch: [27] [  14/  20] time: 146.3853, loss: 0.00005661\n",
      "Epoch: [27] [  15/  20] time: 146.6260, loss: 0.00001433\n",
      "Epoch: [27] [  16/  20] time: 146.8668, loss: 0.00003851\n",
      "Epoch: [27] [  17/  20] time: 147.1073, loss: 0.00004223\n",
      "Epoch: [27] [  18/  20] time: 147.3470, loss: 0.00004454\n",
      "Epoch: [27] [  19/  20] time: 147.5873, loss: 0.00000796\n",
      "[27/50] - ptime: 5.0752 loss: 0.00008723 acc: 0.75000 lr: 0.00081000\n",
      "Epoch: [28] [   0/  20] time: 148.4370, loss: 0.00001989\n",
      "Epoch: [28] [   1/  20] time: 148.6748, loss: 0.00010777\n",
      "Epoch: [28] [   2/  20] time: 148.9155, loss: 0.00003373\n",
      "Epoch: [28] [   3/  20] time: 149.1541, loss: 0.00001980\n",
      "Epoch: [28] [   4/  20] time: 149.3948, loss: 0.00001422\n",
      "Epoch: [28] [   5/  20] time: 149.6347, loss: 0.00002217\n",
      "Epoch: [28] [   6/  20] time: 149.8757, loss: 0.00093098\n",
      "Epoch: [28] [   7/  20] time: 150.1163, loss: 0.00059158\n",
      "Epoch: [28] [   8/  20] time: 150.3555, loss: 0.00000753\n",
      "Epoch: [28] [   9/  20] time: 150.5972, loss: 0.00001541\n",
      "Epoch: [28] [  10/  20] time: 150.8386, loss: 0.00094893\n",
      "Epoch: [28] [  11/  20] time: 151.0786, loss: 0.00005209\n",
      "Epoch: [28] [  12/  20] time: 151.3184, loss: 0.00006746\n",
      "Epoch: [28] [  13/  20] time: 151.5609, loss: 0.00043285\n",
      "Epoch: [28] [  14/  20] time: 151.8077, loss: 0.00000320\n",
      "Epoch: [28] [  15/  20] time: 152.0466, loss: 0.00024290\n",
      "Epoch: [28] [  16/  20] time: 152.2875, loss: 0.00002918\n",
      "Epoch: [28] [  17/  20] time: 152.5309, loss: 0.00002655\n",
      "Epoch: [28] [  18/  20] time: 152.7697, loss: 0.00002097\n",
      "Epoch: [28] [  19/  20] time: 153.0094, loss: 0.00001039\n",
      "[28/50] - ptime: 5.0854 loss: 0.00017988 acc: 0.76000 lr: 0.00081000\n",
      "Epoch: [29] [   0/  20] time: 153.8900, loss: 0.00006068\n",
      "Epoch: [29] [   1/  20] time: 154.1286, loss: 0.00003070\n",
      "Epoch: [29] [   2/  20] time: 154.3685, loss: 0.00005325\n",
      "Epoch: [29] [   3/  20] time: 154.6096, loss: 0.00004145\n",
      "Epoch: [29] [   4/  20] time: 154.8507, loss: 0.00010689\n",
      "Epoch: [29] [   5/  20] time: 155.0904, loss: 0.00006047\n",
      "Epoch: [29] [   6/  20] time: 155.3310, loss: 0.00000289\n",
      "Epoch: [29] [   7/  20] time: 155.5710, loss: 0.00004239\n",
      "Epoch: [29] [   8/  20] time: 155.8132, loss: 0.00002765\n",
      "Epoch: [29] [   9/  20] time: 156.0531, loss: 0.00000498\n",
      "Epoch: [29] [  10/  20] time: 156.2944, loss: 0.00000014\n",
      "Epoch: [29] [  11/  20] time: 156.5353, loss: 0.00000535\n",
      "Epoch: [29] [  12/  20] time: 156.7772, loss: 0.00085205\n",
      "Epoch: [29] [  13/  20] time: 157.0168, loss: 0.00001168\n",
      "Epoch: [29] [  14/  20] time: 157.2567, loss: 0.00092448\n",
      "Epoch: [29] [  15/  20] time: 157.4988, loss: 0.00011546\n",
      "Epoch: [29] [  16/  20] time: 157.7385, loss: 0.00001653\n",
      "Epoch: [29] [  17/  20] time: 157.9791, loss: 0.00004524\n",
      "Epoch: [29] [  18/  20] time: 158.2187, loss: 0.00068757\n",
      "Epoch: [29] [  19/  20] time: 158.4598, loss: 0.00010168\n",
      "[29/50] - ptime: 5.0837 loss: 0.00015958 acc: 0.75000 lr: 0.00081000\n",
      "Epoch: [30] [   0/  20] time: 159.3373, loss: 0.00003366\n",
      "Epoch: [30] [   1/  20] time: 159.5758, loss: 0.00000202\n",
      "Epoch: [30] [   2/  20] time: 159.8168, loss: 0.00039336\n",
      "Epoch: [30] [   3/  20] time: 160.0565, loss: 0.00001131\n",
      "Epoch: [30] [   4/  20] time: 160.2956, loss: 0.00001012\n",
      "Epoch: [30] [   5/  20] time: 160.5352, loss: 0.00045183\n",
      "Epoch: [30] [   6/  20] time: 160.7756, loss: 0.00003258\n",
      "Epoch: [30] [   7/  20] time: 161.0138, loss: 0.00030604\n",
      "Epoch: [30] [   8/  20] time: 161.2520, loss: 0.00000688\n",
      "Epoch: [30] [   9/  20] time: 161.4946, loss: 0.00005387\n",
      "Epoch: [30] [  10/  20] time: 161.7338, loss: 0.00001894\n",
      "Epoch: [30] [  11/  20] time: 161.9745, loss: 0.00002948\n",
      "Epoch: [30] [  12/  20] time: 162.2143, loss: 0.00003989\n",
      "Epoch: [30] [  13/  20] time: 162.4549, loss: 0.00028205\n",
      "Epoch: [30] [  14/  20] time: 162.6960, loss: 0.00003903\n",
      "Epoch: [30] [  15/  20] time: 162.9373, loss: 0.00000337\n",
      "Epoch: [30] [  16/  20] time: 163.1778, loss: 0.00000394\n",
      "Epoch: [30] [  17/  20] time: 163.4182, loss: 0.00026596\n",
      "Epoch: [30] [  18/  20] time: 163.6560, loss: 0.00000005\n",
      "Epoch: [30] [  19/  20] time: 163.9124, loss: 0.00000409\n",
      "[30/50] - ptime: 5.0870 loss: 0.00009942 acc: 0.74000 lr: 0.00072900\n",
      "Epoch: [31] [   0/  20] time: 164.7620, loss: 0.00000732\n",
      "Epoch: [31] [   1/  20] time: 165.0008, loss: 0.00006352\n",
      "Epoch: [31] [   2/  20] time: 165.2402, loss: 0.00002965\n",
      "Epoch: [31] [   3/  20] time: 165.4829, loss: 0.00002217\n",
      "Epoch: [31] [   4/  20] time: 165.7228, loss: 0.00001757\n",
      "Epoch: [31] [   5/  20] time: 165.9638, loss: 0.00007920\n",
      "Epoch: [31] [   6/  20] time: 166.2053, loss: 0.00006805\n",
      "Epoch: [31] [   7/  20] time: 166.4463, loss: 0.00009777\n",
      "Epoch: [31] [   8/  20] time: 166.6864, loss: 0.00000051\n",
      "Epoch: [31] [   9/  20] time: 166.9265, loss: 0.00000036\n",
      "Epoch: [31] [  10/  20] time: 167.1668, loss: 0.00028868\n",
      "Epoch: [31] [  11/  20] time: 167.4073, loss: 0.00003443\n",
      "Epoch: [31] [  12/  20] time: 167.6473, loss: 0.00011095\n",
      "Epoch: [31] [  13/  20] time: 167.8892, loss: 0.00002961\n",
      "Epoch: [31] [  14/  20] time: 168.1301, loss: 0.00006245\n",
      "Epoch: [31] [  15/  20] time: 168.3707, loss: 0.00000450\n",
      "Epoch: [31] [  16/  20] time: 168.6116, loss: 0.00001689\n",
      "Epoch: [31] [  17/  20] time: 168.8558, loss: 0.00012244\n",
      "Epoch: [31] [  18/  20] time: 169.0949, loss: 0.00000441\n",
      "Epoch: [31] [  19/  20] time: 169.3352, loss: 0.00003812\n",
      "[31/50] - ptime: 5.0880 loss: 0.00005493 acc: 0.73000 lr: 0.00072900\n",
      "Epoch: [32] [   0/  20] time: 170.2750, loss: 0.00007751\n",
      "Epoch: [32] [   1/  20] time: 170.5147, loss: 0.00008663\n",
      "Epoch: [32] [   2/  20] time: 170.7573, loss: 0.00000357\n",
      "Epoch: [32] [   3/  20] time: 170.9964, loss: 0.00000198\n",
      "Epoch: [32] [   4/  20] time: 171.2361, loss: 0.00002156\n",
      "Epoch: [32] [   5/  20] time: 171.4769, loss: 0.00160980\n",
      "Epoch: [32] [   6/  20] time: 171.7168, loss: 0.00002012\n",
      "Epoch: [32] [   7/  20] time: 171.9574, loss: 0.00000651\n",
      "Epoch: [32] [   8/  20] time: 172.1969, loss: 0.00006120\n",
      "Epoch: [32] [   9/  20] time: 172.4374, loss: 0.00000187\n",
      "Epoch: [32] [  10/  20] time: 172.6762, loss: 0.00002430\n",
      "Epoch: [32] [  11/  20] time: 172.9173, loss: 0.00003736\n",
      "Epoch: [32] [  12/  20] time: 173.1566, loss: 0.00031516\n",
      "Epoch: [32] [  13/  20] time: 173.3968, loss: 0.00012111\n",
      "Epoch: [32] [  14/  20] time: 173.6386, loss: 0.00000137\n",
      "Epoch: [32] [  15/  20] time: 173.8787, loss: 0.00019797\n",
      "Epoch: [32] [  16/  20] time: 174.1176, loss: 0.00010313\n",
      "Epoch: [32] [  17/  20] time: 174.3563, loss: 0.00006449\n",
      "Epoch: [32] [  18/  20] time: 174.5949, loss: 0.00000345\n",
      "Epoch: [32] [  19/  20] time: 174.8349, loss: 0.00017281\n",
      "[32/50] - ptime: 5.0720 loss: 0.00014660 acc: 0.76000 lr: 0.00072900\n",
      "Epoch: [33] [   0/  20] time: 175.6852, loss: 0.00003838\n",
      "Epoch: [33] [   1/  20] time: 175.9243, loss: 0.00010316\n",
      "Epoch: [33] [   2/  20] time: 176.1633, loss: 0.00000348\n",
      "Epoch: [33] [   3/  20] time: 176.4014, loss: 0.00020960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33] [   4/  20] time: 176.6410, loss: 0.00007388\n",
      "Epoch: [33] [   5/  20] time: 176.8808, loss: 0.00005269\n",
      "Epoch: [33] [   6/  20] time: 177.1195, loss: 0.00000641\n",
      "Epoch: [33] [   7/  20] time: 177.3595, loss: 0.00000160\n",
      "Epoch: [33] [   8/  20] time: 177.6005, loss: 0.00004679\n",
      "Epoch: [33] [   9/  20] time: 177.8417, loss: 0.00000672\n",
      "Epoch: [33] [  10/  20] time: 178.0822, loss: 0.00004087\n",
      "Epoch: [33] [  11/  20] time: 178.3219, loss: 0.00000520\n",
      "Epoch: [33] [  12/  20] time: 178.5635, loss: 0.00014568\n",
      "Epoch: [33] [  13/  20] time: 178.8067, loss: 0.00000503\n",
      "Epoch: [33] [  14/  20] time: 179.0458, loss: 0.00006264\n",
      "Epoch: [33] [  15/  20] time: 179.2855, loss: 0.00000905\n",
      "Epoch: [33] [  16/  20] time: 179.5274, loss: 0.00006288\n",
      "Epoch: [33] [  17/  20] time: 179.7695, loss: 0.00024110\n",
      "Epoch: [33] [  18/  20] time: 180.0086, loss: 0.00000293\n",
      "Epoch: [33] [  19/  20] time: 180.2486, loss: 0.00012068\n",
      "[33/50] - ptime: 5.0760 loss: 0.00006194 acc: 0.74000 lr: 0.00072900\n",
      "Epoch: [34] [   0/  20] time: 181.1416, loss: 0.00000323\n",
      "Epoch: [34] [   1/  20] time: 181.3792, loss: 0.00000089\n",
      "Epoch: [34] [   2/  20] time: 181.6200, loss: 0.00003193\n",
      "Epoch: [34] [   3/  20] time: 181.8606, loss: 0.00020623\n",
      "Epoch: [34] [   4/  20] time: 182.0995, loss: 0.00001741\n",
      "Epoch: [34] [   5/  20] time: 182.3383, loss: 0.00000647\n",
      "Epoch: [34] [   6/  20] time: 182.5792, loss: 0.00000031\n",
      "Epoch: [34] [   7/  20] time: 182.8193, loss: 0.00084956\n",
      "Epoch: [34] [   8/  20] time: 183.0589, loss: 0.00008201\n",
      "Epoch: [34] [   9/  20] time: 183.2980, loss: 0.00000039\n",
      "Epoch: [34] [  10/  20] time: 183.5400, loss: 0.00004977\n",
      "Epoch: [34] [  11/  20] time: 183.7808, loss: 0.00001815\n",
      "Epoch: [34] [  12/  20] time: 184.0190, loss: 0.00000653\n",
      "Epoch: [34] [  13/  20] time: 184.2587, loss: 0.00015140\n",
      "Epoch: [34] [  14/  20] time: 184.4998, loss: 0.00000185\n",
      "Epoch: [34] [  15/  20] time: 184.7387, loss: 0.00000275\n",
      "Epoch: [34] [  16/  20] time: 184.9796, loss: 0.00005237\n",
      "Epoch: [34] [  17/  20] time: 185.2203, loss: 0.00000369\n",
      "Epoch: [34] [  18/  20] time: 185.4613, loss: 0.00010365\n",
      "Epoch: [34] [  19/  20] time: 185.7009, loss: 0.00022779\n",
      "[34/50] - ptime: 5.0722 loss: 0.00009082 acc: 0.73000 lr: 0.00072900\n",
      "Epoch: [35] [   0/  20] time: 186.5547, loss: 0.00000077\n",
      "Epoch: [35] [   1/  20] time: 186.7948, loss: 0.00000807\n",
      "Epoch: [35] [   2/  20] time: 187.0340, loss: 0.00004435\n",
      "Epoch: [35] [   3/  20] time: 187.2730, loss: 0.00001588\n",
      "Epoch: [35] [   4/  20] time: 187.5145, loss: 0.00009224\n",
      "Epoch: [35] [   5/  20] time: 187.7533, loss: 0.00006420\n",
      "Epoch: [35] [   6/  20] time: 187.9923, loss: 0.00000092\n",
      "Epoch: [35] [   7/  20] time: 188.2319, loss: 0.00001013\n",
      "Epoch: [35] [   8/  20] time: 188.4722, loss: 0.00025610\n",
      "Epoch: [35] [   9/  20] time: 188.7114, loss: 0.00001388\n",
      "Epoch: [35] [  10/  20] time: 188.9517, loss: 0.00199144\n",
      "Epoch: [35] [  11/  20] time: 189.1914, loss: 0.00018629\n",
      "Epoch: [35] [  12/  20] time: 189.4310, loss: 0.00006830\n",
      "Epoch: [35] [  13/  20] time: 189.6717, loss: 0.00002683\n",
      "Epoch: [35] [  14/  20] time: 189.9132, loss: 0.00030894\n",
      "Epoch: [35] [  15/  20] time: 190.1532, loss: 0.00006422\n",
      "Epoch: [35] [  16/  20] time: 190.3929, loss: 0.00003432\n",
      "Epoch: [35] [  17/  20] time: 190.6343, loss: 0.00006099\n",
      "Epoch: [35] [  18/  20] time: 190.8780, loss: 0.00000394\n",
      "Epoch: [35] [  19/  20] time: 191.1177, loss: 0.00000322\n",
      "[35/50] - ptime: 5.0768 loss: 0.00016275 acc: 0.71000 lr: 0.00072900\n",
      "Epoch: [36] [   0/  20] time: 191.9728, loss: 0.00000210\n",
      "Epoch: [36] [   1/  20] time: 192.2103, loss: 0.00006826\n",
      "Epoch: [36] [   2/  20] time: 192.4500, loss: 0.00001051\n",
      "Epoch: [36] [   3/  20] time: 192.6898, loss: 0.00001654\n",
      "Epoch: [36] [   4/  20] time: 192.9305, loss: 0.00000493\n",
      "Epoch: [36] [   5/  20] time: 193.1702, loss: 0.00021862\n",
      "Epoch: [36] [   6/  20] time: 193.4092, loss: 0.00000137\n",
      "Epoch: [36] [   7/  20] time: 193.6491, loss: 0.00022738\n",
      "Epoch: [36] [   8/  20] time: 193.8892, loss: 0.00001164\n",
      "Epoch: [36] [   9/  20] time: 194.1280, loss: 0.00002675\n",
      "Epoch: [36] [  10/  20] time: 194.3669, loss: 0.00002879\n",
      "Epoch: [36] [  11/  20] time: 194.6084, loss: 0.00000754\n",
      "Epoch: [36] [  12/  20] time: 194.8489, loss: 0.00002434\n",
      "Epoch: [36] [  13/  20] time: 195.0858, loss: 0.00002054\n",
      "Epoch: [36] [  14/  20] time: 195.3276, loss: 0.00000493\n",
      "Epoch: [36] [  15/  20] time: 195.5713, loss: 0.00000084\n",
      "Epoch: [36] [  16/  20] time: 195.8113, loss: 0.00007650\n",
      "Epoch: [36] [  17/  20] time: 196.0508, loss: 0.00027612\n",
      "Epoch: [36] [  18/  20] time: 196.2900, loss: 0.00002599\n",
      "Epoch: [36] [  19/  20] time: 196.5305, loss: 0.00000135\n",
      "[36/50] - ptime: 5.0723 loss: 0.00005275 acc: 0.71000 lr: 0.00072900\n",
      "Epoch: [37] [   0/  20] time: 197.3798, loss: 0.00004383\n",
      "Epoch: [37] [   1/  20] time: 197.6181, loss: 0.00005589\n",
      "Epoch: [37] [   2/  20] time: 197.8584, loss: 0.00001275\n",
      "Epoch: [37] [   3/  20] time: 198.0969, loss: 0.00000913\n",
      "Epoch: [37] [   4/  20] time: 198.3363, loss: 0.00000550\n",
      "Epoch: [37] [   5/  20] time: 198.5767, loss: 0.00000225\n",
      "Epoch: [37] [   6/  20] time: 198.8170, loss: 0.00005530\n",
      "Epoch: [37] [   7/  20] time: 199.0563, loss: 0.00000167\n",
      "Epoch: [37] [   8/  20] time: 199.2964, loss: 0.00000500\n",
      "Epoch: [37] [   9/  20] time: 199.5363, loss: 0.00002330\n",
      "Epoch: [37] [  10/  20] time: 199.7770, loss: 0.00000196\n",
      "Epoch: [37] [  11/  20] time: 200.0173, loss: 0.00007649\n",
      "Epoch: [37] [  12/  20] time: 200.2564, loss: 0.00000057\n",
      "Epoch: [37] [  13/  20] time: 200.4969, loss: 0.00000677\n",
      "Epoch: [37] [  14/  20] time: 200.7368, loss: 0.00001256\n",
      "Epoch: [37] [  15/  20] time: 200.9770, loss: 0.00001570\n",
      "Epoch: [37] [  16/  20] time: 201.2169, loss: 0.00006028\n",
      "Epoch: [37] [  17/  20] time: 201.4562, loss: 0.00000208\n",
      "Epoch: [37] [  18/  20] time: 201.6964, loss: 0.00000865\n",
      "Epoch: [37] [  19/  20] time: 201.9370, loss: 0.00002983\n",
      "[37/50] - ptime: 5.0674 loss: 0.00002148 acc: 0.71000 lr: 0.00072900\n",
      "Epoch: [38] [   0/  20] time: 202.7969, loss: 0.00000054\n",
      "Epoch: [38] [   1/  20] time: 203.0343, loss: 0.00007660\n",
      "Epoch: [38] [   2/  20] time: 203.2765, loss: 0.00000014\n",
      "Epoch: [38] [   3/  20] time: 203.5301, loss: 0.00002501\n",
      "Epoch: [38] [   4/  20] time: 203.7841, loss: 0.00000946\n",
      "Epoch: [38] [   5/  20] time: 204.0232, loss: 0.00004375\n",
      "Epoch: [38] [   6/  20] time: 204.2634, loss: 0.00001971\n",
      "Epoch: [38] [   7/  20] time: 204.5041, loss: 0.00001363\n",
      "Epoch: [38] [   8/  20] time: 204.7431, loss: 0.00000345\n",
      "Epoch: [38] [   9/  20] time: 204.9830, loss: 0.00000446\n",
      "Epoch: [38] [  10/  20] time: 205.2225, loss: 0.00001765\n",
      "Epoch: [38] [  11/  20] time: 205.4674, loss: 0.00000776\n",
      "Epoch: [38] [  12/  20] time: 205.7114, loss: 0.00005801\n",
      "Epoch: [38] [  13/  20] time: 205.9521, loss: 0.00002423\n",
      "Epoch: [38] [  14/  20] time: 206.1916, loss: 0.00001756\n",
      "Epoch: [38] [  15/  20] time: 206.4314, loss: 0.00001825\n",
      "Epoch: [38] [  16/  20] time: 206.6716, loss: 0.00000586\n",
      "Epoch: [38] [  17/  20] time: 206.9113, loss: 0.00004474\n",
      "Epoch: [38] [  18/  20] time: 207.1523, loss: 0.00002008\n",
      "Epoch: [38] [  19/  20] time: 207.3922, loss: 0.00019029\n",
      "[38/50] - ptime: 5.1107 loss: 0.00003006 acc: 0.71000 lr: 0.00072900\n",
      "Epoch: [39] [   0/  20] time: 208.2506, loss: 0.00000020\n",
      "Epoch: [39] [   1/  20] time: 208.4882, loss: 0.00008205\n",
      "Epoch: [39] [   2/  20] time: 208.7272, loss: 0.00000123\n",
      "Epoch: [39] [   3/  20] time: 208.9675, loss: 0.00002615\n",
      "Epoch: [39] [   4/  20] time: 209.2080, loss: 0.00001434\n",
      "Epoch: [39] [   5/  20] time: 209.4473, loss: 0.00001834\n",
      "Epoch: [39] [   6/  20] time: 209.6879, loss: 0.00002547\n",
      "Epoch: [39] [   7/  20] time: 209.9291, loss: 0.00000109\n",
      "Epoch: [39] [   8/  20] time: 210.1699, loss: 0.00000028\n",
      "Epoch: [39] [   9/  20] time: 210.4106, loss: 0.00000011\n",
      "Epoch: [39] [  10/  20] time: 210.6519, loss: 0.00000182\n",
      "Epoch: [39] [  11/  20] time: 210.8949, loss: 0.00001572\n",
      "Epoch: [39] [  12/  20] time: 211.1338, loss: 0.00010082\n",
      "Epoch: [39] [  13/  20] time: 211.3738, loss: 0.00005420\n",
      "Epoch: [39] [  14/  20] time: 211.6148, loss: 0.00001115\n",
      "Epoch: [39] [  15/  20] time: 211.8557, loss: 0.00000378\n",
      "Epoch: [39] [  16/  20] time: 212.0957, loss: 0.00000223\n",
      "Epoch: [39] [  17/  20] time: 212.3360, loss: 0.00000297\n",
      "Epoch: [39] [  18/  20] time: 212.5793, loss: 0.00000069\n",
      "Epoch: [39] [  19/  20] time: 212.8235, loss: 0.00004967\n",
      "[39/50] - ptime: 5.0864 loss: 0.00002062 acc: 0.71000 lr: 0.00072900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40] [   0/  20] time: 213.6792, loss: 0.00000152\n",
      "Epoch: [40] [   1/  20] time: 213.9198, loss: 0.00000344\n",
      "Epoch: [40] [   2/  20] time: 214.1590, loss: 0.00000032\n",
      "Epoch: [40] [   3/  20] time: 214.3996, loss: 0.00000471\n",
      "Epoch: [40] [   4/  20] time: 214.6394, loss: 0.00007722\n",
      "Epoch: [40] [   5/  20] time: 214.8793, loss: 0.00003841\n",
      "Epoch: [40] [   6/  20] time: 215.1200, loss: 0.00000040\n",
      "Epoch: [40] [   7/  20] time: 215.3593, loss: 0.00002287\n",
      "Epoch: [40] [   8/  20] time: 215.6014, loss: 0.00000837\n",
      "Epoch: [40] [   9/  20] time: 215.8420, loss: 0.00003255\n",
      "Epoch: [40] [  10/  20] time: 216.0822, loss: 0.00000116\n",
      "Epoch: [40] [  11/  20] time: 216.3217, loss: 0.00000026\n",
      "Epoch: [40] [  12/  20] time: 216.5622, loss: 0.00002153\n",
      "Epoch: [40] [  13/  20] time: 216.8078, loss: 0.00001843\n",
      "Epoch: [40] [  14/  20] time: 217.0482, loss: 0.00007363\n",
      "Epoch: [40] [  15/  20] time: 217.2882, loss: 0.00003273\n",
      "Epoch: [40] [  16/  20] time: 217.5290, loss: 0.00010068\n",
      "Epoch: [40] [  17/  20] time: 217.7696, loss: 0.00530604\n",
      "Epoch: [40] [  18/  20] time: 218.0099, loss: 0.00002990\n",
      "Epoch: [40] [  19/  20] time: 218.2503, loss: 0.00039327\n",
      "[40/50] - ptime: 5.0838 loss: 0.00030837 acc: 0.80000 lr: 0.00065610\n",
      "Epoch: [41] [   0/  20] time: 219.1091, loss: 0.00001622\n",
      "Epoch: [41] [   1/  20] time: 219.3466, loss: 0.00069365\n",
      "Epoch: [41] [   2/  20] time: 219.5871, loss: 0.01310268\n",
      "Epoch: [41] [   3/  20] time: 219.8275, loss: 0.00044106\n",
      "Epoch: [41] [   4/  20] time: 220.0666, loss: 0.00000234\n",
      "Epoch: [41] [   5/  20] time: 220.3052, loss: 0.02404718\n",
      "Epoch: [41] [   6/  20] time: 220.5460, loss: 0.00048005\n",
      "Epoch: [41] [   7/  20] time: 220.7862, loss: 0.00010831\n",
      "Epoch: [41] [   8/  20] time: 221.0262, loss: 0.00000210\n",
      "Epoch: [41] [   9/  20] time: 221.2666, loss: 0.00000279\n",
      "Epoch: [41] [  10/  20] time: 221.5059, loss: 0.00242472\n",
      "Epoch: [41] [  11/  20] time: 221.7461, loss: 0.00028982\n",
      "Epoch: [41] [  12/  20] time: 221.9863, loss: 0.00281136\n",
      "Epoch: [41] [  13/  20] time: 222.2261, loss: 0.00184237\n",
      "Epoch: [41] [  14/  20] time: 222.4681, loss: 0.00000737\n",
      "Epoch: [41] [  15/  20] time: 222.7091, loss: 0.00640840\n",
      "Epoch: [41] [  16/  20] time: 222.9517, loss: 0.00145311\n",
      "Epoch: [41] [  17/  20] time: 223.1930, loss: 0.00000719\n",
      "Epoch: [41] [  18/  20] time: 223.4336, loss: 0.00010269\n",
      "Epoch: [41] [  19/  20] time: 223.6757, loss: 0.00541905\n",
      "[41/50] - ptime: 5.0804 loss: 0.00298312 acc: 0.75000 lr: 0.00065610\n",
      "Epoch: [42] [   0/  20] time: 224.5616, loss: 0.00107989\n",
      "Epoch: [42] [   1/  20] time: 224.8005, loss: 0.00204402\n",
      "Epoch: [42] [   2/  20] time: 225.0396, loss: 0.00000456\n",
      "Epoch: [42] [   3/  20] time: 225.2802, loss: 0.00000084\n",
      "Epoch: [42] [   4/  20] time: 225.5204, loss: 0.00034385\n",
      "Epoch: [42] [   5/  20] time: 225.7605, loss: 0.00002914\n",
      "Epoch: [42] [   6/  20] time: 226.0000, loss: 0.00001029\n",
      "Epoch: [42] [   7/  20] time: 226.2410, loss: 0.00003841\n",
      "Epoch: [42] [   8/  20] time: 226.4818, loss: 0.00006944\n",
      "Epoch: [42] [   9/  20] time: 226.7229, loss: 0.00211873\n",
      "Epoch: [42] [  10/  20] time: 226.9644, loss: 0.00002555\n",
      "Epoch: [42] [  11/  20] time: 227.2065, loss: 0.00218406\n",
      "Epoch: [42] [  12/  20] time: 227.4462, loss: 0.00009841\n",
      "Epoch: [42] [  13/  20] time: 227.6881, loss: 0.00006852\n",
      "Epoch: [42] [  14/  20] time: 227.9290, loss: 0.00014817\n",
      "Epoch: [42] [  15/  20] time: 228.1688, loss: 0.00053729\n",
      "Epoch: [42] [  16/  20] time: 228.4080, loss: 0.00114829\n",
      "Epoch: [42] [  17/  20] time: 228.6515, loss: 0.00000296\n",
      "Epoch: [42] [  18/  20] time: 228.8915, loss: 0.00006899\n",
      "Epoch: [42] [  19/  20] time: 229.1310, loss: 0.00021501\n",
      "[42/50] - ptime: 5.0851 loss: 0.00051182 acc: 0.74000 lr: 0.00065610\n",
      "Epoch: [43] [   0/  20] time: 229.9903, loss: 0.00000026\n",
      "Epoch: [43] [   1/  20] time: 230.2275, loss: 0.00014921\n",
      "Epoch: [43] [   2/  20] time: 230.4671, loss: 0.00000490\n",
      "Epoch: [43] [   3/  20] time: 230.7071, loss: 0.00008401\n",
      "Epoch: [43] [   4/  20] time: 230.9494, loss: 0.00026852\n",
      "Epoch: [43] [   5/  20] time: 231.1888, loss: 0.00000524\n",
      "Epoch: [43] [   6/  20] time: 231.4281, loss: 0.00001313\n",
      "Epoch: [43] [   7/  20] time: 231.6701, loss: 0.00001295\n",
      "Epoch: [43] [   8/  20] time: 231.9101, loss: 0.00006001\n",
      "Epoch: [43] [   9/  20] time: 232.1497, loss: 0.00007339\n",
      "Epoch: [43] [  10/  20] time: 232.3885, loss: 0.00116135\n",
      "Epoch: [43] [  11/  20] time: 232.6298, loss: 0.00019394\n",
      "Epoch: [43] [  12/  20] time: 232.8702, loss: 0.00002228\n",
      "Epoch: [43] [  13/  20] time: 233.1090, loss: 0.00001515\n",
      "Epoch: [43] [  14/  20] time: 233.3481, loss: 0.00014382\n",
      "Epoch: [43] [  15/  20] time: 233.5877, loss: 0.00000732\n",
      "Epoch: [43] [  16/  20] time: 233.8270, loss: 0.00004683\n",
      "Epoch: [43] [  17/  20] time: 234.0670, loss: 0.00013160\n",
      "Epoch: [43] [  18/  20] time: 234.3064, loss: 0.00000740\n",
      "Epoch: [43] [  19/  20] time: 234.5471, loss: 0.00000034\n",
      "[43/50] - ptime: 5.0708 loss: 0.00012008 acc: 0.75000 lr: 0.00065610\n",
      "Epoch: [44] [   0/  20] time: 235.3907, loss: 0.00001514\n",
      "Epoch: [44] [   1/  20] time: 235.6305, loss: 0.00004066\n",
      "Epoch: [44] [   2/  20] time: 235.8727, loss: 0.00000742\n",
      "Epoch: [44] [   3/  20] time: 236.1123, loss: 0.00002364\n",
      "Epoch: [44] [   4/  20] time: 236.3529, loss: 0.00017455\n",
      "Epoch: [44] [   5/  20] time: 236.5937, loss: 0.00003296\n",
      "Epoch: [44] [   6/  20] time: 236.8362, loss: 0.00003004\n",
      "Epoch: [44] [   7/  20] time: 237.0752, loss: 0.00072265\n",
      "Epoch: [44] [   8/  20] time: 237.3150, loss: 0.00004861\n",
      "Epoch: [44] [   9/  20] time: 237.5556, loss: 0.00000406\n",
      "Epoch: [44] [  10/  20] time: 237.7970, loss: 0.00010993\n",
      "Epoch: [44] [  11/  20] time: 238.0373, loss: 0.00001433\n",
      "Epoch: [44] [  12/  20] time: 238.2766, loss: 0.00004857\n",
      "Epoch: [44] [  13/  20] time: 238.5172, loss: 0.00007422\n",
      "Epoch: [44] [  14/  20] time: 238.7575, loss: 0.00001846\n",
      "Epoch: [44] [  15/  20] time: 238.9973, loss: 0.00000235\n",
      "Epoch: [44] [  16/  20] time: 239.2370, loss: 0.00001840\n",
      "Epoch: [44] [  17/  20] time: 239.4767, loss: 0.00018323\n",
      "Epoch: [44] [  18/  20] time: 239.7166, loss: 0.00000592\n",
      "Epoch: [44] [  19/  20] time: 239.9576, loss: 0.00005878\n",
      "[44/50] - ptime: 5.0782 loss: 0.00008169 acc: 0.74000 lr: 0.00065610\n",
      "Epoch: [45] [   0/  20] time: 240.8706, loss: 0.00029745\n",
      "Epoch: [45] [   1/  20] time: 241.1082, loss: 0.00000468\n",
      "Epoch: [45] [   2/  20] time: 241.3481, loss: 0.00002876\n",
      "Epoch: [45] [   3/  20] time: 241.5876, loss: 0.00020711\n",
      "Epoch: [45] [   4/  20] time: 241.8289, loss: 0.00000374\n",
      "Epoch: [45] [   5/  20] time: 242.0682, loss: 0.00003908\n",
      "Epoch: [45] [   6/  20] time: 242.3078, loss: 0.00000005\n",
      "Epoch: [45] [   7/  20] time: 242.5505, loss: 0.00000031\n",
      "Epoch: [45] [   8/  20] time: 242.7911, loss: 0.00010682\n",
      "Epoch: [45] [   9/  20] time: 243.0317, loss: 0.00021004\n",
      "Epoch: [45] [  10/  20] time: 243.2704, loss: 0.00004443\n",
      "Epoch: [45] [  11/  20] time: 243.5103, loss: 0.00000887\n",
      "Epoch: [45] [  12/  20] time: 243.7527, loss: 0.00000192\n",
      "Epoch: [45] [  13/  20] time: 243.9924, loss: 0.00340109\n",
      "Epoch: [45] [  14/  20] time: 244.2330, loss: 0.00029932\n",
      "Epoch: [45] [  15/  20] time: 244.4719, loss: 0.00000125\n",
      "Epoch: [45] [  16/  20] time: 244.7134, loss: 0.00014873\n",
      "Epoch: [45] [  17/  20] time: 244.9537, loss: 0.00003986\n",
      "Epoch: [45] [  18/  20] time: 245.1913, loss: 0.00000075\n",
      "Epoch: [45] [  19/  20] time: 245.4309, loss: 0.00006710\n",
      "[45/50] - ptime: 5.0798 loss: 0.00024557 acc: 0.74000 lr: 0.00065610\n",
      "Epoch: [46] [   0/  20] time: 246.2877, loss: 0.00058018\n",
      "Epoch: [46] [   1/  20] time: 246.5251, loss: 0.00001938\n",
      "Epoch: [46] [   2/  20] time: 246.7652, loss: 0.00004216\n",
      "Epoch: [46] [   3/  20] time: 247.0037, loss: 0.00022307\n",
      "Epoch: [46] [   4/  20] time: 247.2431, loss: 0.00007810\n",
      "Epoch: [46] [   5/  20] time: 247.4830, loss: 0.00221946\n",
      "Epoch: [46] [   6/  20] time: 247.7250, loss: 0.00000751\n",
      "Epoch: [46] [   7/  20] time: 247.9662, loss: 0.00002349\n",
      "Epoch: [46] [   8/  20] time: 248.2060, loss: 0.00004804\n",
      "Epoch: [46] [   9/  20] time: 248.4462, loss: 0.00001830\n",
      "Epoch: [46] [  10/  20] time: 248.6879, loss: 0.00002379\n",
      "Epoch: [46] [  11/  20] time: 248.9291, loss: 0.00002480\n",
      "Epoch: [46] [  12/  20] time: 249.1693, loss: 0.00000348\n",
      "Epoch: [46] [  13/  20] time: 249.4099, loss: 0.00000075\n",
      "Epoch: [46] [  14/  20] time: 249.6512, loss: 0.00004433\n",
      "Epoch: [46] [  15/  20] time: 249.8919, loss: 0.00000843\n",
      "Epoch: [46] [  16/  20] time: 250.1311, loss: 0.00001016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46] [  17/  20] time: 250.3726, loss: 0.00044303\n",
      "Epoch: [46] [  18/  20] time: 250.6126, loss: 0.00000109\n",
      "Epoch: [46] [  19/  20] time: 250.8554, loss: 0.00000036\n",
      "[46/50] - ptime: 5.0801 loss: 0.00019100 acc: 0.73000 lr: 0.00065610\n",
      "Epoch: [47] [   0/  20] time: 251.7035, loss: 0.00000336\n",
      "Epoch: [47] [   1/  20] time: 251.9431, loss: 0.00003104\n",
      "Epoch: [47] [   2/  20] time: 252.1822, loss: 0.00001196\n",
      "Epoch: [47] [   3/  20] time: 252.4221, loss: 0.00000760\n",
      "Epoch: [47] [   4/  20] time: 252.6629, loss: 0.00000012\n",
      "Epoch: [47] [   5/  20] time: 252.9037, loss: 0.00000364\n",
      "Epoch: [47] [   6/  20] time: 253.1454, loss: 0.00012064\n",
      "Epoch: [47] [   7/  20] time: 253.3866, loss: 0.00002673\n",
      "Epoch: [47] [   8/  20] time: 253.6292, loss: 0.00135229\n",
      "Epoch: [47] [   9/  20] time: 253.8696, loss: 0.00000271\n",
      "Epoch: [47] [  10/  20] time: 254.1077, loss: 0.00019925\n",
      "Epoch: [47] [  11/  20] time: 254.3486, loss: 0.00001152\n",
      "Epoch: [47] [  12/  20] time: 254.5901, loss: 0.00000060\n",
      "Epoch: [47] [  13/  20] time: 254.8333, loss: 0.00004120\n",
      "Epoch: [47] [  14/  20] time: 255.0719, loss: 0.00004804\n",
      "Epoch: [47] [  15/  20] time: 255.3108, loss: 0.00001410\n",
      "Epoch: [47] [  16/  20] time: 255.5511, loss: 0.00000870\n",
      "Epoch: [47] [  17/  20] time: 255.7914, loss: 0.00006080\n",
      "Epoch: [47] [  18/  20] time: 256.0306, loss: 0.00000165\n",
      "Epoch: [47] [  19/  20] time: 256.2710, loss: 0.00005405\n",
      "[47/50] - ptime: 5.0801 loss: 0.00010000 acc: 0.71000 lr: 0.00065610\n",
      "Epoch: [48] [   0/  20] time: 257.1179, loss: 0.00014965\n",
      "Epoch: [48] [   1/  20] time: 257.3555, loss: 0.00001218\n",
      "Epoch: [48] [   2/  20] time: 257.5974, loss: 0.00002067\n",
      "Epoch: [48] [   3/  20] time: 257.8385, loss: 0.00001622\n",
      "Epoch: [48] [   4/  20] time: 258.0785, loss: 0.00000480\n",
      "Epoch: [48] [   5/  20] time: 258.3176, loss: 0.00002001\n",
      "Epoch: [48] [   6/  20] time: 258.5580, loss: 0.00001439\n",
      "Epoch: [48] [   7/  20] time: 258.7995, loss: 0.00001290\n",
      "Epoch: [48] [   8/  20] time: 259.0391, loss: 0.00001026\n",
      "Epoch: [48] [   9/  20] time: 259.2801, loss: 0.00000198\n",
      "Epoch: [48] [  10/  20] time: 259.5199, loss: 0.00000046\n",
      "Epoch: [48] [  11/  20] time: 259.7605, loss: 0.00001421\n",
      "Epoch: [48] [  12/  20] time: 259.9998, loss: 0.00001008\n",
      "Epoch: [48] [  13/  20] time: 260.2402, loss: 0.00039303\n",
      "Epoch: [48] [  14/  20] time: 260.4814, loss: 0.00043751\n",
      "Epoch: [48] [  15/  20] time: 260.7236, loss: 0.00000626\n",
      "Epoch: [48] [  16/  20] time: 260.9664, loss: 0.00000329\n",
      "Epoch: [48] [  17/  20] time: 261.2061, loss: 0.00000579\n",
      "Epoch: [48] [  18/  20] time: 261.4468, loss: 0.00001966\n",
      "Epoch: [48] [  19/  20] time: 261.6888, loss: 0.00007327\n",
      "[48/50] - ptime: 5.0837 loss: 0.00006133 acc: 0.71000 lr: 0.00065610\n",
      "Epoch: [49] [   0/  20] time: 262.6489, loss: 0.00000005\n",
      "Epoch: [49] [   1/  20] time: 262.8873, loss: 0.00003066\n",
      "Epoch: [49] [   2/  20] time: 263.1273, loss: 0.00000675\n",
      "Epoch: [49] [   3/  20] time: 263.3665, loss: 0.00015066\n",
      "Epoch: [49] [   4/  20] time: 263.6072, loss: 0.00000117\n",
      "Epoch: [49] [   5/  20] time: 263.8481, loss: 0.00003761\n",
      "Epoch: [49] [   6/  20] time: 264.0874, loss: 0.00005608\n",
      "Epoch: [49] [   7/  20] time: 264.3273, loss: 0.00002014\n",
      "Epoch: [49] [   8/  20] time: 264.5675, loss: 0.00003444\n",
      "Epoch: [49] [   9/  20] time: 264.8101, loss: 0.00003668\n",
      "Epoch: [49] [  10/  20] time: 265.0498, loss: 0.00007772\n",
      "Epoch: [49] [  11/  20] time: 265.2892, loss: 0.00000080\n",
      "Epoch: [49] [  12/  20] time: 265.5286, loss: 0.00004381\n",
      "Epoch: [49] [  13/  20] time: 265.7688, loss: 0.00004953\n",
      "Epoch: [49] [  14/  20] time: 266.0069, loss: 0.00002120\n",
      "Epoch: [49] [  15/  20] time: 266.2457, loss: 0.00000191\n",
      "Epoch: [49] [  16/  20] time: 266.4875, loss: 0.00000363\n",
      "Epoch: [49] [  17/  20] time: 266.7293, loss: 0.00003676\n",
      "Epoch: [49] [  18/  20] time: 266.9683, loss: 0.00000509\n",
      "Epoch: [49] [  19/  20] time: 267.2070, loss: 0.00005865\n",
      "[49/50] - ptime: 5.0720 loss: 0.00003367 acc: 0.71000 lr: 0.00065610\n",
      "Epoch: [50] [   0/  20] time: 268.0612, loss: 0.00001666\n",
      "Epoch: [50] [   1/  20] time: 268.2988, loss: 0.00000053\n",
      "Epoch: [50] [   2/  20] time: 268.5378, loss: 0.00000534\n",
      "Epoch: [50] [   3/  20] time: 268.7782, loss: 0.00000072\n",
      "Epoch: [50] [   4/  20] time: 269.0174, loss: 0.00000097\n",
      "Epoch: [50] [   5/  20] time: 269.2564, loss: 0.00005462\n",
      "Epoch: [50] [   6/  20] time: 269.4957, loss: 0.00000199\n",
      "Epoch: [50] [   7/  20] time: 269.7422, loss: 0.00003540\n",
      "Epoch: [50] [   8/  20] time: 269.9812, loss: 0.00000198\n",
      "Epoch: [50] [   9/  20] time: 270.2203, loss: 0.00000256\n",
      "Epoch: [50] [  10/  20] time: 270.4589, loss: 0.00002448\n",
      "Epoch: [50] [  11/  20] time: 270.6993, loss: 0.00000843\n",
      "Epoch: [50] [  12/  20] time: 270.9406, loss: 0.00000141\n",
      "Epoch: [50] [  13/  20] time: 271.1833, loss: 0.00000326\n",
      "Epoch: [50] [  14/  20] time: 271.4217, loss: 0.00002131\n",
      "Epoch: [50] [  15/  20] time: 271.6628, loss: 0.00011697\n",
      "Epoch: [50] [  16/  20] time: 271.9033, loss: 0.00000017\n",
      "Epoch: [50] [  17/  20] time: 272.1431, loss: 0.00028192\n",
      "Epoch: [50] [  18/  20] time: 272.3825, loss: 0.00001133\n",
      "Epoch: [50] [  19/  20] time: 272.6253, loss: 0.00000413\n",
      "[50/50] - ptime: 5.0758 loss: 0.00002971 acc: 0.71000 lr: 0.00059049\n",
      "Avg per epoch ptime: 5.11, total 50 epochs ptime: 273.02\n",
      " [*] Training finished!\n",
      " [*] Best Epoch:  8 , Accuracy:  0.8400000333786011\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/CNN_Pyramid_L_C5_D1_Kernel(3,3)_128_lrdecay/CNN_Pyramid_L_C5_D1_Kernel(3,3)_128_lrdecay-8\n",
      " [*] Finished testing Best Epoch: 8 , accuracy:  0.8400000333786011 !\n"
     ]
    }
   ],
   "source": [
    "dataset = '4_Flowers_1s'\n",
    "epoch = 50\n",
    "batch_size = 100\n",
    "checkpoint_dir = 'checkpoint'\n",
    "log_dir = 'logs'\n",
    "trainhist_dir = 'trainhist'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "# --log_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# --trainhist_dir\n",
    "if not os.path.exists(trainhist_dir):\n",
    "    os.makedirs(trainhist_dir)\n",
    "\n",
    "# open session\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    \n",
    "    # declare instance for GAN\n",
    "    CNN = CNN(sess, epoch=epoch, batch_size=batch_size, dataset_name=dataset, checkpoint_dir=checkpoint_dir, \n",
    "                log_dir=log_dir, trainhist_dir=trainhist_dir)\n",
    "\n",
    "    # build graph\n",
    "    CNN.build_model()\n",
    "\n",
    "    # show network architecture\n",
    "    CNN.show_all_variables()\n",
    "\n",
    "    # launch the graph in a session\n",
    "    CNN.train()\n",
    "    \n",
    "#     CNN.test(epoch)\n",
    "        \n",
    "sess.close()\n",
    "        \n",
    "# lrdecay\n",
    "# [*] Best Epoch:  9 , Accuracy:  0.8199999928474426\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_Pyramid_L_C5_D1_Kernel(3,3)_128_lrdecay/CNN_Pyramid_L_C5_D1_Kernel(3,3)_128_lrdecay-9\n",
    "#  [*] Finished testing Best Epoch: 9 , accuracy:  0.8199999928474426 !\n",
    "\n",
    "# [50/50] - ptime: 5.0758 loss: 0.00002971 acc: 0.71000 lr: 0.00059049\n",
    "# Avg per epoch ptime: 5.11, total 50 epochs ptime: 273.02\n",
    "#  [*] Training finished!\n",
    "#  [*] Best Epoch:  8 , Accuracy:  0.8400000333786011\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_Pyramid_L_C5_D1_Kernel(3,3)_128_lrdecay/CNN_Pyramid_L_C5_D1_Kernel(3,3)_128_lrdecay-8\n",
    "#  [*] Finished testing Best Epoch: 8 , accuracy:  0.8400000333786011 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "file='/home/huiqy/Music/CloudMusic/All_Time_Low.mp3' #文件名是完整路径名\n",
    "pygame.mixer.init() #初始化音频\n",
    "track = pygame.mixer.music.load(file)#载入音乐文件\n",
    "pygame.mixer.music.play()#开始播放\n",
    "time.sleep(60)#播放10秒\n",
    "pygame.mixer.music.stop()#停止播放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
