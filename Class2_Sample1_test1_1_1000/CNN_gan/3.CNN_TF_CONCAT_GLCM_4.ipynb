{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imutils import paths\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import argparse\n",
    "import imutils,sklearn\n",
    "import os, cv2, re, random, shutil, imageio, pickle\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #灰度共生矩阵\n",
    "# #定义最大灰度级数\n",
    "# gray_level = 16\n",
    "\n",
    "# def maxGrayLevel(img):\n",
    "#     max_gray_level=0\n",
    "#     (height,width)=img.shape\n",
    "#     for y in range(height):\n",
    "#         for x in range(width):\n",
    "#             if img[y][x] > max_gray_level:\n",
    "#                 max_gray_level = img[y][x]\n",
    "#     return max_gray_level+1\n",
    "\n",
    "# def getGlcm(img,d_x,d_y):\n",
    "#     srcdata=img.copy()\n",
    "#     ret=[[0.0 for i in range(gray_level)] for j in range(gray_level)]\n",
    "#     (height,width) = img.shape\n",
    "\n",
    "#     max_gray_level=maxGrayLevel(img)\n",
    "\n",
    "#     #若灰度级数大于gray_level，则将图像的灰度级缩小至gray_level，减小灰度共生矩阵的大小\n",
    "#     if max_gray_level > gray_level:\n",
    "#         for j in range(height):\n",
    "#             for i in range(width):\n",
    "#                 srcdata[j][i] = srcdata[j][i]*gray_level / max_gray_level\n",
    "\n",
    "#     for j in range(height-d_y):\n",
    "#         for i in range(width-d_x):\n",
    "#             rows = srcdata[j][i]\n",
    "#             cols = srcdata[j + d_y][i+d_x]\n",
    "#             ret[rows][cols]+=1.0\n",
    "\n",
    "#     for i in range(gray_level):\n",
    "#         for j in range(gray_level):\n",
    "#             ret[i][j]/=float(height*width)\n",
    "\n",
    "#     return np.array(ret)\n",
    "    \n",
    "# def feature_computer(p):\n",
    "#     Con=0.0\n",
    "#     Eng=0.0\n",
    "#     Asm=0.0\n",
    "#     Idm=0.0\n",
    "#     for i in range(gray_level):\n",
    "#         for j in range(gray_level):\n",
    "#             Con+=(i-j)*(i-j)*p[i][j]\n",
    "#             Asm+=p[i][j]*p[i][j]\n",
    "#             Idm+=p[i][j]/(1+(i-j)*(i-j))\n",
    "#             if p[i][j]>0.0:\n",
    "#                 Eng+=p[i][j]*math.log(p[i][j])\n",
    "#     return Asm,Con,-Eng,Idm\n",
    "\n",
    "# def batch_GLCM(images):\n",
    "#     greycomatrix_list_4 = []\n",
    "#     for i in tqdm(range(len(images))):\n",
    "#         img = (images.astype(np.float32)* 255)[i, :, :, :].astype(np.uint8)\n",
    "#         gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "#         glcm_0=getGlcm(gray, 1,0)\n",
    "#         asm0,con0,eng0,idm0 = feature_computer(glcm_0)\n",
    "        \n",
    "#         glcm_1=getGlcm(gray, 0,1)\n",
    "#         asm1,con1,eng1,idm1 = feature_computer(glcm_1)\n",
    "        \n",
    "#         glcm_2=getGlcm(gray, 1,1)\n",
    "#         asm2,con2,eng2,idm2 = feature_computer(glcm_2)\n",
    "\n",
    "#         greycomatrix_list_4.append([asm0,con0,eng0,idm0,\n",
    "#                                   asm1,con1,eng1,idm1,\n",
    "#                                   asm2,con2,eng2,idm2])\n",
    "        \n",
    "#     greycomatrix_list_4 = np.array(greycomatrix_list_4,dtype = float32)\n",
    "    \n",
    "#     print (greycomatrix_list_4.shape)\n",
    "#     return greycomatrix_list_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def atoi(text):\n",
    "#     return int(text) if text.isdigit() else text\n",
    "\n",
    "# def natural_keys(text):\n",
    "#     return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# def load_flower_data():\n",
    "#     # grab the list of images that we'll be describing\n",
    "#     print(\"[INFO] handling images...\")\n",
    "#     TRAIN_ORIGINAL_DIR = '../train/'\n",
    "#     TRAIN_SUB_DIR = '../subsample/'\n",
    "#     TRAIN_GAN = '../../image_gan/'\n",
    "#     TEST_DIR = '../../test/'\n",
    "\n",
    "#     # use this for full dataset\n",
    "#     train_images_gan = [TRAIN_GAN + i for i in os.listdir(TRAIN_GAN)]\n",
    "#     test_images = [TEST_DIR + i for i in os.listdir(TEST_DIR)]\n",
    "    \n",
    "#     train_images = train_images_gan\n",
    "    \n",
    "#     train_images.sort(key=natural_keys)\n",
    "#     test_images.sort(key=natural_keys)\n",
    "\n",
    "#     # initialize the features matrix and labels list\n",
    "#     trainImage = []\n",
    "#     trainLabels = []\n",
    "#     testImage = []\n",
    "#     testLabels = []\n",
    "\n",
    "#     # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(train_images):\n",
    "#         # extract the class label\n",
    "#         # get the labels from the name of the images by extract the string before \"_\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # read and resize image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         trainImage.append(img)\n",
    "#         trainLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(train_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(train_images)))\n",
    "            \n",
    "#       # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(test_images):\n",
    "#         # extract the class label\n",
    "#         # our images were named as labels.image_number.format\n",
    "#         # get the labels from the name of the images by extract the string before \".\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # extract CNN features in the image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         testImage.append(img)\n",
    "#         testLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(test_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(test_images)))\n",
    "\n",
    "\n",
    "#     trainImage = np.array(trainImage,dtype = float32)\n",
    "#     trainLabels = np.array(trainLabels)\n",
    "#     testImage = np.array(testImage,dtype = float32)\n",
    "#     testLabels = np.array(testLabels)\n",
    "#     print (trainImage.shape)\n",
    "    \n",
    "#     trainImage = trainImage.astype(np.float32) / 255\n",
    "#     testImage = testImage.astype(np.float32) / 255\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(trainLabels)\n",
    "#     list(le.classes_)\n",
    "#     trainLabels = le.transform(trainLabels) \n",
    "#     testLabels = le.transform(testLabels) \n",
    "    \n",
    "#     return trainImage, trainLabels, testImage, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainImage, trainLabels, testImage, testLabels = load_flower_data()\n",
    "\n",
    "# trainImage_GLCM_4 = batch_GLCM(trainImage)\n",
    "# testImage_GLCM_4  = batch_GLCM(testImage)\n",
    "# nb_classes = 2\n",
    "\n",
    "# # Convert class vectors to binary class matrices.\n",
    "# trainLabels = keras.utils.to_categorical(trainLabels, nb_classes)\n",
    "# print (trainLabels)\n",
    "# testLabels = keras.utils.to_categorical(testLabels, nb_classes)\n",
    "# print (testLabels)\n",
    "# print (testLabels.shape)\n",
    "\n",
    "# np.save('../trainImage.npy', trainImage)\n",
    "# np.save('../trainLabels.npy', trainLabels)\n",
    "# np.save('../testImage.npy', testImage)\n",
    "# np.save('../testLabels.npy', testLabels)\n",
    "# np.save('../trainImage_GLCM_4.npy', trainImage_GLCM_4)\n",
    "# np.save('../testImage_GLCM_4.npy', testImage_GLCM_4)\n",
    "\n",
    "# print(\"[INFO] trainImage matrix: {:.2f}MB\".format(\n",
    "#     (trainImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainLabels matrix: {:.4f}MB\".format(\n",
    "#     (trainLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage matrix: {:.2f}MB\".format(\n",
    "#     (testImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testLabels matrix: {:.4f}MB\".format(\n",
    "#     (testLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainImage_GLCM_4 matrix: {:.2f}MB\".format(\n",
    "#     (trainImage_GLCM_4.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage_GLCM_4 matrix: {:.4f}MB\".format(\n",
    "#     (testImage_GLCM_4.nbytes) / (1024 * 1000.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self, sess, epoch, batch_size, dataset_name, checkpoint_dir, log_dir, trainhist_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\\\n",
    "        \n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.trainhist_dir = trainhist_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.classname = ['Iris', 'Pansy']\n",
    "\n",
    "        # parameters\n",
    "        self.input_height = 128\n",
    "        self.input_width = 128\n",
    "        self.c_dim = 3  # color dimension\n",
    "        self.glcm_dim = 12  # glcm dimension\n",
    "        self.nb_class = 2\n",
    "        \n",
    "        # number of convolutional filters to use  \n",
    "        self.nb_CNN = [32, 64, 64, 64, 128]  \n",
    "        # number of dense filters to use  \n",
    "        self.nb_Dense = [256] \n",
    "        # size of pooling area for max pooling  \n",
    "        self.pool_size = (2, 2)  \n",
    "        # convolution kernel size  \n",
    "        self.kernel_size = (3, 3)\n",
    "        self.batch_normalization_control = True\n",
    "        \n",
    "        # name for checkpoint\n",
    "        self.model_name = 'CNN_GLCM_4_C%d_D%d_Kernel(%d,%d)_%d_lrdecay' % (len(self.nb_CNN), len(self.nb_Dense),\n",
    "                                                          self.kernel_size[0], self.kernel_size[1], max(self.nb_CNN))\n",
    "\n",
    "        # train\n",
    "        #设置一个全局的计数器\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(0.001, \n",
    "                                             global_step=self.global_step, \n",
    "                                             decay_steps=10, \n",
    "                                             decay_rate=0.9, \n",
    "                                             staircase=True)\n",
    "        self.beta1 = 0.5\n",
    "        #max model to keep saving\n",
    "        self.max_to_keep = 300\n",
    "\n",
    "        #定义最大灰度级数\n",
    "        self.gray_level = 128\n",
    "        \n",
    "        # test\n",
    "\n",
    "        #load_flower_data\n",
    "        self.train_x = np.load('../trainImage.npy')\n",
    "        self.train_y = np.load('../trainLabels.npy')\n",
    "        self.test_x = np.load('../testImage.npy')\n",
    "        self.test_y = np.load('../testLabels.npy')\n",
    "        self.train_x_glcm = np.load('../trainImage_GLCM_4.npy')\n",
    "        self.test_x_glcm = np.load('../testImage_GLCM_4.npy')\n",
    "        \n",
    "        #记录\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['losses'] = []\n",
    "        self.train_hist['accuracy'] = []\n",
    "        self.train_hist['learning_rate'] = []\n",
    "        self.train_hist['per_epoch_ptimes'] = []\n",
    "        self.train_hist['total_ptime'] = []\n",
    "        \n",
    "        # get number of batches for a single epoch\n",
    "        self.num_batches_train = len(self.train_x) // self.batch_size\n",
    "        self.num_batches_test= len(self.test_x) // self.batch_size\n",
    "\n",
    "    def cnn_model(self, x, x_GLCM, keep_prob, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(\"cnn\", reuse=reuse):\n",
    "             \n",
    "            #初始化参数\n",
    "            W = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "            B = tf.constant_initializer(0.0)\n",
    "        \n",
    "            print(\"CNN:x\",x.get_shape()) # 128, 128, 3 \n",
    "            print(\"CNN:x_GLCM\",x_GLCM.get_shape()) # 128, 128, 3 \n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_1 = tf.layers.conv2d(inputs = x,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_1'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_1.get_shape())\n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_2 = tf.layers.conv2d(inputs = x_GLCM,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_2'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_2.get_shape())\n",
    "\n",
    "            #把数据和边缘进行连接\n",
    "            net = tf.concat([net1_1, net1_2], 3)\n",
    "            net = tf.layers.batch_normalization(net, training=is_training)\n",
    "            net = tf.nn.relu(net, name = 'relu_conv_1')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_1'\n",
    "                                             )\n",
    "            \n",
    "            for i in range(2,len(self.nb_CNN)+1):\n",
    "                net = tf.layers.conv2d(inputs = net,                 # 输入,\n",
    "                                       filters = self.nb_CNN[i-1],      # 卷积核个数,\n",
    "                                       kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                       strides = (1, 1),\n",
    "                                       padding = 'same',              # padding方法\n",
    "                                       kernel_initializer = W,\n",
    "                                       bias_initializer = B,\n",
    "                                       kernel_regularizer = None,\n",
    "                                       bias_regularizer = None,\n",
    "                                       activity_regularizer = None,\n",
    "                                       name = 'conv_'+ str(i)        # 命名用于获取变量\n",
    "                                       )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "                if self.batch_normalization_control:\n",
    "                    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_conv_' + str(i))\n",
    "                net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_' + str(i)\n",
    "                                             )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #flatten\n",
    "            net = tf.reshape(net, [-1, int(net.get_shape()[1]*net.get_shape()[2]*net.get_shape()[3])],name='flatten')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #dense layer\n",
    "            for i in range(1,len(self.nb_Dense)+1):\n",
    "                net = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_Dense[i-1],\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_' + str(i)\n",
    "                                    )\n",
    "#                 net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_dense_' + str(i))\n",
    "                net = tf.layers.dropout(inputs = net,\n",
    "                                        rate=keep_prob,\n",
    "                                        noise_shape=None,\n",
    "                                        seed=None,\n",
    "                                        training = is_training,\n",
    "                                        name= 'dropout_dense_' + str(i)\n",
    "                                        )\n",
    "            #output\n",
    "            logit = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_class,\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_output'\n",
    "                                    )\n",
    "            out_logit = tf.nn.softmax(logit, name=\"softmax\")\n",
    "            print(\"CNN:out_logit\",out_logit.get_shape())\n",
    "            print(\"------------------------\")    \n",
    "\n",
    "            return out_logit, logit\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.x = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.c_dim], \n",
    "                                name='x_image')\n",
    "        \n",
    "        self.x_GLCM_fill = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.glcm_dim], \n",
    "                                name='x_GLCM_fill')\n",
    "\n",
    "        self.y = tf.placeholder(tf.float32, shape=[self.batch_size, self.nb_class], name='y_label')\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.add_global = self.global_step.assign_add(1)\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "\n",
    "        # output of cnn_model\n",
    "        self.out_logit, self.logit = self.cnn_model(self.x, self.x_GLCM_fill, self.keep_prob, is_training=True, reuse=False)\n",
    "        \n",
    "        self.loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y,\n",
    "                                                                                         logits =self.logit))\n",
    "        \n",
    "        \"\"\" Training \"\"\"\n",
    "        # trainable variables into a group\n",
    "        tf_vars = tf.trainable_variables()\n",
    "        cnn_vars = [var for var in tf_vars if var.name.startswith('cnn')]\n",
    "\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.cnn_optim = tf.train.AdamOptimizer(self.lr, beta1=self.beta1).minimize(self.loss_cross_entropy,\n",
    "                                                                                        var_list=cnn_vars)\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        # output of cnn_model\n",
    "        self.out_logit_test, self.logit_test = self.cnn_model(self.x, self.x_GLCM_fill, self.keep_prob, is_training=False, reuse=True)\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit_test, 1), tf.argmax(self.y, 1))\n",
    "        self.predict = tf.argmax(self.logit_test, 1)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        self.loss_sum = tf.summary.scalar(\"loss\", self.loss_cross_entropy)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver(max_to_keep = self.max_to_keep)\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_epoch = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_epoch) + 1\n",
    "            counter = 1\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name+'.pkl', 'rb') \n",
    "            self.train_hist = pickle.load(f)\n",
    "            f.close()\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "            print(\" [!] START_EPOCH is \", start_epoch)\n",
    "        else:\n",
    "            start_epoch = 1\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        for epoch_loop in range(start_epoch, self.epoch + 1):\n",
    "\n",
    "            CNN_losses = []\n",
    "  \n",
    "            epoch_start_time = time.time()\n",
    "            shuffle_idxs = random.sample(range(0, self.train_x.shape[0]), self.train_x.shape[0])\n",
    "            shuffled_set = self.train_x[shuffle_idxs]\n",
    "            shuffled_set_glcm = self.train_x_glcm[shuffle_idxs]\n",
    "            shuffled_label = self.train_y[shuffle_idxs]\n",
    "    \n",
    "            # get batch data\n",
    "            for idx in range(self.num_batches_train):\n",
    "                batch_x = shuffled_set[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "#                 batch_x_GLCM = self.batch_GLCM(batch_x)\n",
    "                batch_x_GLCM = shuffled_set_glcm[idx*self.batch_size:(idx+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, 1, 1, self.glcm_dim])\n",
    "                batch_x_GLCM_fill = batch_x_GLCM * np.ones([self.batch_size, self.input_height, self.input_width,\n",
    "                                                            self.glcm_dim])\n",
    "                batch_y = shuffled_label[idx*self.batch_size:(idx+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                \n",
    "\n",
    "                # update D network\n",
    "                _, summary_str, cnn_loss = self.sess.run([self.cnn_optim, self.loss_sum, self.loss_cross_entropy],\n",
    "                                               feed_dict={self.x: batch_x,\n",
    "                                                          self.x_GLCM_fill: batch_x_GLCM_fill,\n",
    "                                                          self.y: batch_y,\n",
    "                                                          self.keep_prob: 0.5}\n",
    "                                                      )\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                CNN_losses.append(cnn_loss)\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f\" % (epoch_loop, idx, self.num_batches_train, \n",
    "                                                                          time.time() - start_time, cnn_loss))\n",
    "\n",
    "            # After an epoch\n",
    "            # Evaluates accuracy on test set\n",
    "            test_accuracy_list = []\n",
    "            for idx_test in range(self.num_batches_test):\n",
    "                batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "#                 batch_x_GLCM_test =self.batch_GLCM(batch_x_test)\n",
    "                batch_x_GLCM_test = self.test_x_glcm[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, 1, 1, self.glcm_dim])\n",
    "                batch_x_GLCM_fill_test = batch_x_GLCM_test * np.ones([self.batch_size, self.input_height, self.input_width, self.glcm_dim])\n",
    "                batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                    self.x_GLCM_fill: batch_x_GLCM_fill_test,\n",
    "                                                                    self.y: batch_y_tes,\n",
    "                                                                    self.keep_prob: 1.0})\n",
    "                test_accuracy_list.append(accuracy)\n",
    "            test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        \n",
    "            #update learning rate\n",
    "            _, rate = sess.run([self.add_global, self.lr])\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            print('[%d/%d] - ptime: %.4f loss: %.8f acc: %.5f lr: %.8f'% (epoch_loop, self.epoch, per_epoch_ptime, \n",
    "                                                                    np.mean(CNN_losses), test_accuracy, rate))\n",
    "            \n",
    "            self.train_hist['losses'].append(np.mean(CNN_losses))\n",
    "            self.train_hist['accuracy'].append( test_accuracy)\n",
    "            self.train_hist['learning_rate'].append(rate)\n",
    "            self.train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "            \n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, epoch_loop)\n",
    "            \n",
    "            # save trainhist for train\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name + '.pkl', 'wb') \n",
    "            pickle.dump(self.train_hist, f)\n",
    "            f.close()\n",
    "            self.show_train_hist(self.train_hist, save=True, path= self.trainhist_dir + '/' \n",
    "                                 + self.model_name + '.png')\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_ptime = end_time - start_time\n",
    "        self.train_hist['total_ptime'].append(total_ptime)\n",
    "        print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(self.train_hist['per_epoch_ptimes']), \n",
    "                                                                          self.epoch, total_ptime))\n",
    "        print(\" [*] Training finished!\")\n",
    "        \n",
    "        \"\"\"test after train\"\"\"\n",
    "        best_acc = max(self.train_hist['accuracy'])\n",
    "        beat_epoch = self.train_hist['accuracy'].index(best_acc) + 1\n",
    "        print (\" [*] Best Epoch: \", beat_epoch, \", Accuracy: \", best_acc)\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(beat_epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_GLCM_test = self.test_x_glcm[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, 1, 1, self.glcm_dim])\n",
    "            batch_x_GLCM_fill_test = batch_x_GLCM_test * np.ones([self.batch_size, self.input_height, \n",
    "                                                                  self.input_width, self.glcm_dim])\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_GLCM_fill: batch_x_GLCM_fill_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Best Epoch:\", beat_epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "\n",
    "    def test(self, epoch):\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_GLCM_test = self.test_x_glcm[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, 1, 1, self.glcm_dim])\n",
    "            batch_x_GLCM_fill_test = batch_x_GLCM_test * np.ones([self.batch_size, self.input_height, \n",
    "                                                                  self.input_width, self.glcm_dim])\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_GLCM_fill: batch_x_GLCM_fill_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Epoch:\", epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "        \n",
    "    def show_all_variables(self):\n",
    "        model_vars = tf.trainable_variables()\n",
    "        tf.contrib.slim.model_analyzer.analyze_vars(model_vars, print_info=True) \n",
    "\n",
    "    def maxGrayLevel(self, img):\n",
    "        max_gray_level=0\n",
    "        (height,width)=img.shape\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                if img[y][x] > max_gray_level:\n",
    "                    max_gray_level = img[y][x]\n",
    "        return max_gray_level+1\n",
    "\n",
    "    def getGlcm(self, img,d_x,d_y):\n",
    "        srcdata=img.copy()\n",
    "        ret=[[0.0 for i in range(self.gray_level)] for j in range(self.gray_level)]\n",
    "        (height,width) = img.shape\n",
    "\n",
    "        max_gray_level=self.maxGrayLevel(img)\n",
    "\n",
    "        #若灰度级数大于gray_level，则将图像的灰度级缩小至gray_level，减小灰度共生矩阵的大小\n",
    "        if max_gray_level > self.gray_level:\n",
    "            for j in range(height):\n",
    "                for i in range(width):\n",
    "                    srcdata[j][i] = srcdata[j][i]*self.gray_level / max_gray_level\n",
    "\n",
    "        for j in range(height-d_y):\n",
    "            for i in range(width-d_x):\n",
    "                rows = srcdata[j][i]\n",
    "                cols = srcdata[j + d_y][i+d_x]\n",
    "                ret[rows][cols]+=1.0\n",
    "\n",
    "        for i in range(self.gray_level):\n",
    "            for j in range(self.gray_level):\n",
    "                ret[i][j]/=float(height*width)\n",
    "\n",
    "        return np.array(ret)\n",
    "    \n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            epoch = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read [{}], epoch [{}]\".format(ckpt_name,epoch))\n",
    "            return True, epoch\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "        \n",
    "    def show_train_hist(self, hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "        x = range(1, len(hist['losses'])+1)\n",
    "\n",
    "        y1 = hist['losses']\n",
    "        y2 = hist['accuracy']\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "                            \n",
    "        ax2 = ax1.twinx()  \n",
    "\n",
    "        ax1.plot(x, y1, 'b')\n",
    "        ax2.plot(x, y2, 'r')\n",
    "                            \n",
    "        ax1.set_xlabel('Epoch')\n",
    "                            \n",
    "        ax1.set_ylabel('CNN_loss')    \n",
    "        ax2.set_ylabel('accuracy')\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(path, dpi = 400)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_GLCM (100, 128, 128, 12)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_GLCM (100, 128, 128, 12)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "cnn/conv_1_1/kernel:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "cnn/conv_1_1/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/conv_1_2/kernel:0 (float32_ref 3x3x12x32) [3456, bytes: 13824]\n",
      "cnn/conv_1_2/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/batch_normalization/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_2/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_2/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_3/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_3/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_4/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_4/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_5/kernel:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
      "cnn/conv_5/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/dense_1/kernel:0 (float32_ref 2048x256) [524288, bytes: 2097152]\n",
      "cnn/dense_1/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "cnn/dense_output/kernel:0 (float32_ref 256x2) [512, bytes: 2048]\n",
      "cnn/dense_output/bias:0 (float32_ref 2) [2, bytes: 8]\n",
      "Total size of variables: 714850\n",
      "Total bytes of variables: 2859400\n",
      " [*] Reading checkpoints...\n",
      " [*] Failed to find a checkpoint\n",
      " [!] Load failed...\n",
      "Epoch: [ 1] [   0/  20] time: 1.8311, loss: 0.72903705\n",
      "Epoch: [ 1] [   1/  20] time: 2.2280, loss: 0.64296496\n",
      "Epoch: [ 1] [   2/  20] time: 2.6246, loss: 0.55526507\n",
      "Epoch: [ 1] [   3/  20] time: 3.0200, loss: 0.36518207\n",
      "Epoch: [ 1] [   4/  20] time: 3.4176, loss: 0.18161975\n",
      "Epoch: [ 1] [   5/  20] time: 3.8148, loss: 0.16712444\n",
      "Epoch: [ 1] [   6/  20] time: 4.2135, loss: 0.13485123\n",
      "Epoch: [ 1] [   7/  20] time: 4.6096, loss: 0.08440006\n",
      "Epoch: [ 1] [   8/  20] time: 5.0041, loss: 0.04515081\n",
      "Epoch: [ 1] [   9/  20] time: 5.4035, loss: 0.05809127\n",
      "Epoch: [ 1] [  10/  20] time: 5.7970, loss: 0.04757144\n",
      "Epoch: [ 1] [  11/  20] time: 6.1922, loss: 0.03738606\n",
      "Epoch: [ 1] [  12/  20] time: 6.5891, loss: 0.03287074\n",
      "Epoch: [ 1] [  13/  20] time: 6.9821, loss: 0.02411179\n",
      "Epoch: [ 1] [  14/  20] time: 7.3783, loss: 0.03108764\n",
      "Epoch: [ 1] [  15/  20] time: 7.7743, loss: 0.01104783\n",
      "Epoch: [ 1] [  16/  20] time: 8.1707, loss: 0.02519835\n",
      "Epoch: [ 1] [  17/  20] time: 8.5651, loss: 0.06087268\n",
      "Epoch: [ 1] [  18/  20] time: 8.9596, loss: 0.05037591\n",
      "Epoch: [ 1] [  19/  20] time: 9.3573, loss: 0.02024801\n",
      "[1/50] - ptime: 9.5624 loss: 0.16522285 acc: 0.80000 lr: 0.00100000\n",
      "Epoch: [ 2] [   0/  20] time: 10.4642, loss: 0.01770490\n",
      "Epoch: [ 2] [   1/  20] time: 10.8634, loss: 0.02764650\n",
      "Epoch: [ 2] [   2/  20] time: 11.2639, loss: 0.11129646\n",
      "Epoch: [ 2] [   3/  20] time: 11.6663, loss: 0.02359250\n",
      "Epoch: [ 2] [   4/  20] time: 12.0640, loss: 0.01335783\n",
      "Epoch: [ 2] [   5/  20] time: 12.4646, loss: 0.01316798\n",
      "Epoch: [ 2] [   6/  20] time: 12.8617, loss: 0.05713362\n",
      "Epoch: [ 2] [   7/  20] time: 13.2589, loss: 0.11767265\n",
      "Epoch: [ 2] [   8/  20] time: 13.6599, loss: 0.06876335\n",
      "Epoch: [ 2] [   9/  20] time: 14.0566, loss: 0.05124222\n",
      "Epoch: [ 2] [  10/  20] time: 14.4548, loss: 0.05620397\n",
      "Epoch: [ 2] [  11/  20] time: 14.8549, loss: 0.03613215\n",
      "Epoch: [ 2] [  12/  20] time: 15.2565, loss: 0.01047914\n",
      "Epoch: [ 2] [  13/  20] time: 15.6602, loss: 0.06561997\n",
      "Epoch: [ 2] [  14/  20] time: 16.0656, loss: 0.09615909\n",
      "Epoch: [ 2] [  15/  20] time: 16.4676, loss: 0.02936875\n",
      "Epoch: [ 2] [  16/  20] time: 16.8697, loss: 0.05451425\n",
      "Epoch: [ 2] [  17/  20] time: 17.2707, loss: 0.03966201\n",
      "Epoch: [ 2] [  18/  20] time: 17.6709, loss: 0.08482064\n",
      "Epoch: [ 2] [  19/  20] time: 18.0692, loss: 0.08401609\n",
      "[2/50] - ptime: 8.3221 loss: 0.05292770 acc: 0.80000 lr: 0.00100000\n",
      "Epoch: [ 3] [   0/  20] time: 19.1271, loss: 0.03548326\n",
      "Epoch: [ 3] [   1/  20] time: 19.5295, loss: 0.04246034\n",
      "Epoch: [ 3] [   2/  20] time: 19.9300, loss: 0.02956955\n",
      "Epoch: [ 3] [   3/  20] time: 20.3303, loss: 0.02418905\n",
      "Epoch: [ 3] [   4/  20] time: 20.7326, loss: 0.00380000\n",
      "Epoch: [ 3] [   5/  20] time: 21.1267, loss: 0.02380143\n",
      "Epoch: [ 3] [   6/  20] time: 21.5232, loss: 0.07001729\n",
      "Epoch: [ 3] [   7/  20] time: 21.9196, loss: 0.02519442\n",
      "Epoch: [ 3] [   8/  20] time: 22.3179, loss: 0.02568487\n",
      "Epoch: [ 3] [   9/  20] time: 22.7167, loss: 0.02766282\n",
      "Epoch: [ 3] [  10/  20] time: 23.1133, loss: 0.04379823\n",
      "Epoch: [ 3] [  11/  20] time: 23.5093, loss: 0.01224960\n",
      "Epoch: [ 3] [  12/  20] time: 23.9065, loss: 0.04582223\n",
      "Epoch: [ 3] [  13/  20] time: 24.3019, loss: 0.05276235\n",
      "Epoch: [ 3] [  14/  20] time: 24.7016, loss: 0.04434545\n",
      "Epoch: [ 3] [  15/  20] time: 25.0977, loss: 0.01611048\n",
      "Epoch: [ 3] [  16/  20] time: 25.4948, loss: 0.01590500\n",
      "Epoch: [ 3] [  17/  20] time: 25.8925, loss: 0.07533578\n",
      "Epoch: [ 3] [  18/  20] time: 26.2864, loss: 0.09160315\n",
      "Epoch: [ 3] [  19/  20] time: 26.6855, loss: 0.01107685\n",
      "[3/50] - ptime: 8.2716 loss: 0.03584361 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 4] [   0/  20] time: 27.8209, loss: 0.01381670\n",
      "Epoch: [ 4] [   1/  20] time: 28.2198, loss: 0.06430043\n",
      "Epoch: [ 4] [   2/  20] time: 28.6185, loss: 0.02645212\n",
      "Epoch: [ 4] [   3/  20] time: 29.0165, loss: 0.04578665\n",
      "Epoch: [ 4] [   4/  20] time: 29.4170, loss: 0.05260857\n",
      "Epoch: [ 4] [   5/  20] time: 29.8179, loss: 0.02807181\n",
      "Epoch: [ 4] [   6/  20] time: 30.2166, loss: 0.02044250\n",
      "Epoch: [ 4] [   7/  20] time: 30.6152, loss: 0.02315994\n",
      "Epoch: [ 4] [   8/  20] time: 31.0126, loss: 0.00043222\n",
      "Epoch: [ 4] [   9/  20] time: 31.4090, loss: 0.01873687\n",
      "Epoch: [ 4] [  10/  20] time: 31.8039, loss: 0.04464768\n",
      "Epoch: [ 4] [  11/  20] time: 32.2013, loss: 0.06311516\n",
      "Epoch: [ 4] [  12/  20] time: 32.5987, loss: 0.01007883\n",
      "Epoch: [ 4] [  13/  20] time: 32.9951, loss: 0.04206531\n",
      "Epoch: [ 4] [  14/  20] time: 33.3914, loss: 0.00347248\n",
      "Epoch: [ 4] [  15/  20] time: 33.7904, loss: 0.00533278\n",
      "Epoch: [ 4] [  16/  20] time: 34.1882, loss: 0.00925420\n",
      "Epoch: [ 4] [  17/  20] time: 34.5886, loss: 0.02075894\n",
      "Epoch: [ 4] [  18/  20] time: 34.9889, loss: 0.03668806\n",
      "Epoch: [ 4] [  19/  20] time: 35.3865, loss: 0.03471880\n",
      "[4/50] - ptime: 8.2850 loss: 0.02819700 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 5] [   0/  20] time: 36.4866, loss: 0.00906426\n",
      "Epoch: [ 5] [   1/  20] time: 36.8820, loss: 0.04865126\n",
      "Epoch: [ 5] [   2/  20] time: 37.2786, loss: 0.00561268\n",
      "Epoch: [ 5] [   3/  20] time: 37.6784, loss: 0.01687939\n",
      "Epoch: [ 5] [   4/  20] time: 38.0804, loss: 0.00654763\n",
      "Epoch: [ 5] [   5/  20] time: 38.4807, loss: 0.03525575\n",
      "Epoch: [ 5] [   6/  20] time: 38.8820, loss: 0.02411612\n",
      "Epoch: [ 5] [   7/  20] time: 39.2816, loss: 0.04918084\n",
      "Epoch: [ 5] [   8/  20] time: 39.6808, loss: 0.00708407\n",
      "Epoch: [ 5] [   9/  20] time: 40.0781, loss: 0.04702867\n",
      "Epoch: [ 5] [  10/  20] time: 40.4778, loss: 0.01456945\n",
      "Epoch: [ 5] [  11/  20] time: 40.8733, loss: 0.00945164\n",
      "Epoch: [ 5] [  12/  20] time: 41.2727, loss: 0.02344904\n",
      "Epoch: [ 5] [  13/  20] time: 41.6800, loss: 0.00600153\n",
      "Epoch: [ 5] [  14/  20] time: 42.0837, loss: 0.02353917\n",
      "Epoch: [ 5] [  15/  20] time: 42.4734, loss: 0.01094032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 5] [  16/  20] time: 42.8721, loss: 0.08398198\n",
      "Epoch: [ 5] [  17/  20] time: 43.2751, loss: 0.02445393\n",
      "Epoch: [ 5] [  18/  20] time: 43.6779, loss: 0.04666627\n",
      "Epoch: [ 5] [  19/  20] time: 44.0768, loss: 0.02884663\n",
      "[5/50] - ptime: 8.3224 loss: 0.02606603 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 6] [   0/  20] time: 45.2517, loss: 0.01076504\n",
      "Epoch: [ 6] [   1/  20] time: 45.6535, loss: 0.00703770\n",
      "Epoch: [ 6] [   2/  20] time: 46.0488, loss: 0.00626811\n",
      "Epoch: [ 6] [   3/  20] time: 46.4450, loss: 0.02275191\n",
      "Epoch: [ 6] [   4/  20] time: 46.8435, loss: 0.03972794\n",
      "Epoch: [ 6] [   5/  20] time: 47.2474, loss: 0.03683512\n",
      "Epoch: [ 6] [   6/  20] time: 47.6525, loss: 0.00563183\n",
      "Epoch: [ 6] [   7/  20] time: 48.0551, loss: 0.09456391\n",
      "Epoch: [ 6] [   8/  20] time: 48.4510, loss: 0.03872626\n",
      "Epoch: [ 6] [   9/  20] time: 48.8377, loss: 0.04416813\n",
      "Epoch: [ 6] [  10/  20] time: 49.2223, loss: 0.02481940\n",
      "Epoch: [ 6] [  11/  20] time: 49.6103, loss: 0.01568204\n",
      "Epoch: [ 6] [  12/  20] time: 49.9968, loss: 0.00863161\n",
      "Epoch: [ 6] [  13/  20] time: 50.3874, loss: 0.03252118\n",
      "Epoch: [ 6] [  14/  20] time: 50.7749, loss: 0.02227816\n",
      "Epoch: [ 6] [  15/  20] time: 51.1657, loss: 0.01083531\n",
      "Epoch: [ 6] [  16/  20] time: 51.5568, loss: 0.04109506\n",
      "Epoch: [ 6] [  17/  20] time: 51.9497, loss: 0.02041983\n",
      "Epoch: [ 6] [  18/  20] time: 52.3430, loss: 0.02234594\n",
      "Epoch: [ 6] [  19/  20] time: 52.7419, loss: 0.05707679\n",
      "[6/50] - ptime: 8.2294 loss: 0.02810907 acc: 0.79000 lr: 0.00100000\n",
      "Epoch: [ 7] [   0/  20] time: 53.8388, loss: 0.02784190\n",
      "Epoch: [ 7] [   1/  20] time: 54.2415, loss: 0.01206551\n",
      "Epoch: [ 7] [   2/  20] time: 54.6482, loss: 0.00053197\n",
      "Epoch: [ 7] [   3/  20] time: 55.0548, loss: 0.02829568\n",
      "Epoch: [ 7] [   4/  20] time: 55.4624, loss: 0.01065593\n",
      "Epoch: [ 7] [   5/  20] time: 55.8581, loss: 0.03714638\n",
      "Epoch: [ 7] [   6/  20] time: 56.2544, loss: 0.01077426\n",
      "Epoch: [ 7] [   7/  20] time: 56.6530, loss: 0.04555140\n",
      "Epoch: [ 7] [   8/  20] time: 57.0495, loss: 0.02442258\n",
      "Epoch: [ 7] [   9/  20] time: 57.4485, loss: 0.01108073\n",
      "Epoch: [ 7] [  10/  20] time: 57.8424, loss: 0.21093290\n",
      "Epoch: [ 7] [  11/  20] time: 58.2274, loss: 0.06846726\n",
      "Epoch: [ 7] [  12/  20] time: 58.6267, loss: 0.01098697\n",
      "Epoch: [ 7] [  13/  20] time: 59.0226, loss: 0.01994846\n",
      "Epoch: [ 7] [  14/  20] time: 59.4231, loss: 0.01675565\n",
      "Epoch: [ 7] [  15/  20] time: 59.8254, loss: 0.00108022\n",
      "Epoch: [ 7] [  16/  20] time: 60.2213, loss: 0.09816903\n",
      "Epoch: [ 7] [  17/  20] time: 60.6181, loss: 0.02367787\n",
      "Epoch: [ 7] [  18/  20] time: 61.0142, loss: 0.01775167\n",
      "Epoch: [ 7] [  19/  20] time: 61.4177, loss: 0.00701445\n",
      "[7/50] - ptime: 8.2968 loss: 0.03415754 acc: 0.75000 lr: 0.00100000\n",
      "Epoch: [ 8] [   0/  20] time: 62.4796, loss: 0.02407565\n",
      "Epoch: [ 8] [   1/  20] time: 62.8785, loss: 0.01315214\n",
      "Epoch: [ 8] [   2/  20] time: 63.2758, loss: 0.04407100\n",
      "Epoch: [ 8] [   3/  20] time: 63.6730, loss: 0.06907609\n",
      "Epoch: [ 8] [   4/  20] time: 64.0705, loss: 0.02083184\n",
      "Epoch: [ 8] [   5/  20] time: 64.4671, loss: 0.02862203\n",
      "Epoch: [ 8] [   6/  20] time: 64.8652, loss: 0.01252304\n",
      "Epoch: [ 8] [   7/  20] time: 65.2646, loss: 0.02483507\n",
      "Epoch: [ 8] [   8/  20] time: 65.6650, loss: 0.06734638\n",
      "Epoch: [ 8] [   9/  20] time: 66.0641, loss: 0.00955732\n",
      "Epoch: [ 8] [  10/  20] time: 66.4681, loss: 0.01746502\n",
      "Epoch: [ 8] [  11/  20] time: 66.8693, loss: 0.01066540\n",
      "Epoch: [ 8] [  12/  20] time: 67.2769, loss: 0.01080801\n",
      "Epoch: [ 8] [  13/  20] time: 67.6762, loss: 0.00245589\n",
      "Epoch: [ 8] [  14/  20] time: 68.0754, loss: 0.04194295\n",
      "Epoch: [ 8] [  15/  20] time: 68.4732, loss: 0.01916851\n",
      "Epoch: [ 8] [  16/  20] time: 68.8721, loss: 0.02375519\n",
      "Epoch: [ 8] [  17/  20] time: 69.2707, loss: 0.02366960\n",
      "Epoch: [ 8] [  18/  20] time: 69.6684, loss: 0.03688466\n",
      "Epoch: [ 8] [  19/  20] time: 70.0659, loss: 0.00923846\n",
      "[8/50] - ptime: 8.3051 loss: 0.02550722 acc: 0.80000 lr: 0.00100000\n",
      "Epoch: [ 9] [   0/  20] time: 71.1290, loss: 0.00572360\n",
      "Epoch: [ 9] [   1/  20] time: 71.5255, loss: 0.01862449\n",
      "Epoch: [ 9] [   2/  20] time: 71.9270, loss: 0.00754265\n",
      "Epoch: [ 9] [   3/  20] time: 72.3240, loss: 0.00580282\n",
      "Epoch: [ 9] [   4/  20] time: 72.7218, loss: 0.02396286\n",
      "Epoch: [ 9] [   5/  20] time: 73.1167, loss: 0.00358654\n",
      "Epoch: [ 9] [   6/  20] time: 73.5184, loss: 0.01184258\n",
      "Epoch: [ 9] [   7/  20] time: 73.9164, loss: 0.01595674\n",
      "Epoch: [ 9] [   8/  20] time: 74.3146, loss: 0.00960503\n",
      "Epoch: [ 9] [   9/  20] time: 74.7159, loss: 0.02091543\n",
      "Epoch: [ 9] [  10/  20] time: 75.1180, loss: 0.01078393\n",
      "Epoch: [ 9] [  11/  20] time: 75.5207, loss: 0.00015256\n",
      "Epoch: [ 9] [  12/  20] time: 75.9261, loss: 0.02418175\n",
      "Epoch: [ 9] [  13/  20] time: 76.3263, loss: 0.00054136\n",
      "Epoch: [ 9] [  14/  20] time: 76.7251, loss: 0.03112176\n",
      "Epoch: [ 9] [  15/  20] time: 77.1285, loss: 0.01503132\n",
      "Epoch: [ 9] [  16/  20] time: 77.5326, loss: 0.02262994\n",
      "Epoch: [ 9] [  17/  20] time: 77.9287, loss: 0.02271889\n",
      "Epoch: [ 9] [  18/  20] time: 78.3268, loss: 0.01514583\n",
      "Epoch: [ 9] [  19/  20] time: 78.7240, loss: 0.00901305\n",
      "[9/50] - ptime: 8.3107 loss: 0.01374416 acc: 0.78000 lr: 0.00100000\n",
      "Epoch: [10] [   0/  20] time: 79.7949, loss: 0.00464852\n",
      "Epoch: [10] [   1/  20] time: 80.1951, loss: 0.00764241\n",
      "Epoch: [10] [   2/  20] time: 80.5977, loss: 0.01150282\n",
      "Epoch: [10] [   3/  20] time: 80.9942, loss: 0.00591081\n",
      "Epoch: [10] [   4/  20] time: 81.3936, loss: 0.01212663\n",
      "Epoch: [10] [   5/  20] time: 81.7897, loss: 0.00931419\n",
      "Epoch: [10] [   6/  20] time: 82.1866, loss: 0.05097666\n",
      "Epoch: [10] [   7/  20] time: 82.5832, loss: 0.00607328\n",
      "Epoch: [10] [   8/  20] time: 82.9806, loss: 0.00937167\n",
      "Epoch: [10] [   9/  20] time: 83.3795, loss: 0.02310749\n",
      "Epoch: [10] [  10/  20] time: 83.7764, loss: 0.01064147\n",
      "Epoch: [10] [  11/  20] time: 84.1723, loss: 0.00703832\n",
      "Epoch: [10] [  12/  20] time: 84.5724, loss: 0.03299338\n",
      "Epoch: [10] [  13/  20] time: 84.9687, loss: 0.00127160\n",
      "Epoch: [10] [  14/  20] time: 85.3635, loss: 0.00294287\n",
      "Epoch: [10] [  15/  20] time: 85.7635, loss: 0.03392918\n",
      "Epoch: [10] [  16/  20] time: 86.1624, loss: 0.02612390\n",
      "Epoch: [10] [  17/  20] time: 86.5617, loss: 0.02308634\n",
      "Epoch: [10] [  18/  20] time: 86.9582, loss: 0.00030782\n",
      "Epoch: [10] [  19/  20] time: 87.3550, loss: 0.00705862\n",
      "[10/50] - ptime: 8.2835 loss: 0.01430340 acc: 0.78000 lr: 0.00090000\n",
      "Epoch: [11] [   0/  20] time: 88.4215, loss: 0.01038409\n",
      "Epoch: [11] [   1/  20] time: 88.8177, loss: 0.00187182\n",
      "Epoch: [11] [   2/  20] time: 89.2163, loss: 0.00517501\n",
      "Epoch: [11] [   3/  20] time: 89.6179, loss: 0.00133861\n",
      "Epoch: [11] [   4/  20] time: 90.0176, loss: 0.01315685\n",
      "Epoch: [11] [   5/  20] time: 90.4139, loss: 0.02618635\n",
      "Epoch: [11] [   6/  20] time: 90.8131, loss: 0.04541036\n",
      "Epoch: [11] [   7/  20] time: 91.2081, loss: 0.00758612\n",
      "Epoch: [11] [   8/  20] time: 91.6153, loss: 0.01074350\n",
      "Epoch: [11] [   9/  20] time: 92.0152, loss: 0.03212978\n",
      "Epoch: [11] [  10/  20] time: 92.4103, loss: 0.01837879\n",
      "Epoch: [11] [  11/  20] time: 92.8076, loss: 0.00689641\n",
      "Epoch: [11] [  12/  20] time: 93.2070, loss: 0.01749645\n",
      "Epoch: [11] [  13/  20] time: 93.6078, loss: 0.00752844\n",
      "Epoch: [11] [  14/  20] time: 94.0054, loss: 0.00613909\n",
      "Epoch: [11] [  15/  20] time: 94.4045, loss: 0.01112861\n",
      "Epoch: [11] [  16/  20] time: 94.8045, loss: 0.01600520\n",
      "Epoch: [11] [  17/  20] time: 95.2023, loss: 0.00414074\n",
      "Epoch: [11] [  18/  20] time: 95.6052, loss: 0.00070192\n",
      "Epoch: [11] [  19/  20] time: 96.0087, loss: 0.02084121\n",
      "[11/50] - ptime: 8.3102 loss: 0.01316197 acc: 0.78000 lr: 0.00090000\n",
      "Epoch: [12] [   0/  20] time: 97.0660, loss: 0.00306026\n",
      "Epoch: [12] [   1/  20] time: 97.4554, loss: 0.01473142\n",
      "Epoch: [12] [   2/  20] time: 97.8517, loss: 0.01112315\n",
      "Epoch: [12] [   3/  20] time: 98.2484, loss: 0.00626186\n",
      "Epoch: [12] [   4/  20] time: 98.6451, loss: 0.01540569\n",
      "Epoch: [12] [   5/  20] time: 99.0433, loss: 0.00407405\n",
      "Epoch: [12] [   6/  20] time: 99.4407, loss: 0.01373897\n",
      "Epoch: [12] [   7/  20] time: 99.8379, loss: 0.00176968\n",
      "Epoch: [12] [   8/  20] time: 100.2332, loss: 0.00038470\n",
      "Epoch: [12] [   9/  20] time: 100.6311, loss: 0.01012528\n",
      "Epoch: [12] [  10/  20] time: 101.0284, loss: 0.00123266\n",
      "Epoch: [12] [  11/  20] time: 101.4256, loss: 0.01246271\n",
      "Epoch: [12] [  12/  20] time: 101.8272, loss: 0.00997563\n",
      "Epoch: [12] [  13/  20] time: 102.2279, loss: 0.00677821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12] [  14/  20] time: 102.6267, loss: 0.00164070\n",
      "Epoch: [12] [  15/  20] time: 103.0240, loss: 0.01599712\n",
      "Epoch: [12] [  16/  20] time: 103.4199, loss: 0.00446005\n",
      "Epoch: [12] [  17/  20] time: 103.8177, loss: 0.00863937\n",
      "Epoch: [12] [  18/  20] time: 104.2131, loss: 0.00065396\n",
      "Epoch: [12] [  19/  20] time: 104.6098, loss: 0.02442282\n",
      "[12/50] - ptime: 8.2557 loss: 0.00834691 acc: 0.73000 lr: 0.00090000\n",
      "Epoch: [13] [   0/  20] time: 105.6959, loss: 0.00338952\n",
      "Epoch: [13] [   1/  20] time: 106.0936, loss: 0.01117324\n",
      "Epoch: [13] [   2/  20] time: 106.4951, loss: 0.02626865\n",
      "Epoch: [13] [   3/  20] time: 106.8962, loss: 0.00153270\n",
      "Epoch: [13] [   4/  20] time: 107.2960, loss: 0.00021074\n",
      "Epoch: [13] [   5/  20] time: 107.7063, loss: 0.00011741\n",
      "Epoch: [13] [   6/  20] time: 108.1118, loss: 0.00623184\n",
      "Epoch: [13] [   7/  20] time: 108.5078, loss: 0.01260487\n",
      "Epoch: [13] [   8/  20] time: 108.9051, loss: 0.00134460\n",
      "Epoch: [13] [   9/  20] time: 109.3008, loss: 0.01408030\n",
      "Epoch: [13] [  10/  20] time: 109.6941, loss: 0.01179324\n",
      "Epoch: [13] [  11/  20] time: 110.0897, loss: 0.01999704\n",
      "Epoch: [13] [  12/  20] time: 110.4943, loss: 0.00426431\n",
      "Epoch: [13] [  13/  20] time: 110.8905, loss: 0.01877124\n",
      "Epoch: [13] [  14/  20] time: 111.2859, loss: 0.00070520\n",
      "Epoch: [13] [  15/  20] time: 111.6849, loss: 0.04206506\n",
      "Epoch: [13] [  16/  20] time: 112.0814, loss: 0.00692435\n",
      "Epoch: [13] [  17/  20] time: 112.4769, loss: 0.00480687\n",
      "Epoch: [13] [  18/  20] time: 112.8732, loss: 0.00339346\n",
      "Epoch: [13] [  19/  20] time: 113.2690, loss: 0.00567145\n",
      "[13/50] - ptime: 8.2870 loss: 0.00976730 acc: 0.75000 lr: 0.00090000\n",
      "Epoch: [14] [   0/  20] time: 114.3356, loss: 0.00104032\n",
      "Epoch: [14] [   1/  20] time: 114.7363, loss: 0.00188911\n",
      "Epoch: [14] [   2/  20] time: 115.1324, loss: 0.02526706\n",
      "Epoch: [14] [   3/  20] time: 115.5276, loss: 0.02414184\n",
      "Epoch: [14] [   4/  20] time: 115.9229, loss: 0.03030683\n",
      "Epoch: [14] [   5/  20] time: 116.3220, loss: 0.00041245\n",
      "Epoch: [14] [   6/  20] time: 116.7208, loss: 0.01855757\n",
      "Epoch: [14] [   7/  20] time: 117.1167, loss: 0.01974848\n",
      "Epoch: [14] [   8/  20] time: 117.5157, loss: 0.00757131\n",
      "Epoch: [14] [   9/  20] time: 117.9129, loss: 0.01088422\n",
      "Epoch: [14] [  10/  20] time: 118.3093, loss: 0.00356088\n",
      "Epoch: [14] [  11/  20] time: 118.7117, loss: 0.05537359\n",
      "Epoch: [14] [  12/  20] time: 119.1078, loss: 0.00003013\n",
      "Epoch: [14] [  13/  20] time: 119.5047, loss: 0.02262373\n",
      "Epoch: [14] [  14/  20] time: 119.9036, loss: 0.00926661\n",
      "Epoch: [14] [  15/  20] time: 120.2988, loss: 0.01031273\n",
      "Epoch: [14] [  16/  20] time: 120.6976, loss: 0.01104518\n",
      "Epoch: [14] [  17/  20] time: 121.0975, loss: 0.00750175\n",
      "Epoch: [14] [  18/  20] time: 121.4956, loss: 0.03241554\n",
      "Epoch: [14] [  19/  20] time: 121.8930, loss: 0.01055267\n",
      "[14/50] - ptime: 8.2748 loss: 0.01512510 acc: 0.64000 lr: 0.00090000\n",
      "Epoch: [15] [   0/  20] time: 122.9882, loss: 0.00157831\n",
      "Epoch: [15] [   1/  20] time: 123.3845, loss: 0.01226675\n",
      "Epoch: [15] [   2/  20] time: 123.7798, loss: 0.00228176\n",
      "Epoch: [15] [   3/  20] time: 124.1765, loss: 0.00258015\n",
      "Epoch: [15] [   4/  20] time: 124.5733, loss: 0.00237213\n",
      "Epoch: [15] [   5/  20] time: 124.9709, loss: 0.00093434\n",
      "Epoch: [15] [   6/  20] time: 125.3685, loss: 0.01193567\n",
      "Epoch: [15] [   7/  20] time: 125.7712, loss: 0.06439773\n",
      "Epoch: [15] [   8/  20] time: 126.1683, loss: 0.00060730\n",
      "Epoch: [15] [   9/  20] time: 126.5648, loss: 0.01945073\n",
      "Epoch: [15] [  10/  20] time: 126.9663, loss: 0.01589854\n",
      "Epoch: [15] [  11/  20] time: 127.3708, loss: 0.04681110\n",
      "Epoch: [15] [  12/  20] time: 127.7681, loss: 0.02126419\n",
      "Epoch: [15] [  13/  20] time: 128.1642, loss: 0.02676922\n",
      "Epoch: [15] [  14/  20] time: 128.5665, loss: 0.00252242\n",
      "Epoch: [15] [  15/  20] time: 128.9713, loss: 0.00549059\n",
      "Epoch: [15] [  16/  20] time: 129.3604, loss: 0.01014982\n",
      "Epoch: [15] [  17/  20] time: 129.7584, loss: 0.02519017\n",
      "Epoch: [15] [  18/  20] time: 130.1561, loss: 0.00004308\n",
      "Epoch: [15] [  19/  20] time: 130.5593, loss: 0.02293447\n",
      "[15/50] - ptime: 8.2917 loss: 0.01477392 acc: 0.76000 lr: 0.00090000\n",
      "Epoch: [16] [   0/  20] time: 131.6491, loss: 0.01627064\n",
      "Epoch: [16] [   1/  20] time: 132.0465, loss: 0.00877642\n",
      "Epoch: [16] [   2/  20] time: 132.4425, loss: 0.00737212\n",
      "Epoch: [16] [   3/  20] time: 132.8430, loss: 0.00016505\n",
      "Epoch: [16] [   4/  20] time: 133.2464, loss: 0.01233060\n",
      "Epoch: [16] [   5/  20] time: 133.6521, loss: 0.01934140\n",
      "Epoch: [16] [   6/  20] time: 134.0481, loss: 0.03258146\n",
      "Epoch: [16] [   7/  20] time: 134.4435, loss: 0.01469223\n",
      "Epoch: [16] [   8/  20] time: 134.8434, loss: 0.00492606\n",
      "Epoch: [16] [   9/  20] time: 135.2396, loss: 0.03655878\n",
      "Epoch: [16] [  10/  20] time: 135.6383, loss: 0.00508124\n",
      "Epoch: [16] [  11/  20] time: 136.0278, loss: 0.00111772\n",
      "Epoch: [16] [  12/  20] time: 136.4255, loss: 0.00192441\n",
      "Epoch: [16] [  13/  20] time: 136.8160, loss: 0.01798516\n",
      "Epoch: [16] [  14/  20] time: 137.2135, loss: 0.01037457\n",
      "Epoch: [16] [  15/  20] time: 137.6118, loss: 0.00003037\n",
      "Epoch: [16] [  16/  20] time: 138.0102, loss: 0.03996199\n",
      "Epoch: [16] [  17/  20] time: 138.4066, loss: 0.00546531\n",
      "Epoch: [16] [  18/  20] time: 138.8025, loss: 0.02674592\n",
      "Epoch: [16] [  19/  20] time: 139.1985, loss: 0.00486968\n",
      "[16/50] - ptime: 8.2676 loss: 0.01332856 acc: 0.78000 lr: 0.00090000\n",
      "Epoch: [17] [   0/  20] time: 140.3628, loss: 0.01059007\n",
      "Epoch: [17] [   1/  20] time: 140.7608, loss: 0.00685264\n",
      "Epoch: [17] [   2/  20] time: 141.1582, loss: 0.06694671\n",
      "Epoch: [17] [   3/  20] time: 141.5604, loss: 0.01338711\n",
      "Epoch: [17] [   4/  20] time: 141.9570, loss: 0.04180537\n",
      "Epoch: [17] [   5/  20] time: 142.3531, loss: 0.00523128\n",
      "Epoch: [17] [   6/  20] time: 142.7529, loss: 0.05712837\n",
      "Epoch: [17] [   7/  20] time: 143.1388, loss: 0.00121272\n",
      "Epoch: [17] [   8/  20] time: 143.5378, loss: 0.01891561\n",
      "Epoch: [17] [   9/  20] time: 143.9357, loss: 0.00840099\n",
      "Epoch: [17] [  10/  20] time: 144.3336, loss: 0.00367356\n",
      "Epoch: [17] [  11/  20] time: 144.7531, loss: 0.00180904\n",
      "Epoch: [17] [  12/  20] time: 145.1539, loss: 0.00098290\n",
      "Epoch: [17] [  13/  20] time: 145.5520, loss: 0.04915132\n",
      "Epoch: [17] [  14/  20] time: 145.9562, loss: 0.01075429\n",
      "Epoch: [17] [  15/  20] time: 146.3585, loss: 0.02459285\n",
      "Epoch: [17] [  16/  20] time: 146.7438, loss: 0.02889020\n",
      "Epoch: [17] [  17/  20] time: 147.1458, loss: 0.00306916\n",
      "Epoch: [17] [  18/  20] time: 147.5445, loss: 0.01215011\n",
      "Epoch: [17] [  19/  20] time: 147.9428, loss: 0.00019960\n",
      "[17/50] - ptime: 8.2924 loss: 0.01828720 acc: 0.51000 lr: 0.00090000\n",
      "Epoch: [18] [   0/  20] time: 149.0019, loss: 0.05457527\n",
      "Epoch: [18] [   1/  20] time: 149.3997, loss: 0.00260793\n",
      "Epoch: [18] [   2/  20] time: 149.7996, loss: 0.00687138\n",
      "Epoch: [18] [   3/  20] time: 150.2002, loss: 0.02647357\n",
      "Epoch: [18] [   4/  20] time: 150.5982, loss: 0.00067129\n",
      "Epoch: [18] [   5/  20] time: 150.9940, loss: 0.01130055\n",
      "Epoch: [18] [   6/  20] time: 151.3961, loss: 0.00372473\n",
      "Epoch: [18] [   7/  20] time: 151.8086, loss: 0.00105862\n",
      "Epoch: [18] [   8/  20] time: 152.2119, loss: 0.06066129\n",
      "Epoch: [18] [   9/  20] time: 152.6145, loss: 0.00010049\n",
      "Epoch: [18] [  10/  20] time: 153.0153, loss: 0.00500064\n",
      "Epoch: [18] [  11/  20] time: 153.4127, loss: 0.00135666\n",
      "Epoch: [18] [  12/  20] time: 153.8144, loss: 0.00503237\n",
      "Epoch: [18] [  13/  20] time: 154.2145, loss: 0.07218787\n",
      "Epoch: [18] [  14/  20] time: 154.6083, loss: 0.00384936\n",
      "Epoch: [18] [  15/  20] time: 155.0068, loss: 0.03064699\n",
      "Epoch: [18] [  16/  20] time: 155.4034, loss: 0.00647913\n",
      "Epoch: [18] [  17/  20] time: 155.8006, loss: 0.01765677\n",
      "Epoch: [18] [  18/  20] time: 156.2014, loss: 0.02900527\n",
      "Epoch: [18] [  19/  20] time: 156.6016, loss: 0.04372945\n",
      "[18/50] - ptime: 8.3123 loss: 0.01914948 acc: 0.59000 lr: 0.00090000\n",
      "Epoch: [19] [   0/  20] time: 157.6644, loss: 0.00701322\n",
      "Epoch: [19] [   1/  20] time: 158.0604, loss: 0.00700942\n",
      "Epoch: [19] [   2/  20] time: 158.4587, loss: 0.00496957\n",
      "Epoch: [19] [   3/  20] time: 158.8543, loss: 0.01194359\n",
      "Epoch: [19] [   4/  20] time: 159.2511, loss: 0.00777161\n",
      "Epoch: [19] [   5/  20] time: 159.6513, loss: 0.00271077\n",
      "Epoch: [19] [   6/  20] time: 160.0506, loss: 0.00408287\n",
      "Epoch: [19] [   7/  20] time: 160.4485, loss: 0.01923190\n",
      "Epoch: [19] [   8/  20] time: 160.8437, loss: 0.00042774\n",
      "Epoch: [19] [   9/  20] time: 161.2389, loss: 0.04041596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [19] [  10/  20] time: 161.6391, loss: 0.00059184\n",
      "Epoch: [19] [  11/  20] time: 162.0278, loss: 0.02454558\n",
      "Epoch: [19] [  12/  20] time: 162.4352, loss: 0.00185023\n",
      "Epoch: [19] [  13/  20] time: 162.8309, loss: 0.00404632\n",
      "Epoch: [19] [  14/  20] time: 163.2271, loss: 0.00722813\n",
      "Epoch: [19] [  15/  20] time: 163.6271, loss: 0.00141096\n",
      "Epoch: [19] [  16/  20] time: 164.0227, loss: 0.02632907\n",
      "Epoch: [19] [  17/  20] time: 164.4248, loss: 0.00130809\n",
      "Epoch: [19] [  18/  20] time: 164.8284, loss: 0.03346567\n",
      "Epoch: [19] [  19/  20] time: 165.2249, loss: 0.00711981\n",
      "[19/50] - ptime: 8.2815 loss: 0.01067362 acc: 0.59000 lr: 0.00090000\n",
      "Epoch: [20] [   0/  20] time: 166.2944, loss: 0.01796000\n",
      "Epoch: [20] [   1/  20] time: 166.6967, loss: 0.00154205\n",
      "Epoch: [20] [   2/  20] time: 167.0997, loss: 0.00595906\n",
      "Epoch: [20] [   3/  20] time: 167.4953, loss: 0.00665877\n",
      "Epoch: [20] [   4/  20] time: 167.8897, loss: 0.00090729\n",
      "Epoch: [20] [   5/  20] time: 168.2859, loss: 0.00000515\n",
      "Epoch: [20] [   6/  20] time: 168.6897, loss: 0.00226271\n",
      "Epoch: [20] [   7/  20] time: 169.0851, loss: 0.00480308\n",
      "Epoch: [20] [   8/  20] time: 169.4845, loss: 0.01229036\n",
      "Epoch: [20] [   9/  20] time: 169.8797, loss: 0.00003626\n",
      "Epoch: [20] [  10/  20] time: 170.2810, loss: 0.01305148\n",
      "Epoch: [20] [  11/  20] time: 170.6798, loss: 0.00387700\n",
      "Epoch: [20] [  12/  20] time: 171.0784, loss: 0.00173207\n",
      "Epoch: [20] [  13/  20] time: 171.4790, loss: 0.00013767\n",
      "Epoch: [20] [  14/  20] time: 171.8739, loss: 0.00760551\n",
      "Epoch: [20] [  15/  20] time: 172.2599, loss: 0.01270739\n",
      "Epoch: [20] [  16/  20] time: 172.6612, loss: 0.00742790\n",
      "Epoch: [20] [  17/  20] time: 173.0619, loss: 0.01393427\n",
      "Epoch: [20] [  18/  20] time: 173.4586, loss: 0.00001903\n",
      "Epoch: [20] [  19/  20] time: 173.8543, loss: 0.00119627\n",
      "[20/50] - ptime: 8.2727 loss: 0.00570567 acc: 0.63000 lr: 0.00081000\n",
      "Epoch: [21] [   0/  20] time: 174.9214, loss: 0.00509178\n",
      "Epoch: [21] [   1/  20] time: 175.3179, loss: 0.00063746\n",
      "Epoch: [21] [   2/  20] time: 175.7164, loss: 0.00431314\n",
      "Epoch: [21] [   3/  20] time: 176.1055, loss: 0.00090258\n",
      "Epoch: [21] [   4/  20] time: 176.5017, loss: 0.00059233\n",
      "Epoch: [21] [   5/  20] time: 176.8988, loss: 0.00009121\n",
      "Epoch: [21] [   6/  20] time: 177.2830, loss: 0.00452369\n",
      "Epoch: [21] [   7/  20] time: 177.6828, loss: 0.00666022\n",
      "Epoch: [21] [   8/  20] time: 178.0802, loss: 0.00001250\n",
      "Epoch: [21] [   9/  20] time: 178.4758, loss: 0.00676391\n",
      "Epoch: [21] [  10/  20] time: 178.8741, loss: 0.00384265\n",
      "Epoch: [21] [  11/  20] time: 179.2708, loss: 0.00135644\n",
      "Epoch: [21] [  12/  20] time: 179.6688, loss: 0.00290026\n",
      "Epoch: [21] [  13/  20] time: 180.0672, loss: 0.00023712\n",
      "Epoch: [21] [  14/  20] time: 180.4612, loss: 0.00157927\n",
      "Epoch: [21] [  15/  20] time: 180.8564, loss: 0.00045432\n",
      "Epoch: [21] [  16/  20] time: 181.2535, loss: 0.00057784\n",
      "Epoch: [21] [  17/  20] time: 181.6529, loss: 0.00101821\n",
      "Epoch: [21] [  18/  20] time: 182.0433, loss: 0.00097126\n",
      "Epoch: [21] [  19/  20] time: 182.4413, loss: 0.00100372\n",
      "[21/50] - ptime: 8.2400 loss: 0.00217650 acc: 0.60000 lr: 0.00081000\n",
      "Epoch: [22] [   0/  20] time: 183.5165, loss: 0.00124555\n",
      "Epoch: [22] [   1/  20] time: 183.9118, loss: 0.00544264\n",
      "Epoch: [22] [   2/  20] time: 184.3075, loss: 0.00001545\n",
      "Epoch: [22] [   3/  20] time: 184.7040, loss: 0.00014378\n",
      "Epoch: [22] [   4/  20] time: 185.1008, loss: 0.00057869\n",
      "Epoch: [22] [   5/  20] time: 185.5024, loss: 0.00035355\n",
      "Epoch: [22] [   6/  20] time: 185.9020, loss: 0.00064979\n",
      "Epoch: [22] [   7/  20] time: 186.3019, loss: 0.00248394\n",
      "Epoch: [22] [   8/  20] time: 186.7031, loss: 0.00022563\n",
      "Epoch: [22] [   9/  20] time: 187.0985, loss: 0.00089511\n",
      "Epoch: [22] [  10/  20] time: 187.4948, loss: 0.00035261\n",
      "Epoch: [22] [  11/  20] time: 187.8894, loss: 0.00084876\n",
      "Epoch: [22] [  12/  20] time: 188.2921, loss: 0.00210973\n",
      "Epoch: [22] [  13/  20] time: 188.6929, loss: 0.00001495\n",
      "Epoch: [22] [  14/  20] time: 189.0900, loss: 0.00024278\n",
      "Epoch: [22] [  15/  20] time: 189.4940, loss: 0.00003573\n",
      "Epoch: [22] [  16/  20] time: 189.8960, loss: 0.00003292\n",
      "Epoch: [22] [  17/  20] time: 190.2949, loss: 0.00000894\n",
      "Epoch: [22] [  18/  20] time: 190.7045, loss: 0.00038147\n",
      "Epoch: [22] [  19/  20] time: 191.1040, loss: 0.00051828\n",
      "[22/50] - ptime: 8.3068 loss: 0.00082901 acc: 0.62000 lr: 0.00081000\n",
      "Epoch: [23] [   0/  20] time: 192.1658, loss: 0.00036227\n",
      "Epoch: [23] [   1/  20] time: 192.5622, loss: 0.00015306\n",
      "Epoch: [23] [   2/  20] time: 192.9640, loss: 0.00094462\n",
      "Epoch: [23] [   3/  20] time: 193.3654, loss: 0.00164285\n",
      "Epoch: [23] [   4/  20] time: 193.7685, loss: 0.00015735\n",
      "Epoch: [23] [   5/  20] time: 194.1633, loss: 0.01613407\n",
      "Epoch: [23] [   6/  20] time: 194.5609, loss: 0.00060863\n",
      "Epoch: [23] [   7/  20] time: 194.9629, loss: 0.00027298\n",
      "Epoch: [23] [   8/  20] time: 195.3609, loss: 0.00616491\n",
      "Epoch: [23] [   9/  20] time: 195.7568, loss: 0.00372906\n",
      "Epoch: [23] [  10/  20] time: 196.1528, loss: 0.01035985\n",
      "Epoch: [23] [  11/  20] time: 196.5500, loss: 0.02425937\n",
      "Epoch: [23] [  12/  20] time: 196.9469, loss: 0.04277031\n",
      "Epoch: [23] [  13/  20] time: 197.3432, loss: 0.00309574\n",
      "Epoch: [23] [  14/  20] time: 197.7406, loss: 0.00388710\n",
      "Epoch: [23] [  15/  20] time: 198.1393, loss: 0.00033477\n",
      "Epoch: [23] [  16/  20] time: 198.5373, loss: 0.00681073\n",
      "Epoch: [23] [  17/  20] time: 198.9355, loss: 0.00316767\n",
      "Epoch: [23] [  18/  20] time: 199.3391, loss: 0.00495282\n",
      "Epoch: [23] [  19/  20] time: 199.7403, loss: 0.00114059\n",
      "[23/50] - ptime: 8.2884 loss: 0.00654744 acc: 0.54000 lr: 0.00081000\n",
      "Epoch: [24] [   0/  20] time: 200.8041, loss: 0.01104752\n",
      "Epoch: [24] [   1/  20] time: 201.2047, loss: 0.00136724\n",
      "Epoch: [24] [   2/  20] time: 201.5944, loss: 0.00774294\n",
      "Epoch: [24] [   3/  20] time: 201.9912, loss: 0.00001989\n",
      "Epoch: [24] [   4/  20] time: 202.3877, loss: 0.00376573\n",
      "Epoch: [24] [   5/  20] time: 202.7885, loss: 0.00372266\n",
      "Epoch: [24] [   6/  20] time: 203.1871, loss: 0.00062287\n",
      "Epoch: [24] [   7/  20] time: 203.5891, loss: 0.00001003\n",
      "Epoch: [24] [   8/  20] time: 203.9880, loss: 0.00267967\n",
      "Epoch: [24] [   9/  20] time: 204.3875, loss: 0.00933071\n",
      "Epoch: [24] [  10/  20] time: 204.7786, loss: 0.00014926\n",
      "Epoch: [24] [  11/  20] time: 205.1764, loss: 0.00006112\n",
      "Epoch: [24] [  12/  20] time: 205.5781, loss: 0.00314856\n",
      "Epoch: [24] [  13/  20] time: 205.9782, loss: 0.00027006\n",
      "Epoch: [24] [  14/  20] time: 206.3785, loss: 0.00534020\n",
      "Epoch: [24] [  15/  20] time: 206.7778, loss: 0.00003286\n",
      "Epoch: [24] [  16/  20] time: 207.1764, loss: 0.00004027\n",
      "Epoch: [24] [  17/  20] time: 207.5706, loss: 0.00050215\n",
      "Epoch: [24] [  18/  20] time: 207.9661, loss: 0.00026839\n",
      "Epoch: [24] [  19/  20] time: 208.3668, loss: 0.00244755\n",
      "[24/50] - ptime: 8.2805 loss: 0.00262848 acc: 0.62000 lr: 0.00081000\n",
      "Epoch: [25] [   0/  20] time: 209.4277, loss: 0.00010871\n",
      "Epoch: [25] [   1/  20] time: 209.8251, loss: 0.00074436\n",
      "Epoch: [25] [   2/  20] time: 210.2213, loss: 0.00142600\n",
      "Epoch: [25] [   3/  20] time: 210.6164, loss: 0.00117747\n",
      "Epoch: [25] [   4/  20] time: 211.0149, loss: 0.00025562\n",
      "Epoch: [25] [   5/  20] time: 211.4108, loss: 0.00217095\n",
      "Epoch: [25] [   6/  20] time: 211.8112, loss: 0.00010536\n",
      "Epoch: [25] [   7/  20] time: 212.2090, loss: 0.00057070\n",
      "Epoch: [25] [   8/  20] time: 212.6077, loss: 0.00529835\n",
      "Epoch: [25] [   9/  20] time: 213.0045, loss: 0.00010417\n",
      "Epoch: [25] [  10/  20] time: 213.4002, loss: 0.00428193\n",
      "Epoch: [25] [  11/  20] time: 213.7967, loss: 0.00013145\n",
      "Epoch: [25] [  12/  20] time: 214.1932, loss: 0.00103577\n",
      "Epoch: [25] [  13/  20] time: 214.5900, loss: 0.00085231\n",
      "Epoch: [25] [  14/  20] time: 214.9906, loss: 0.00013433\n",
      "Epoch: [25] [  15/  20] time: 215.3879, loss: 0.00183582\n",
      "Epoch: [25] [  16/  20] time: 215.7886, loss: 0.00013964\n",
      "Epoch: [25] [  17/  20] time: 216.1871, loss: 0.00000969\n",
      "Epoch: [25] [  18/  20] time: 216.5852, loss: 0.00005026\n",
      "Epoch: [25] [  19/  20] time: 216.9830, loss: 0.00003732\n",
      "[25/50] - ptime: 8.2709 loss: 0.00102351 acc: 0.64000 lr: 0.00081000\n",
      "Epoch: [26] [   0/  20] time: 218.0483, loss: 0.00006441\n",
      "Epoch: [26] [   1/  20] time: 218.4513, loss: 0.00150204\n",
      "Epoch: [26] [   2/  20] time: 218.8482, loss: 0.00036435\n",
      "Epoch: [26] [   3/  20] time: 219.2424, loss: 0.00078399\n",
      "Epoch: [26] [   4/  20] time: 219.6415, loss: 0.00105184\n",
      "Epoch: [26] [   5/  20] time: 220.0383, loss: 0.00005774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [26] [   6/  20] time: 220.4361, loss: 0.00069824\n",
      "Epoch: [26] [   7/  20] time: 220.8320, loss: 0.00004370\n",
      "Epoch: [26] [   8/  20] time: 221.2295, loss: 0.00054956\n",
      "Epoch: [26] [   9/  20] time: 221.6270, loss: 0.00421159\n",
      "Epoch: [26] [  10/  20] time: 222.0101, loss: 0.00106613\n",
      "Epoch: [26] [  11/  20] time: 222.3992, loss: 0.00167091\n",
      "Epoch: [26] [  12/  20] time: 222.7835, loss: 0.00002996\n",
      "Epoch: [26] [  13/  20] time: 223.1792, loss: 0.00526634\n",
      "Epoch: [26] [  14/  20] time: 223.5788, loss: 0.00050604\n",
      "Epoch: [26] [  15/  20] time: 223.9746, loss: 0.00068766\n",
      "Epoch: [26] [  16/  20] time: 224.3695, loss: 0.00044894\n",
      "Epoch: [26] [  17/  20] time: 224.7662, loss: 0.02820353\n",
      "Epoch: [26] [  18/  20] time: 225.1615, loss: 0.00004946\n",
      "Epoch: [26] [  19/  20] time: 225.5497, loss: 0.02161461\n",
      "[26/50] - ptime: 8.2119 loss: 0.00344355 acc: 0.57000 lr: 0.00081000\n",
      "Epoch: [27] [   0/  20] time: 226.6091, loss: 0.00161061\n",
      "Epoch: [27] [   1/  20] time: 227.0056, loss: 0.00001493\n",
      "Epoch: [27] [   2/  20] time: 227.4053, loss: 0.01272942\n",
      "Epoch: [27] [   3/  20] time: 227.8015, loss: 0.00009306\n",
      "Epoch: [27] [   4/  20] time: 228.1982, loss: 0.00039382\n",
      "Epoch: [27] [   5/  20] time: 228.5980, loss: 0.00471043\n",
      "Epoch: [27] [   6/  20] time: 228.9968, loss: 0.00068325\n",
      "Epoch: [27] [   7/  20] time: 229.3970, loss: 0.01037053\n",
      "Epoch: [27] [   8/  20] time: 229.7939, loss: 0.00118630\n",
      "Epoch: [27] [   9/  20] time: 230.1801, loss: 0.01410594\n",
      "Epoch: [27] [  10/  20] time: 230.5795, loss: 0.00022880\n",
      "Epoch: [27] [  11/  20] time: 230.9757, loss: 0.00050407\n",
      "Epoch: [27] [  12/  20] time: 231.3788, loss: 0.01882515\n",
      "Epoch: [27] [  13/  20] time: 231.7776, loss: 0.00193439\n",
      "Epoch: [27] [  14/  20] time: 232.1798, loss: 0.03594714\n",
      "Epoch: [27] [  15/  20] time: 232.5822, loss: 0.01190845\n",
      "Epoch: [27] [  16/  20] time: 232.9819, loss: 0.00093363\n",
      "Epoch: [27] [  17/  20] time: 233.3828, loss: 0.00583947\n",
      "Epoch: [27] [  18/  20] time: 233.7828, loss: 0.02463261\n",
      "Epoch: [27] [  19/  20] time: 234.1812, loss: 0.00809991\n",
      "[27/50] - ptime: 8.2876 loss: 0.00773759 acc: 0.55000 lr: 0.00081000\n",
      "Epoch: [28] [   0/  20] time: 235.2482, loss: 0.00338986\n",
      "Epoch: [28] [   1/  20] time: 235.6477, loss: 0.00058880\n",
      "Epoch: [28] [   2/  20] time: 236.0472, loss: 0.00857663\n",
      "Epoch: [28] [   3/  20] time: 236.4418, loss: 0.00266668\n",
      "Epoch: [28] [   4/  20] time: 236.8391, loss: 0.00517151\n",
      "Epoch: [28] [   5/  20] time: 237.2237, loss: 0.00238880\n",
      "Epoch: [28] [   6/  20] time: 237.6268, loss: 0.00035408\n",
      "Epoch: [28] [   7/  20] time: 238.0245, loss: 0.00113142\n",
      "Epoch: [28] [   8/  20] time: 238.4228, loss: 0.00344242\n",
      "Epoch: [28] [   9/  20] time: 238.8188, loss: 0.00011134\n",
      "Epoch: [28] [  10/  20] time: 239.2133, loss: 0.00297423\n",
      "Epoch: [28] [  11/  20] time: 239.6093, loss: 0.00036797\n",
      "Epoch: [28] [  12/  20] time: 240.0042, loss: 0.02279002\n",
      "Epoch: [28] [  13/  20] time: 240.3993, loss: 0.00002938\n",
      "Epoch: [28] [  14/  20] time: 240.7948, loss: 0.00153222\n",
      "Epoch: [28] [  15/  20] time: 241.1908, loss: 0.10652597\n",
      "Epoch: [28] [  16/  20] time: 241.5876, loss: 0.01741446\n",
      "Epoch: [28] [  17/  20] time: 241.9887, loss: 0.04404540\n",
      "Epoch: [28] [  18/  20] time: 242.3849, loss: 0.00770157\n",
      "Epoch: [28] [  19/  20] time: 242.7810, loss: 0.01786646\n",
      "[28/50] - ptime: 8.2488 loss: 0.01245346 acc: 0.61000 lr: 0.00081000\n",
      "Epoch: [29] [   0/  20] time: 243.8455, loss: 0.00038131\n",
      "Epoch: [29] [   1/  20] time: 244.2366, loss: 0.00028866\n",
      "Epoch: [29] [   2/  20] time: 244.6326, loss: 0.00000236\n",
      "Epoch: [29] [   3/  20] time: 245.0299, loss: 0.00704734\n",
      "Epoch: [29] [   4/  20] time: 245.4279, loss: 0.01301369\n",
      "Epoch: [29] [   5/  20] time: 245.8235, loss: 0.00048781\n",
      "Epoch: [29] [   6/  20] time: 246.2180, loss: 0.00973266\n",
      "Epoch: [29] [   7/  20] time: 246.6144, loss: 0.00098977\n",
      "Epoch: [29] [   8/  20] time: 247.0117, loss: 0.01917018\n",
      "Epoch: [29] [   9/  20] time: 247.4066, loss: 0.00042051\n",
      "Epoch: [29] [  10/  20] time: 247.8059, loss: 0.00002678\n",
      "Epoch: [29] [  11/  20] time: 248.2007, loss: 0.00212264\n",
      "Epoch: [29] [  12/  20] time: 248.5961, loss: 0.02046285\n",
      "Epoch: [29] [  13/  20] time: 248.9916, loss: 0.04371221\n",
      "Epoch: [29] [  14/  20] time: 249.3891, loss: 0.00940467\n",
      "Epoch: [29] [  15/  20] time: 249.7893, loss: 0.00652572\n",
      "Epoch: [29] [  16/  20] time: 250.1887, loss: 0.01483696\n",
      "Epoch: [29] [  17/  20] time: 250.5975, loss: 0.01353823\n",
      "Epoch: [29] [  18/  20] time: 250.9954, loss: 0.01077867\n",
      "Epoch: [29] [  19/  20] time: 251.3913, loss: 0.00034967\n",
      "[29/50] - ptime: 8.2565 loss: 0.00866464 acc: 0.56000 lr: 0.00081000\n",
      "Epoch: [30] [   0/  20] time: 252.4508, loss: 0.00099038\n",
      "Epoch: [30] [   1/  20] time: 252.8479, loss: 0.00472788\n",
      "Epoch: [30] [   2/  20] time: 253.2438, loss: 0.00634909\n",
      "Epoch: [30] [   3/  20] time: 253.6405, loss: 0.00017613\n",
      "Epoch: [30] [   4/  20] time: 254.0379, loss: 0.01386638\n",
      "Epoch: [30] [   5/  20] time: 254.4402, loss: 0.00345910\n",
      "Epoch: [30] [   6/  20] time: 254.8392, loss: 0.00078836\n",
      "Epoch: [30] [   7/  20] time: 255.2345, loss: 0.00266260\n",
      "Epoch: [30] [   8/  20] time: 255.6311, loss: 0.00154263\n",
      "Epoch: [30] [   9/  20] time: 256.0323, loss: 0.00256021\n",
      "Epoch: [30] [  10/  20] time: 256.4335, loss: 0.00004145\n",
      "Epoch: [30] [  11/  20] time: 256.8351, loss: 0.00206934\n",
      "Epoch: [30] [  12/  20] time: 257.2345, loss: 0.00659839\n",
      "Epoch: [30] [  13/  20] time: 257.6368, loss: 0.00052047\n",
      "Epoch: [30] [  14/  20] time: 258.0377, loss: 0.00170454\n",
      "Epoch: [30] [  15/  20] time: 258.4378, loss: 0.00040581\n",
      "Epoch: [30] [  16/  20] time: 258.8564, loss: 0.00196916\n",
      "Epoch: [30] [  17/  20] time: 259.2617, loss: 0.01698860\n",
      "Epoch: [30] [  18/  20] time: 259.6582, loss: 0.00011773\n",
      "Epoch: [30] [  19/  20] time: 260.0539, loss: 0.00177104\n",
      "[30/50] - ptime: 8.3126 loss: 0.00346546 acc: 0.60000 lr: 0.00072900\n",
      "Epoch: [31] [   0/  20] time: 261.1139, loss: 0.00098076\n",
      "Epoch: [31] [   1/  20] time: 261.5141, loss: 0.00008456\n",
      "Epoch: [31] [   2/  20] time: 261.9099, loss: 0.00109184\n",
      "Epoch: [31] [   3/  20] time: 262.2949, loss: 0.00132494\n",
      "Epoch: [31] [   4/  20] time: 262.6888, loss: 0.00444624\n",
      "Epoch: [31] [   5/  20] time: 263.0852, loss: 0.00188665\n",
      "Epoch: [31] [   6/  20] time: 263.4837, loss: 0.00082364\n",
      "Epoch: [31] [   7/  20] time: 263.8825, loss: 0.00419847\n",
      "Epoch: [31] [   8/  20] time: 264.2773, loss: 0.00003425\n",
      "Epoch: [31] [   9/  20] time: 264.6747, loss: 0.00010724\n",
      "Epoch: [31] [  10/  20] time: 265.0730, loss: 0.00022117\n",
      "Epoch: [31] [  11/  20] time: 265.4691, loss: 0.00001395\n",
      "Epoch: [31] [  12/  20] time: 265.8655, loss: 0.00083058\n",
      "Epoch: [31] [  13/  20] time: 266.2594, loss: 0.00287858\n",
      "Epoch: [31] [  14/  20] time: 266.6548, loss: 0.00075379\n",
      "Epoch: [31] [  15/  20] time: 267.0530, loss: 0.00092336\n",
      "Epoch: [31] [  16/  20] time: 267.4493, loss: 0.00063177\n",
      "Epoch: [31] [  17/  20] time: 267.8442, loss: 0.00162169\n",
      "Epoch: [31] [  18/  20] time: 268.2400, loss: 0.00029601\n",
      "Epoch: [31] [  19/  20] time: 268.6420, loss: 0.00012097\n",
      "[31/50] - ptime: 8.2431 loss: 0.00116352 acc: 0.61000 lr: 0.00072900\n",
      "Epoch: [32] [   0/  20] time: 269.8332, loss: 0.00047860\n",
      "Epoch: [32] [   1/  20] time: 270.2298, loss: 0.00056059\n",
      "Epoch: [32] [   2/  20] time: 270.6287, loss: 0.00094553\n",
      "Epoch: [32] [   3/  20] time: 271.0258, loss: 0.00016382\n",
      "Epoch: [32] [   4/  20] time: 271.4248, loss: 0.00002951\n",
      "Epoch: [32] [   5/  20] time: 271.8180, loss: 0.00002733\n",
      "Epoch: [32] [   6/  20] time: 272.2036, loss: 0.00000658\n",
      "Epoch: [32] [   7/  20] time: 272.6086, loss: 0.00067554\n",
      "Epoch: [32] [   8/  20] time: 273.0117, loss: 0.00067901\n",
      "Epoch: [32] [   9/  20] time: 273.4100, loss: 0.00063868\n",
      "Epoch: [32] [  10/  20] time: 273.8074, loss: 0.00100091\n",
      "Epoch: [32] [  11/  20] time: 274.2055, loss: 0.00031328\n",
      "Epoch: [32] [  12/  20] time: 274.6083, loss: 0.00007866\n",
      "Epoch: [32] [  13/  20] time: 275.0027, loss: 0.00000411\n",
      "Epoch: [32] [  14/  20] time: 275.3981, loss: 0.00113052\n",
      "Epoch: [32] [  15/  20] time: 275.7971, loss: 0.00020165\n",
      "Epoch: [32] [  16/  20] time: 276.1920, loss: 0.00609431\n",
      "Epoch: [32] [  17/  20] time: 276.5889, loss: 0.00012730\n",
      "Epoch: [32] [  18/  20] time: 276.9858, loss: 0.00757013\n",
      "Epoch: [32] [  19/  20] time: 277.3813, loss: 0.00045693\n",
      "[32/50] - ptime: 8.2619 loss: 0.00105915 acc: 0.56000 lr: 0.00072900\n",
      "Epoch: [33] [   0/  20] time: 278.4412, loss: 0.00008264\n",
      "Epoch: [33] [   1/  20] time: 278.8467, loss: 0.00079625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [33] [   2/  20] time: 279.2461, loss: 0.00178625\n",
      "Epoch: [33] [   3/  20] time: 279.6445, loss: 0.00038757\n",
      "Epoch: [33] [   4/  20] time: 280.0405, loss: 0.00498284\n",
      "Epoch: [33] [   5/  20] time: 280.4370, loss: 0.00106574\n",
      "Epoch: [33] [   6/  20] time: 280.8340, loss: 0.00106844\n",
      "Epoch: [33] [   7/  20] time: 281.2300, loss: 0.00009658\n",
      "Epoch: [33] [   8/  20] time: 281.6339, loss: 0.00012258\n",
      "Epoch: [33] [   9/  20] time: 282.0286, loss: 0.00016265\n",
      "Epoch: [33] [  10/  20] time: 282.4227, loss: 0.00037253\n",
      "Epoch: [33] [  11/  20] time: 282.8188, loss: 0.00065994\n",
      "Epoch: [33] [  12/  20] time: 283.2137, loss: 0.00076691\n",
      "Epoch: [33] [  13/  20] time: 283.6087, loss: 0.00001889\n",
      "Epoch: [33] [  14/  20] time: 284.0037, loss: 0.00170172\n",
      "Epoch: [33] [  15/  20] time: 284.3985, loss: 0.00054808\n",
      "Epoch: [33] [  16/  20] time: 284.7925, loss: 0.00006413\n",
      "Epoch: [33] [  17/  20] time: 285.1868, loss: 0.00011419\n",
      "Epoch: [33] [  18/  20] time: 285.5855, loss: 0.00542350\n",
      "Epoch: [33] [  19/  20] time: 285.9797, loss: 0.00093386\n",
      "[33/50] - ptime: 8.2474 loss: 0.00105776 acc: 0.58000 lr: 0.00072900\n",
      "Epoch: [34] [   0/  20] time: 287.0320, loss: 0.00015125\n",
      "Epoch: [34] [   1/  20] time: 287.4280, loss: 0.00016182\n",
      "Epoch: [34] [   2/  20] time: 287.8235, loss: 0.00003993\n",
      "Epoch: [34] [   3/  20] time: 288.2174, loss: 0.00122191\n",
      "Epoch: [34] [   4/  20] time: 288.6140, loss: 0.00081601\n",
      "Epoch: [34] [   5/  20] time: 289.0098, loss: 0.00366668\n",
      "Epoch: [34] [   6/  20] time: 289.4066, loss: 0.00000059\n",
      "Epoch: [34] [   7/  20] time: 289.8017, loss: 0.00000115\n",
      "Epoch: [34] [   8/  20] time: 290.1982, loss: 0.00197541\n",
      "Epoch: [34] [   9/  20] time: 290.5976, loss: 0.00018204\n",
      "Epoch: [34] [  10/  20] time: 290.9954, loss: 0.00000554\n",
      "Epoch: [34] [  11/  20] time: 291.3914, loss: 0.00014539\n",
      "Epoch: [34] [  12/  20] time: 291.7853, loss: 0.00017889\n",
      "Epoch: [34] [  13/  20] time: 292.1816, loss: 0.00001559\n",
      "Epoch: [34] [  14/  20] time: 292.5757, loss: 0.00048841\n",
      "Epoch: [34] [  15/  20] time: 292.9693, loss: 0.00004081\n",
      "Epoch: [34] [  16/  20] time: 293.3647, loss: 0.00045720\n",
      "Epoch: [34] [  17/  20] time: 293.7624, loss: 0.00056767\n",
      "Epoch: [34] [  18/  20] time: 294.1600, loss: 0.00001249\n",
      "Epoch: [34] [  19/  20] time: 294.5553, loss: 0.00000878\n",
      "[34/50] - ptime: 8.2325 loss: 0.00050688 acc: 0.57000 lr: 0.00072900\n",
      "Epoch: [35] [   0/  20] time: 295.6164, loss: 0.00000219\n",
      "Epoch: [35] [   1/  20] time: 296.0129, loss: 0.00177828\n",
      "Epoch: [35] [   2/  20] time: 296.4086, loss: 0.00014980\n",
      "Epoch: [35] [   3/  20] time: 296.8051, loss: 0.00032405\n",
      "Epoch: [35] [   4/  20] time: 297.2016, loss: 0.00037308\n",
      "Epoch: [35] [   5/  20] time: 297.5953, loss: 0.00012451\n",
      "Epoch: [35] [   6/  20] time: 297.9905, loss: 0.00001605\n",
      "Epoch: [35] [   7/  20] time: 298.3914, loss: 0.00005613\n",
      "Epoch: [35] [   8/  20] time: 298.7868, loss: 0.00012835\n",
      "Epoch: [35] [   9/  20] time: 299.1841, loss: 0.00000065\n",
      "Epoch: [35] [  10/  20] time: 299.5798, loss: 0.00044351\n",
      "Epoch: [35] [  11/  20] time: 299.9749, loss: 0.00221494\n",
      "Epoch: [35] [  12/  20] time: 300.3687, loss: 0.00034710\n",
      "Epoch: [35] [  13/  20] time: 300.7679, loss: 0.00056288\n",
      "Epoch: [35] [  14/  20] time: 301.1643, loss: 0.00009901\n",
      "Epoch: [35] [  15/  20] time: 301.5600, loss: 0.00000293\n",
      "Epoch: [35] [  16/  20] time: 301.9556, loss: 0.00020092\n",
      "Epoch: [35] [  17/  20] time: 302.3520, loss: 0.00059142\n",
      "Epoch: [35] [  18/  20] time: 302.7366, loss: 0.00000283\n",
      "Epoch: [35] [  19/  20] time: 303.1379, loss: 0.00011005\n",
      "[35/50] - ptime: 8.2326 loss: 0.00037644 acc: 0.57000 lr: 0.00072900\n",
      "Epoch: [36] [   0/  20] time: 304.2053, loss: 0.00004701\n",
      "Epoch: [36] [   1/  20] time: 304.5992, loss: 0.00003349\n",
      "Epoch: [36] [   2/  20] time: 304.9937, loss: 0.00002932\n",
      "Epoch: [36] [   3/  20] time: 305.4133, loss: 0.00001285\n",
      "Epoch: [36] [   4/  20] time: 305.8133, loss: 0.00012753\n",
      "Epoch: [36] [   5/  20] time: 306.2083, loss: 0.00003904\n",
      "Epoch: [36] [   6/  20] time: 306.6047, loss: 0.00016047\n",
      "Epoch: [36] [   7/  20] time: 307.0027, loss: 0.00004057\n",
      "Epoch: [36] [   8/  20] time: 307.3989, loss: 0.00009094\n",
      "Epoch: [36] [   9/  20] time: 307.7936, loss: 0.00011009\n",
      "Epoch: [36] [  10/  20] time: 308.1962, loss: 0.00009639\n",
      "Epoch: [36] [  11/  20] time: 308.5952, loss: 0.00003964\n",
      "Epoch: [36] [  12/  20] time: 308.9958, loss: 0.00008668\n",
      "Epoch: [36] [  13/  20] time: 309.3907, loss: 0.00000385\n",
      "Epoch: [36] [  14/  20] time: 309.7863, loss: 0.00017054\n",
      "Epoch: [36] [  15/  20] time: 310.1798, loss: 0.00012087\n",
      "Epoch: [36] [  16/  20] time: 310.5768, loss: 0.00001135\n",
      "Epoch: [36] [  17/  20] time: 310.9717, loss: 0.00003013\n",
      "Epoch: [36] [  18/  20] time: 311.3695, loss: 0.00047101\n",
      "Epoch: [36] [  19/  20] time: 311.7673, loss: 0.00004973\n",
      "[36/50] - ptime: 8.2723 loss: 0.00008857 acc: 0.56000 lr: 0.00072900\n",
      "Epoch: [37] [   0/  20] time: 312.8270, loss: 0.00000488\n",
      "Epoch: [37] [   1/  20] time: 313.2225, loss: 0.00027781\n",
      "Epoch: [37] [   2/  20] time: 313.6177, loss: 0.00000407\n",
      "Epoch: [37] [   3/  20] time: 314.0127, loss: 0.00008458\n",
      "Epoch: [37] [   4/  20] time: 314.4066, loss: 0.00010369\n",
      "Epoch: [37] [   5/  20] time: 314.8012, loss: 0.00001650\n",
      "Epoch: [37] [   6/  20] time: 315.1960, loss: 0.00001759\n",
      "Epoch: [37] [   7/  20] time: 315.5919, loss: 0.00000006\n",
      "Epoch: [37] [   8/  20] time: 315.9896, loss: 0.00013871\n",
      "Epoch: [37] [   9/  20] time: 316.3877, loss: 0.00010951\n",
      "Epoch: [37] [  10/  20] time: 316.7824, loss: 0.00027595\n",
      "Epoch: [37] [  11/  20] time: 317.1761, loss: 0.00048396\n",
      "Epoch: [37] [  12/  20] time: 317.5725, loss: 0.00009694\n",
      "Epoch: [37] [  13/  20] time: 317.9711, loss: 0.00027643\n",
      "Epoch: [37] [  14/  20] time: 318.3694, loss: 0.00008081\n",
      "Epoch: [37] [  15/  20] time: 318.7718, loss: 0.00006572\n",
      "Epoch: [37] [  16/  20] time: 319.1728, loss: 0.00029770\n",
      "Epoch: [37] [  17/  20] time: 319.5689, loss: 0.00005138\n",
      "Epoch: [37] [  18/  20] time: 319.9642, loss: 0.00005071\n",
      "Epoch: [37] [  19/  20] time: 320.3606, loss: 0.00000025\n",
      "[37/50] - ptime: 8.2463 loss: 0.00012186 acc: 0.57000 lr: 0.00072900\n",
      "Epoch: [38] [   0/  20] time: 321.4196, loss: 0.00000249\n",
      "Epoch: [38] [   1/  20] time: 321.8158, loss: 0.00000452\n",
      "Epoch: [38] [   2/  20] time: 322.2095, loss: 0.00023783\n",
      "Epoch: [38] [   3/  20] time: 322.6038, loss: 0.00062422\n",
      "Epoch: [38] [   4/  20] time: 322.9995, loss: 0.00005618\n",
      "Epoch: [38] [   5/  20] time: 323.3983, loss: 0.00118878\n",
      "Epoch: [38] [   6/  20] time: 323.7952, loss: 0.00000819\n",
      "Epoch: [38] [   7/  20] time: 324.1916, loss: 0.00000660\n",
      "Epoch: [38] [   8/  20] time: 324.5910, loss: 0.00008332\n",
      "Epoch: [38] [   9/  20] time: 324.9893, loss: 0.00000913\n",
      "Epoch: [38] [  10/  20] time: 325.3841, loss: 0.00009029\n",
      "Epoch: [38] [  11/  20] time: 325.7773, loss: 0.00003037\n",
      "Epoch: [38] [  12/  20] time: 326.1704, loss: 0.00000672\n",
      "Epoch: [38] [  13/  20] time: 326.5644, loss: 0.00000380\n",
      "Epoch: [38] [  14/  20] time: 326.9601, loss: 0.00005036\n",
      "Epoch: [38] [  15/  20] time: 327.3554, loss: 0.00008421\n",
      "Epoch: [38] [  16/  20] time: 327.7500, loss: 0.00006339\n",
      "Epoch: [38] [  17/  20] time: 328.1511, loss: 0.00003739\n",
      "Epoch: [38] [  18/  20] time: 328.5440, loss: 0.00001587\n",
      "Epoch: [38] [  19/  20] time: 328.9408, loss: 0.00064988\n",
      "[38/50] - ptime: 8.2302 loss: 0.00016267 acc: 0.57000 lr: 0.00072900\n",
      "Epoch: [39] [   0/  20] time: 329.9931, loss: 0.00000714\n",
      "Epoch: [39] [   1/  20] time: 330.3873, loss: 0.00000296\n",
      "Epoch: [39] [   2/  20] time: 330.7865, loss: 0.00000006\n",
      "Epoch: [39] [   3/  20] time: 331.1848, loss: 0.00024991\n",
      "Epoch: [39] [   4/  20] time: 331.5858, loss: 0.00145401\n",
      "Epoch: [39] [   5/  20] time: 331.9872, loss: 0.00000592\n",
      "Epoch: [39] [   6/  20] time: 332.3874, loss: 0.00001565\n",
      "Epoch: [39] [   7/  20] time: 332.7809, loss: 0.00004112\n",
      "Epoch: [39] [   8/  20] time: 333.1752, loss: 0.00009208\n",
      "Epoch: [39] [   9/  20] time: 333.5703, loss: 0.00058067\n",
      "Epoch: [39] [  10/  20] time: 333.9703, loss: 0.00006166\n",
      "Epoch: [39] [  11/  20] time: 334.3704, loss: 0.00015460\n",
      "Epoch: [39] [  12/  20] time: 334.7706, loss: 0.00006107\n",
      "Epoch: [39] [  13/  20] time: 335.1694, loss: 0.00008210\n",
      "Epoch: [39] [  14/  20] time: 335.5643, loss: 0.00008768\n",
      "Epoch: [39] [  15/  20] time: 335.9820, loss: 0.00006419\n",
      "Epoch: [39] [  16/  20] time: 336.3904, loss: 0.00004323\n",
      "Epoch: [39] [  17/  20] time: 336.7998, loss: 0.00000843\n",
      "Epoch: [39] [  18/  20] time: 337.1923, loss: 0.00002348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [39] [  19/  20] time: 337.5905, loss: 0.00011917\n",
      "[39/50] - ptime: 8.3098 loss: 0.00015776 acc: 0.56000 lr: 0.00072900\n",
      "Epoch: [40] [   0/  20] time: 338.6793, loss: 0.00001455\n",
      "Epoch: [40] [   1/  20] time: 339.0758, loss: 0.00005765\n",
      "Epoch: [40] [   2/  20] time: 339.4705, loss: 0.00004717\n",
      "Epoch: [40] [   3/  20] time: 339.8641, loss: 0.00008811\n",
      "Epoch: [40] [   4/  20] time: 340.2603, loss: 0.00000447\n",
      "Epoch: [40] [   5/  20] time: 340.6570, loss: 0.00000724\n",
      "Epoch: [40] [   6/  20] time: 341.0567, loss: 0.00011518\n",
      "Epoch: [40] [   7/  20] time: 341.4522, loss: 0.00001452\n",
      "Epoch: [40] [   8/  20] time: 341.8527, loss: 0.00017789\n",
      "Epoch: [40] [   9/  20] time: 342.2485, loss: 0.00000874\n",
      "Epoch: [40] [  10/  20] time: 342.6438, loss: 0.00009174\n",
      "Epoch: [40] [  11/  20] time: 343.0388, loss: 0.00000884\n",
      "Epoch: [40] [  12/  20] time: 343.4332, loss: 0.00000441\n",
      "Epoch: [40] [  13/  20] time: 343.8269, loss: 0.00001130\n",
      "Epoch: [40] [  14/  20] time: 344.2276, loss: 0.00021691\n",
      "Epoch: [40] [  15/  20] time: 344.6241, loss: 0.00005242\n",
      "Epoch: [40] [  16/  20] time: 345.0190, loss: 0.00000072\n",
      "Epoch: [40] [  17/  20] time: 345.4137, loss: 0.00003114\n",
      "Epoch: [40] [  18/  20] time: 345.8102, loss: 0.00001080\n",
      "Epoch: [40] [  19/  20] time: 346.2068, loss: 0.00005785\n",
      "[40/50] - ptime: 8.2364 loss: 0.00005108 acc: 0.56000 lr: 0.00065610\n",
      "Epoch: [41] [   0/  20] time: 347.2976, loss: 0.00006106\n",
      "Epoch: [41] [   1/  20] time: 347.6936, loss: 0.00002237\n",
      "Epoch: [41] [   2/  20] time: 348.0894, loss: 0.00000484\n",
      "Epoch: [41] [   3/  20] time: 348.4857, loss: 0.00010530\n",
      "Epoch: [41] [   4/  20] time: 348.8794, loss: 0.00001641\n",
      "Epoch: [41] [   5/  20] time: 349.2741, loss: 0.00038431\n",
      "Epoch: [41] [   6/  20] time: 349.6730, loss: 0.00008231\n",
      "Epoch: [41] [   7/  20] time: 350.0686, loss: 0.00000922\n",
      "Epoch: [41] [   8/  20] time: 350.4636, loss: 0.00010776\n",
      "Epoch: [41] [   9/  20] time: 350.8584, loss: 0.00012888\n",
      "Epoch: [41] [  10/  20] time: 351.2540, loss: 0.00165641\n",
      "Epoch: [41] [  11/  20] time: 351.6513, loss: 0.00003693\n",
      "Epoch: [41] [  12/  20] time: 352.0465, loss: 0.00028793\n",
      "Epoch: [41] [  13/  20] time: 352.4417, loss: 0.00000631\n",
      "Epoch: [41] [  14/  20] time: 352.8365, loss: 0.00053614\n",
      "Epoch: [41] [  15/  20] time: 353.2521, loss: 0.00000311\n",
      "Epoch: [41] [  16/  20] time: 353.6489, loss: 0.00003728\n",
      "Epoch: [41] [  17/  20] time: 354.0494, loss: 0.00002739\n",
      "Epoch: [41] [  18/  20] time: 354.4456, loss: 0.00001550\n",
      "Epoch: [41] [  19/  20] time: 354.8391, loss: 0.00000077\n",
      "[41/50] - ptime: 8.2517 loss: 0.00017651 acc: 0.57000 lr: 0.00065610\n",
      "Epoch: [42] [   0/  20] time: 355.9012, loss: 0.00000793\n",
      "Epoch: [42] [   1/  20] time: 356.2990, loss: 0.00001280\n",
      "Epoch: [42] [   2/  20] time: 356.6948, loss: 0.00039101\n",
      "Epoch: [42] [   3/  20] time: 357.0886, loss: 0.00003825\n",
      "Epoch: [42] [   4/  20] time: 357.4876, loss: 0.00000012\n",
      "Epoch: [42] [   5/  20] time: 357.8879, loss: 0.00000310\n",
      "Epoch: [42] [   6/  20] time: 358.3048, loss: 0.00001619\n",
      "Epoch: [42] [   7/  20] time: 358.7191, loss: 0.00000216\n",
      "Epoch: [42] [   8/  20] time: 359.1152, loss: 0.00137570\n",
      "Epoch: [42] [   9/  20] time: 359.5119, loss: 0.00000576\n",
      "Epoch: [42] [  10/  20] time: 359.9110, loss: 0.00002064\n",
      "Epoch: [42] [  11/  20] time: 360.3055, loss: 0.00000843\n",
      "Epoch: [42] [  12/  20] time: 360.7075, loss: 0.00000298\n",
      "Epoch: [42] [  13/  20] time: 361.1065, loss: 0.00021553\n",
      "Epoch: [42] [  14/  20] time: 361.5078, loss: 0.00007317\n",
      "Epoch: [42] [  15/  20] time: 361.9041, loss: 0.00000528\n",
      "Epoch: [42] [  16/  20] time: 362.3016, loss: 0.00013754\n",
      "Epoch: [42] [  17/  20] time: 362.7001, loss: 0.00010435\n",
      "Epoch: [42] [  18/  20] time: 363.0966, loss: 0.00000929\n",
      "Epoch: [42] [  19/  20] time: 363.4926, loss: 0.00991790\n",
      "[42/50] - ptime: 8.3004 loss: 0.00061741 acc: 0.53000 lr: 0.00065610\n",
      "Epoch: [43] [   0/  20] time: 364.5411, loss: 0.00216911\n",
      "Epoch: [43] [   1/  20] time: 364.9359, loss: 0.00132606\n",
      "Epoch: [43] [   2/  20] time: 365.3287, loss: 0.00001359\n",
      "Epoch: [43] [   3/  20] time: 365.7231, loss: 0.00011358\n",
      "Epoch: [43] [   4/  20] time: 366.1198, loss: 0.00570673\n",
      "Epoch: [43] [   5/  20] time: 366.5152, loss: 0.00000564\n",
      "Epoch: [43] [   6/  20] time: 366.9097, loss: 0.00026228\n",
      "Epoch: [43] [   7/  20] time: 367.3060, loss: 0.00000275\n",
      "Epoch: [43] [   8/  20] time: 367.7079, loss: 0.00011875\n",
      "Epoch: [43] [   9/  20] time: 368.1038, loss: 0.00003283\n",
      "Epoch: [43] [  10/  20] time: 368.4983, loss: 0.00056205\n",
      "Epoch: [43] [  11/  20] time: 368.8908, loss: 0.00264660\n",
      "Epoch: [43] [  12/  20] time: 369.2844, loss: 0.00140047\n",
      "Epoch: [43] [  13/  20] time: 369.6805, loss: 0.00000549\n",
      "Epoch: [43] [  14/  20] time: 370.0766, loss: 0.00171730\n",
      "Epoch: [43] [  15/  20] time: 370.4722, loss: 0.00020017\n",
      "Epoch: [43] [  16/  20] time: 370.8679, loss: 0.00300843\n",
      "Epoch: [43] [  17/  20] time: 371.2642, loss: 0.00000635\n",
      "Epoch: [43] [  18/  20] time: 371.6676, loss: 0.00004218\n",
      "Epoch: [43] [  19/  20] time: 372.0619, loss: 0.00010798\n",
      "[43/50] - ptime: 8.2274 loss: 0.00097242 acc: 0.52000 lr: 0.00065610\n",
      "Epoch: [44] [   0/  20] time: 373.1143, loss: 0.00195194\n",
      "Epoch: [44] [   1/  20] time: 373.5079, loss: 0.00067685\n",
      "Epoch: [44] [   2/  20] time: 373.9047, loss: 0.00023818\n",
      "Epoch: [44] [   3/  20] time: 374.2899, loss: 0.00003776\n",
      "Epoch: [44] [   4/  20] time: 374.6889, loss: 0.00000265\n",
      "Epoch: [44] [   5/  20] time: 375.0840, loss: 0.00009501\n",
      "Epoch: [44] [   6/  20] time: 375.4801, loss: 0.00028368\n",
      "Epoch: [44] [   7/  20] time: 375.8730, loss: 0.00007749\n",
      "Epoch: [44] [   8/  20] time: 376.2662, loss: 0.00001761\n",
      "Epoch: [44] [   9/  20] time: 376.6622, loss: 0.00000966\n",
      "Epoch: [44] [  10/  20] time: 377.0555, loss: 0.00000223\n",
      "Epoch: [44] [  11/  20] time: 377.4497, loss: 0.00001327\n",
      "Epoch: [44] [  12/  20] time: 377.8439, loss: 0.00025955\n",
      "Epoch: [44] [  13/  20] time: 378.2383, loss: 0.00000369\n",
      "Epoch: [44] [  14/  20] time: 378.6351, loss: 0.00000099\n",
      "Epoch: [44] [  15/  20] time: 379.0298, loss: 0.00000774\n",
      "Epoch: [44] [  16/  20] time: 379.4262, loss: 0.00000892\n",
      "Epoch: [44] [  17/  20] time: 379.8202, loss: 0.00110320\n",
      "Epoch: [44] [  18/  20] time: 380.2142, loss: 0.00000345\n",
      "Epoch: [44] [  19/  20] time: 380.6106, loss: 0.00002268\n",
      "[44/50] - ptime: 8.2038 loss: 0.00024083 acc: 0.51000 lr: 0.00065610\n",
      "Epoch: [45] [   0/  20] time: 381.6892, loss: 0.00000525\n",
      "Epoch: [45] [   1/  20] time: 382.0879, loss: 0.00000420\n",
      "Epoch: [45] [   2/  20] time: 382.4833, loss: 0.00005340\n",
      "Epoch: [45] [   3/  20] time: 382.8773, loss: 0.00003698\n",
      "Epoch: [45] [   4/  20] time: 383.2697, loss: 0.00000277\n",
      "Epoch: [45] [   5/  20] time: 383.6650, loss: 0.00015327\n",
      "Epoch: [45] [   6/  20] time: 384.0601, loss: 0.00000699\n",
      "Epoch: [45] [   7/  20] time: 384.4529, loss: 0.00034079\n",
      "Epoch: [45] [   8/  20] time: 384.8464, loss: 0.00000416\n",
      "Epoch: [45] [   9/  20] time: 385.2397, loss: 0.00000053\n",
      "Epoch: [45] [  10/  20] time: 385.6352, loss: 0.00000231\n",
      "Epoch: [45] [  11/  20] time: 386.0322, loss: 0.00003765\n",
      "Epoch: [45] [  12/  20] time: 386.4323, loss: 0.00019969\n",
      "Epoch: [45] [  13/  20] time: 386.8305, loss: 0.00001219\n",
      "Epoch: [45] [  14/  20] time: 387.2263, loss: 0.00000005\n",
      "Epoch: [45] [  15/  20] time: 387.6291, loss: 0.00013429\n",
      "Epoch: [45] [  16/  20] time: 388.0280, loss: 0.00000222\n",
      "Epoch: [45] [  17/  20] time: 388.4248, loss: 0.00010935\n",
      "Epoch: [45] [  18/  20] time: 388.8227, loss: 0.00008399\n",
      "Epoch: [45] [  19/  20] time: 389.2200, loss: 0.00000180\n",
      "[45/50] - ptime: 8.2397 loss: 0.00005959 acc: 0.52000 lr: 0.00065610\n",
      "Epoch: [46] [   0/  20] time: 390.2718, loss: 0.00003702\n",
      "Epoch: [46] [   1/  20] time: 390.6659, loss: 0.00000649\n",
      "Epoch: [46] [   2/  20] time: 391.0626, loss: 0.00002537\n",
      "Epoch: [46] [   3/  20] time: 391.4589, loss: 0.00000503\n",
      "Epoch: [46] [   4/  20] time: 391.8529, loss: 0.00019225\n",
      "Epoch: [46] [   5/  20] time: 392.2471, loss: 0.00039359\n",
      "Epoch: [46] [   6/  20] time: 392.6417, loss: 0.00003809\n",
      "Epoch: [46] [   7/  20] time: 393.0369, loss: 0.00002020\n",
      "Epoch: [46] [   8/  20] time: 393.4336, loss: 0.00003133\n",
      "Epoch: [46] [   9/  20] time: 393.8299, loss: 0.00006408\n",
      "Epoch: [46] [  10/  20] time: 394.2228, loss: 0.00001498\n",
      "Epoch: [46] [  11/  20] time: 394.6187, loss: 0.00002870\n",
      "Epoch: [46] [  12/  20] time: 395.0180, loss: 0.00001172\n",
      "Epoch: [46] [  13/  20] time: 395.4141, loss: 0.00011214\n",
      "Epoch: [46] [  14/  20] time: 395.8117, loss: 0.00000068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46] [  15/  20] time: 396.2101, loss: 0.00001864\n",
      "Epoch: [46] [  16/  20] time: 396.6049, loss: 0.00001300\n",
      "Epoch: [46] [  17/  20] time: 396.9914, loss: 0.00000419\n",
      "Epoch: [46] [  18/  20] time: 397.3736, loss: 0.00000306\n",
      "Epoch: [46] [  19/  20] time: 397.7710, loss: 0.00000646\n",
      "[46/50] - ptime: 8.2079 loss: 0.00005135 acc: 0.51000 lr: 0.00065610\n",
      "Epoch: [47] [   0/  20] time: 398.8228, loss: 0.00000001\n",
      "Epoch: [47] [   1/  20] time: 399.2238, loss: 0.00000087\n",
      "Epoch: [47] [   2/  20] time: 399.6265, loss: 0.00036327\n",
      "Epoch: [47] [   3/  20] time: 400.0236, loss: 0.00004539\n",
      "Epoch: [47] [   4/  20] time: 400.4170, loss: 0.00000440\n",
      "Epoch: [47] [   5/  20] time: 400.8112, loss: 0.00011001\n",
      "Epoch: [47] [   6/  20] time: 401.2113, loss: 0.00031850\n",
      "Epoch: [47] [   7/  20] time: 401.6141, loss: 0.00000004\n",
      "Epoch: [47] [   8/  20] time: 402.0116, loss: 0.00001933\n",
      "Epoch: [47] [   9/  20] time: 402.4091, loss: 0.00001315\n",
      "Epoch: [47] [  10/  20] time: 402.8223, loss: 0.00003156\n",
      "Epoch: [47] [  11/  20] time: 403.2172, loss: 0.00000181\n",
      "Epoch: [47] [  12/  20] time: 403.6124, loss: 0.00010847\n",
      "Epoch: [47] [  13/  20] time: 404.0057, loss: 0.00064467\n",
      "Epoch: [47] [  14/  20] time: 404.4019, loss: 0.00002089\n",
      "Epoch: [47] [  15/  20] time: 404.8210, loss: 0.00000619\n",
      "Epoch: [47] [  16/  20] time: 405.2389, loss: 0.00000414\n",
      "Epoch: [47] [  17/  20] time: 405.6495, loss: 0.00006911\n",
      "Epoch: [47] [  18/  20] time: 406.0404, loss: 0.00001126\n",
      "Epoch: [47] [  19/  20] time: 406.4349, loss: 0.00001706\n",
      "[47/50] - ptime: 8.3231 loss: 0.00008951 acc: 0.52000 lr: 0.00065610\n",
      "Epoch: [48] [   0/  20] time: 407.4887, loss: 0.00013688\n",
      "Epoch: [48] [   1/  20] time: 407.8840, loss: 0.00008041\n",
      "Epoch: [48] [   2/  20] time: 408.2809, loss: 0.00000182\n",
      "Epoch: [48] [   3/  20] time: 408.6770, loss: 0.00007156\n",
      "Epoch: [48] [   4/  20] time: 409.0751, loss: 0.00000531\n",
      "Epoch: [48] [   5/  20] time: 409.4723, loss: 0.00001280\n",
      "Epoch: [48] [   6/  20] time: 409.8722, loss: 0.00000021\n",
      "Epoch: [48] [   7/  20] time: 410.2690, loss: 0.00005267\n",
      "Epoch: [48] [   8/  20] time: 410.6680, loss: 0.00046417\n",
      "Epoch: [48] [   9/  20] time: 411.0627, loss: 0.00000750\n",
      "Epoch: [48] [  10/  20] time: 411.4589, loss: 0.00000513\n",
      "Epoch: [48] [  11/  20] time: 411.8442, loss: 0.00039773\n",
      "Epoch: [48] [  12/  20] time: 412.2386, loss: 0.00008830\n",
      "Epoch: [48] [  13/  20] time: 412.6339, loss: 0.00000970\n",
      "Epoch: [48] [  14/  20] time: 413.0284, loss: 0.00005997\n",
      "Epoch: [48] [  15/  20] time: 413.4227, loss: 0.00017315\n",
      "Epoch: [48] [  16/  20] time: 413.8184, loss: 0.00006154\n",
      "Epoch: [48] [  17/  20] time: 414.2120, loss: 0.00001384\n",
      "Epoch: [48] [  18/  20] time: 414.6080, loss: 0.00002730\n",
      "Epoch: [48] [  19/  20] time: 415.0035, loss: 0.00009832\n",
      "[48/50] - ptime: 8.2209 loss: 0.00008842 acc: 0.52000 lr: 0.00065610\n",
      "Epoch: [49] [   0/  20] time: 416.1849, loss: 0.00013403\n",
      "Epoch: [49] [   1/  20] time: 416.5819, loss: 0.00119943\n",
      "Epoch: [49] [   2/  20] time: 416.9755, loss: 0.00000019\n",
      "Epoch: [49] [   3/  20] time: 417.3691, loss: 0.00000029\n",
      "Epoch: [49] [   4/  20] time: 417.7620, loss: 0.00000449\n",
      "Epoch: [49] [   5/  20] time: 418.1570, loss: 0.00012554\n",
      "Epoch: [49] [   6/  20] time: 418.5534, loss: 0.00000001\n",
      "Epoch: [49] [   7/  20] time: 418.9460, loss: 0.00015822\n",
      "Epoch: [49] [   8/  20] time: 419.3410, loss: 0.00000513\n",
      "Epoch: [49] [   9/  20] time: 419.7388, loss: 0.00012395\n",
      "Epoch: [49] [  10/  20] time: 420.1347, loss: 0.00003928\n",
      "Epoch: [49] [  11/  20] time: 420.5273, loss: 0.00000712\n",
      "Epoch: [49] [  12/  20] time: 420.9241, loss: 0.00000082\n",
      "Epoch: [49] [  13/  20] time: 421.3211, loss: 0.00015765\n",
      "Epoch: [49] [  14/  20] time: 421.7143, loss: 0.00000502\n",
      "Epoch: [49] [  15/  20] time: 422.1307, loss: 0.00000003\n",
      "Epoch: [49] [  16/  20] time: 422.5284, loss: 0.00000759\n",
      "Epoch: [49] [  17/  20] time: 422.9240, loss: 0.00005598\n",
      "Epoch: [49] [  18/  20] time: 423.3170, loss: 0.00002109\n",
      "Epoch: [49] [  19/  20] time: 423.7066, loss: 0.00001334\n",
      "[49/50] - ptime: 8.2296 loss: 0.00010296 acc: 0.55000 lr: 0.00065610\n",
      "Epoch: [50] [   0/  20] time: 424.7574, loss: 0.00000011\n",
      "Epoch: [50] [   1/  20] time: 425.1518, loss: 0.00003120\n",
      "Epoch: [50] [   2/  20] time: 425.5442, loss: 0.00006404\n",
      "Epoch: [50] [   3/  20] time: 425.9372, loss: 0.00001169\n",
      "Epoch: [50] [   4/  20] time: 426.3316, loss: 0.00022205\n",
      "Epoch: [50] [   5/  20] time: 426.7366, loss: 0.00000193\n",
      "Epoch: [50] [   6/  20] time: 427.1335, loss: 0.00001558\n",
      "Epoch: [50] [   7/  20] time: 427.5265, loss: 0.00000799\n",
      "Epoch: [50] [   8/  20] time: 427.9201, loss: 0.00018620\n",
      "Epoch: [50] [   9/  20] time: 428.3157, loss: 0.00003505\n",
      "Epoch: [50] [  10/  20] time: 428.7265, loss: 0.00000951\n",
      "Epoch: [50] [  11/  20] time: 429.1355, loss: 0.00009511\n",
      "Epoch: [50] [  12/  20] time: 429.5338, loss: 0.00000061\n",
      "Epoch: [50] [  13/  20] time: 429.9303, loss: 0.00000064\n",
      "Epoch: [50] [  14/  20] time: 430.3254, loss: 0.00002793\n",
      "Epoch: [50] [  15/  20] time: 430.7240, loss: 0.00000293\n",
      "Epoch: [50] [  16/  20] time: 431.1182, loss: 0.00004651\n",
      "Epoch: [50] [  17/  20] time: 431.5162, loss: 0.00007194\n",
      "Epoch: [50] [  18/  20] time: 431.9100, loss: 0.00005060\n",
      "Epoch: [50] [  19/  20] time: 432.3037, loss: 0.00001146\n",
      "[50/50] - ptime: 8.2512 loss: 0.00004465 acc: 0.55000 lr: 0.00059049\n",
      "Avg per epoch ptime: 8.29, total 50 epochs ptime: 432.88\n",
      " [*] Training finished!\n",
      " [*] Best Epoch:  1 , Accuracy:  0.7999999523162842\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/CNN_GLCM_4_C5_D1_Kernel(3,3)_128_lrdecay/CNN_GLCM_4_C5_D1_Kernel(3,3)_128_lrdecay-1\n",
      " [*] Finished testing Best Epoch: 1 , accuracy:  0.7999999523162842 !\n"
     ]
    }
   ],
   "source": [
    "dataset = '4_Flowers_1s'\n",
    "epoch = 50\n",
    "batch_size = 100\n",
    "checkpoint_dir = 'checkpoint'\n",
    "log_dir = 'logs'\n",
    "trainhist_dir = 'trainhist'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "# --log_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# --trainhist_dir\n",
    "if not os.path.exists(trainhist_dir):\n",
    "    os.makedirs(trainhist_dir)\n",
    "\n",
    "# open session\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    \n",
    "    # declare instance for GAN\n",
    "    CNN = CNN(sess, epoch=epoch, batch_size=batch_size, dataset_name=dataset, checkpoint_dir=checkpoint_dir, \n",
    "                log_dir=log_dir, trainhist_dir=trainhist_dir)\n",
    "\n",
    "    # build graph\n",
    "    CNN.build_model()\n",
    "\n",
    "    # show network architecture\n",
    "    CNN.show_all_variables()\n",
    "\n",
    "    # launch the graph in a session\n",
    "    CNN.train()\n",
    "    \n",
    "#     CNN.test(epoch)\n",
    "        \n",
    "sess.close()\n",
    "        \n",
    "# lrdecay\n",
    "#  [*] Best Epoch:  33 , Accuracy:  0.7799999713897705\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_GLCM_4_C5_D1_Kernel(3,3)_128_lrdecay/CNN_GLCM_4_C5_D1_Kernel(3,3)_128_lrdecay-33\n",
    "#  [*] Finished testing Best Epoch: 33 , accuracy:  0.7799999713897705 !\n",
    "\n",
    "# Avg per epoch ptime: 8.29, total 50 epochs ptime: 432.88\n",
    "#  [*] Training finished!\n",
    "#  [*] Best Epoch:  1 , Accuracy:  0.7999999523162842\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_GLCM_4_C5_D1_Kernel(3,3)_128_lrdecay/CNN_GLCM_4_C5_D1_Kernel(3,3)_128_lrdecay-1\n",
    "#  [*] Finished testing Best Epoch: 1 , accuracy:  0.7999999523162842 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "file='/home/huiqy/Music/CloudMusic/All_Time_Low.mp3' #文件名是完整路径名\n",
    "pygame.mixer.init() #初始化音频\n",
    "track = pygame.mixer.music.load(file)#载入音乐文件\n",
    "pygame.mixer.music.play()#开始播放\n",
    "time.sleep(60)#播放10秒\n",
    "pygame.mixer.music.stop()#停止播放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
