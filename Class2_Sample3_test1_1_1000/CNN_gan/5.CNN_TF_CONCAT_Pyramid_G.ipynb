{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/huiqy/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imutils import paths\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import argparse\n",
    "import imutils,sklearn\n",
    "import os, cv2, re, random, shutil, imageio, pickle\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Pyramid\n",
    "# def batch_pyramid(images):\n",
    "#     pyramid_list = []\n",
    "#     for i in tqdm(range(len(images))):\n",
    "#         img = (images.astype(np.float32)* 255)[i, :, :, :].astype(np.uint8)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         G = img.copy()\n",
    "#         gp = [G]\n",
    "#         for i in range(4):\n",
    "#             G = cv2.pyrDown(G)\n",
    "#             gp.append(G)\n",
    "#         gp_out = [cv2.resize(gp[2], (128,128)),cv2.resize(gp[3], (128,128))]\n",
    "#         pyramid_list.append((np.array(gp_out)).astype(np.uint8))\n",
    "#     pyramid_list = np.array(pyramid_list,dtype = float32).reshape([images.shape[0],images.shape[1],images.shape[2],2])\n",
    "    \n",
    "#     print (pyramid_list.shape)\n",
    "#     return pyramid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def atoi(text):\n",
    "#     return int(text) if text.isdigit() else text\n",
    "\n",
    "# def natural_keys(text):\n",
    "#     return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# def load_flower_data():\n",
    "#     # grab the list of images that we'll be describing\n",
    "#     print(\"[INFO] handling images...\")\n",
    "#     TRAIN_ORIGINAL_DIR = '../train/'\n",
    "#     TRAIN_SUB_DIR = '../subsample/'\n",
    "#     TRAIN_GAN = '../../image_gan/'\n",
    "#     TEST_DIR = '../../test/'\n",
    "\n",
    "#     # use this for full dataset\n",
    "#     train_images_gan = [TRAIN_GAN + i for i in os.listdir(TRAIN_GAN)]\n",
    "#     test_images = [TEST_DIR + i for i in os.listdir(TEST_DIR)]\n",
    "    \n",
    "#     train_images = train_images_gan\n",
    "    \n",
    "#     train_images.sort(key=natural_keys)\n",
    "#     test_images.sort(key=natural_keys)\n",
    "\n",
    "#     # initialize the features matrix and labels list\n",
    "#     trainImage = []\n",
    "#     trainLabels = []\n",
    "#     testImage = []\n",
    "#     testLabels = []\n",
    "\n",
    "#     # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(train_images):\n",
    "#         # extract the class label\n",
    "#         # get the labels from the name of the images by extract the string before \"_\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # read and resize image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         trainImage.append(img)\n",
    "#         trainLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(train_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(train_images)))\n",
    "            \n",
    "#       # loop over the input images\n",
    "#     for (i, imagePath) in enumerate(test_images):\n",
    "#         # extract the class label\n",
    "#         # our images were named as labels.image_number.format\n",
    "#         # get the labels from the name of the images by extract the string before \".\"\n",
    "#         label = imagePath.split(os.path.sep)[-1].split(\"_\")[0]\n",
    "\n",
    "#         # extract CNN features in the image\n",
    "#         img = cv2.imread(imagePath)\n",
    "#         img = cv2.resize(img, (128,128))\n",
    "\n",
    "#         # add the messages we got to features and labels matricies\n",
    "#         testImage.append(img)\n",
    "#         testLabels.append(label)\n",
    "\n",
    "#         # show an update every 100 images until the last image\n",
    "#         if i > 0 and ((i + 1) % 1000 == 0 or i == len(test_images) - 1):\n",
    "#             print(\"[INFO] processed {}/{}\".format(i + 1, len(test_images)))\n",
    "\n",
    "\n",
    "#     trainImage = np.array(trainImage,dtype = float32)\n",
    "#     trainLabels = np.array(trainLabels)\n",
    "#     testImage = np.array(testImage,dtype = float32)\n",
    "#     testLabels = np.array(testLabels)\n",
    "#     print (trainImage.shape)\n",
    "    \n",
    "#     trainImage = trainImage.astype(np.float32) / 255\n",
    "#     testImage = testImage.astype(np.float32) / 255\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(trainLabels)\n",
    "#     list(le.classes_)\n",
    "#     trainLabels = le.transform(trainLabels) \n",
    "#     testLabels = le.transform(testLabels) \n",
    "    \n",
    "#     return trainImage, trainLabels, testImage, testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainImage, trainLabels, testImage, testLabels = load_flower_data()\n",
    "\n",
    "# trainImage_pyramid = batch_pyramid(trainImage)\n",
    "# testImage_pyramid = batch_pyramid(testImage)\n",
    "# nb_classes = 2\n",
    "\n",
    "# # Convert class vectors to binary class matrices.\n",
    "# trainLabels = keras.utils.to_categorical(trainLabels, nb_classes)\n",
    "# print (trainLabels)\n",
    "# testLabels = keras.utils.to_categorical(testLabels, nb_classes)\n",
    "# print (testLabels)\n",
    "# print (testLabels.shape)\n",
    "\n",
    "# np.save('../trainImage.npy', trainImage)\n",
    "# np.save('../trainLabels.npy', trainLabels)\n",
    "# np.save('../testImage.npy', testImage)\n",
    "# np.save('../testLabels.npy', testLabels)\n",
    "# np.save('../trainImage_pyramid_G.npy', trainImage_pyramid)\n",
    "# np.save('../testImage_pyramid_G.npy', testImage_pyramid)\n",
    "\n",
    "# print(\"[INFO] trainImage matrix: {:.2f}MB\".format(\n",
    "#     (trainImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainLabels matrix: {:.4f}MB\".format(\n",
    "#     (trainLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage matrix: {:.2f}MB\".format(\n",
    "#     (testImage.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testLabels matrix: {:.4f}MB\".format(\n",
    "#     (testLabels.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] trainImage_pyramid matrix: {:.2f}MB\".format(\n",
    "#     (trainImage_pyramid.nbytes) / (1024 * 1000.0)))\n",
    "# print(\"[INFO] testImage_pyramid matrix: {:.4f}MB\".format(\n",
    "#     (testImage_pyramid.nbytes) / (1024 * 1000.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self, sess, epoch, batch_size, dataset_name, checkpoint_dir, log_dir, trainhist_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.trainhist_dir = trainhist_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.classname = ['Iris', 'Pansy']\n",
    "\n",
    "        # parameters\n",
    "        self.input_height = 128\n",
    "        self.input_width = 128\n",
    "        self.c_dim = 3  # color dimension\n",
    "        self.pyramid_dim = 2  # color dimension\n",
    "        self.nb_class = 2\n",
    "        \n",
    "        # number of convolutional filters to use  \n",
    "        self.nb_CNN = [32, 64, 64, 64, 128]  \n",
    "        # number of dense filters to use  \n",
    "        self.nb_Dense = [256] \n",
    "        # size of pooling area for max pooling  \n",
    "        self.pool_size = (2, 2)  \n",
    "        # convolution kernel size  \n",
    "        self.kernel_size = (3, 3)\n",
    "        self.batch_normalization_control = True\n",
    "        \n",
    "        # name for checkpoint\n",
    "        self.model_name = 'CNN_Pyramid_G_C%d_D%d_Kernel(%d,%d)_%d_lrdecay' % (len(self.nb_CNN), len(self.nb_Dense),\n",
    "                                                          self.kernel_size[0], self.kernel_size[1], max(self.nb_CNN))\n",
    "\n",
    "        # train\n",
    "        #设置一个全局的计数器\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(0.001, \n",
    "                                             global_step=self.global_step, \n",
    "                                             decay_steps=10, \n",
    "                                             decay_rate=0.9, \n",
    "                                             staircase=True)\n",
    "        self.beta1 = 0.5\n",
    "        #max model to keep saving\n",
    "        self.max_to_keep = 300\n",
    "        \n",
    "        # test\n",
    "\n",
    "        #load_flower_data\n",
    "        self.train_x = np.load('../trainImage.npy')\n",
    "        self.train_y = np.load('../trainLabels.npy')\n",
    "        self.test_x = np.load('../testImage.npy')\n",
    "        self.test_y = np.load('../testLabels.npy')\n",
    "        self.train_x_pyramid = np.load('../trainImage_pyramid_L.npy')\n",
    "        self.test_x_pyramid = np.load('../testImage_pyramid_L.npy')\n",
    "        \n",
    "        #记录\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['losses'] = []\n",
    "        self.train_hist['accuracy'] = []\n",
    "        self.train_hist['learning_rate'] = []\n",
    "        self.train_hist['per_epoch_ptimes'] = []\n",
    "        self.train_hist['total_ptime'] = []\n",
    "        \n",
    "        # get number of batches for a single epoch\n",
    "        self.num_batches_train = len(self.train_x) // self.batch_size\n",
    "        self.num_batches_test= len(self.test_x) // self.batch_size\n",
    "\n",
    "    def cnn_model(self, x, x_pyramid, keep_prob, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(\"cnn\", reuse=reuse):\n",
    "             \n",
    "            #初始化参数\n",
    "            W = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "            B = tf.constant_initializer(0.0)\n",
    "        \n",
    "            print(\"CNN:x\",x.get_shape()) # 128, 128, 3 \n",
    "            print(\"CNN:x_pyramid\",x_pyramid.get_shape()) # 128, 128, 3 \n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_1 = tf.layers.conv2d(inputs = x,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_1'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_1.get_shape())\n",
    "            \n",
    "            #输入x,卷积核为3*3 输出维度为32\n",
    "            net1_2 = tf.layers.conv2d(inputs = x_pyramid,                 # 输入,\n",
    "                                    filters = self.nb_CNN[0],      # 卷积核个数,\n",
    "                                    kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                    strides = (1, 1),\n",
    "                                    padding = 'same',              # padding方法\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer = None,\n",
    "                                    bias_regularizer = None,\n",
    "                                    activity_regularizer = None,\n",
    "                                    name = 'conv_1_2'               # 命名用于获取变量\n",
    "                                    )\n",
    "            print(\"CNN:\",net1_2.get_shape())\n",
    "\n",
    "            #把数据和边缘进行连接\n",
    "            net = tf.concat([net1_1, net1_2], 3)\n",
    "            net = tf.layers.batch_normalization(net, training=is_training)\n",
    "            net = tf.nn.relu(net, name = 'relu_conv_1')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_1'\n",
    "                                             )\n",
    "            \n",
    "            for i in range(2,len(self.nb_CNN)+1):\n",
    "                net = tf.layers.conv2d(inputs = net,                 # 输入,\n",
    "                                       filters = self.nb_CNN[i-1],      # 卷积核个数,\n",
    "                                       kernel_size = self.kernel_size,          # 卷积核尺寸\n",
    "                                       strides = (1, 1),\n",
    "                                       padding = 'same',              # padding方法\n",
    "                                       kernel_initializer = W,\n",
    "                                       bias_initializer = B,\n",
    "                                       kernel_regularizer = None,\n",
    "                                       bias_regularizer = None,\n",
    "                                       activity_regularizer = None,\n",
    "                                       name = 'conv_'+ str(i)        # 命名用于获取变量\n",
    "                                       )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "                if self.batch_normalization_control:\n",
    "                    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_conv_' + str(i))\n",
    "                net = tf.layers.max_pooling2d(inputs = net,\n",
    "                                              pool_size = self.pool_size,\n",
    "                                              strides = (2, 2),\n",
    "                                              padding = 'same',\n",
    "                                              name = 'pool_conv_' + str(i)\n",
    "                                             )\n",
    "                print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #flatten\n",
    "            net = tf.reshape(net, [-1, int(net.get_shape()[1]*net.get_shape()[2]*net.get_shape()[3])],name='flatten')\n",
    "            print(\"CNN:\",net.get_shape())\n",
    "            \n",
    "            #dense layer\n",
    "            for i in range(1,len(self.nb_Dense)+1):\n",
    "                net = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_Dense[i-1],\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_' + str(i)\n",
    "                                    )\n",
    "#                 net = tf.layers.batch_normalization(net, training=is_training)\n",
    "                net = tf.nn.relu( net, name = 'relu_dense_' + str(i))\n",
    "                net = tf.layers.dropout(inputs = net,\n",
    "                                        rate=keep_prob,\n",
    "                                        noise_shape=None,\n",
    "                                        seed=None,\n",
    "                                        training = is_training,\n",
    "                                        name= 'dropout_dense_' + str(i)\n",
    "                                        )\n",
    "            #output\n",
    "            logit = tf.layers.dense(inputs = net,\n",
    "                                    units = self.nb_class,\n",
    "                                    kernel_initializer = W,\n",
    "                                    bias_initializer = B,\n",
    "                                    kernel_regularizer=None,\n",
    "                                    bias_regularizer=None,\n",
    "                                    activity_regularizer=None,\n",
    "                                    name = 'dense_output'\n",
    "                                    )\n",
    "            out_logit = tf.nn.softmax(logit, name=\"softmax\")\n",
    "            print(\"CNN:out_logit\",out_logit.get_shape())\n",
    "            print(\"------------------------\")    \n",
    "\n",
    "            return out_logit, logit\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.x = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.c_dim], \n",
    "                                name='x_image')\n",
    "        \n",
    "        self.x_pyramid = tf.placeholder(tf.float32, shape=[self.batch_size,self.input_height, self.input_width, self.pyramid_dim], \n",
    "                                name='x_pyramid')\n",
    "\n",
    "        self.y = tf.placeholder(tf.float32, shape=[self.batch_size, self.nb_class], name='y_label')\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.add_global = self.global_step.assign_add(1)\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "\n",
    "        # output of cnn_model\n",
    "        self.out_logit, self.logit = self.cnn_model(self.x, self.x_pyramid, self.keep_prob, is_training=True, reuse=False)\n",
    "        \n",
    "        self.loss_cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y,\n",
    "                                                                                         logits =self.logit))\n",
    "        \n",
    "        \"\"\" Training \"\"\"\n",
    "        # trainable variables into a group\n",
    "        tf_vars = tf.trainable_variables()\n",
    "        cnn_vars = [var for var in tf_vars if var.name.startswith('cnn')]\n",
    "\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.cnn_optim = tf.train.AdamOptimizer(self.lr, beta1=self.beta1).minimize(self.loss_cross_entropy,\n",
    "                                                                                        var_list=cnn_vars)\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        # output of cnn_model\n",
    "        self.out_logit_test, self.logit_test = self.cnn_model(self.x, self.x_pyramid, self.keep_prob, is_training=False, reuse=True)\n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.logit_test, 1), tf.argmax(self.y, 1))\n",
    "        self.predict = tf.argmax(self.logit_test, 1)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        self.loss_sum = tf.summary.scalar(\"loss\", self.loss_cross_entropy)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver(max_to_keep = self.max_to_keep)\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_epoch = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_epoch) + 1\n",
    "            counter = 1\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name+'.pkl', 'rb') \n",
    "            self.train_hist = pickle.load(f)\n",
    "            f.close()\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "            print(\" [!] START_EPOCH is \", start_epoch)\n",
    "        else:\n",
    "            start_epoch = 1\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        for epoch_loop in range(start_epoch, self.epoch + 1):\n",
    "\n",
    "            CNN_losses = []\n",
    "  \n",
    "            epoch_start_time = time.time()\n",
    "            shuffle_idxs = random.sample(range(0, self.train_x.shape[0]), self.train_x.shape[0])\n",
    "            shuffled_set = self.train_x[shuffle_idxs]\n",
    "            shuffled_set_pyramid = self.train_x_pyramid[shuffle_idxs]\n",
    "            shuffled_label = self.train_y[shuffle_idxs]\n",
    "    \n",
    "            # get batch data\n",
    "            for idx in range(self.num_batches_train):\n",
    "                batch_x = shuffled_set[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_x_pyramid= shuffled_set_pyramid[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_y = shuffled_label[idx*self.batch_size:(idx+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                \n",
    "\n",
    "                # update D network\n",
    "                _, summary_str, cnn_loss = self.sess.run([self.cnn_optim, self.loss_sum, self.loss_cross_entropy],\n",
    "                                               feed_dict={self.x: batch_x,\n",
    "                                                          self.x_pyramid: batch_x_pyramid,\n",
    "                                                          self.y: batch_y,\n",
    "                                                          self.keep_prob: 0.5}\n",
    "                                                      )\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                CNN_losses.append(cnn_loss)\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, loss: %.8f\" % (epoch_loop, idx, self.num_batches_train, \n",
    "                                                                          time.time() - start_time, cnn_loss))\n",
    "\n",
    "            # After an epoch\n",
    "            # Evaluates accuracy on test set\n",
    "            test_accuracy_list = []\n",
    "            for idx_test in range(self.num_batches_test):\n",
    "                batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "                batch_x_pyramid_test =self.test_x_pyramid[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "                batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                        [self.batch_size, self.nb_class])\n",
    "                accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                    self.x_pyramid: batch_x_pyramid_test,\n",
    "                                                                    self.y: batch_y_tes,\n",
    "                                                                    self.keep_prob: 1.0})\n",
    "                test_accuracy_list.append(accuracy)\n",
    "            test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        \n",
    "            #update learning rate\n",
    "            _, rate = sess.run([self.add_global, self.lr])\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "            \n",
    "            print('[%d/%d] - ptime: %.4f loss: %.8f acc: %.5f lr: %.8f'% (epoch_loop, self.epoch, per_epoch_ptime, \n",
    "                                                                    np.mean(CNN_losses), test_accuracy, rate))\n",
    "            \n",
    "            self.train_hist['losses'].append(np.mean(CNN_losses))\n",
    "            self.train_hist['accuracy'].append( test_accuracy)\n",
    "            self.train_hist['learning_rate'].append(rate)\n",
    "            self.train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "            \n",
    "            # save model\n",
    "            self.save(self.checkpoint_dir, epoch_loop)\n",
    "            \n",
    "            # save trainhist for train\n",
    "            f = open(self.trainhist_dir + '/' + self.model_name + '.pkl', 'wb') \n",
    "            pickle.dump(self.train_hist, f)\n",
    "            f.close()\n",
    "            self.show_train_hist(self.train_hist, save=True, path= self.trainhist_dir + '/' \n",
    "                                 + self.model_name + '.png')\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_ptime = end_time - start_time\n",
    "        self.train_hist['total_ptime'].append(total_ptime)\n",
    "        print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(self.train_hist['per_epoch_ptimes']), \n",
    "                                                                          self.epoch, total_ptime))\n",
    "        print(\" [*] Training finished!\")\n",
    "        \n",
    "        \"\"\"test after train\"\"\"\n",
    "        best_acc = max(self.train_hist['accuracy'])\n",
    "        beat_epoch = self.train_hist['accuracy'].index(best_acc) + 1\n",
    "        print (\" [*] Best Epoch: \", beat_epoch, \", Accuracy: \", best_acc)\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(beat_epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_pyramid_test =self.test_x_pyramid[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_pyramid: batch_x_pyramid_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Best Epoch:\", beat_epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "\n",
    "    def test(self, epoch):\n",
    "        path_model = self.checkpoint_dir + '/' + self.model_name + '/'+ self.model_name +'-'+ str(epoch)\n",
    "\n",
    "        \"\"\" restore epoch \"\"\"\n",
    "        new_saver = tf.train.import_meta_graph(path_model + '.meta' )\n",
    "        new_saver.restore(self.sess,path_model)\n",
    "\n",
    "        # Evaluates accuracy on test set\n",
    "        test_accuracy_list = []\n",
    "        for idx_test in range(self.num_batches_test):\n",
    "            batch_x_test = self.test_x[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_x_pyramidtest =self.test_x_pyramid[idx_test*self.batch_size:(idx_test+1)*self.batch_size]\n",
    "            batch_y_tes = self.test_y[idx_test*self.batch_size:(idx_test+1)*self.batch_size].reshape(\n",
    "                                    [self.batch_size, self.nb_class])\n",
    "            accuracy = self.sess.run([self.accuracy],feed_dict={self.x: batch_x_test, \n",
    "                                                                self.x_pyramid: batch_x_pyramid_test,\n",
    "                                                                self.y: batch_y_tes,\n",
    "                                                                self.keep_prob: 1.0})\n",
    "            test_accuracy_list.append(accuracy)\n",
    "        test_accuracy = np.sum(test_accuracy_list)/self.num_batches_test\n",
    "        print(\" [*] Finished testing Epoch:\", epoch, \", accuracy: \",test_accuracy, \"!\")\n",
    "        \n",
    "    def show_all_variables(self):\n",
    "        model_vars = tf.trainable_variables()\n",
    "        tf.contrib.slim.model_analyzer.analyze_vars(model_vars, print_info=True) \n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            epoch = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read [{}], epoch [{}]\".format(ckpt_name,epoch))\n",
    "            return True, epoch\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0\n",
    "        \n",
    "    def show_train_hist(self, hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "        x = range(1, len(hist['losses'])+1)\n",
    "\n",
    "        y1 = hist['losses']\n",
    "        y2 = hist['accuracy']\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "                            \n",
    "        ax2 = ax1.twinx()  \n",
    "\n",
    "        ax1.plot(x, y1, 'b')\n",
    "        ax2.plot(x, y2, 'r')\n",
    "                            \n",
    "        ax1.set_xlabel('Epoch')\n",
    "                            \n",
    "        ax1.set_ylabel('CNN_loss')    \n",
    "        ax2.set_ylabel('accuracy')\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(path, dpi = 400)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_pyramid (100, 128, 128, 2)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "CNN:x (100, 128, 128, 3)\n",
      "CNN:x_pyramid (100, 128, 128, 2)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 32)\n",
      "CNN: (100, 128, 128, 64)\n",
      "CNN: (100, 64, 64, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 32, 32, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 16, 16, 64)\n",
      "CNN: (100, 8, 8, 64)\n",
      "CNN: (100, 8, 8, 128)\n",
      "CNN: (100, 4, 4, 128)\n",
      "CNN: (100, 2048)\n",
      "CNN:out_logit (100, 2)\n",
      "------------------------\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "cnn/conv_1_1/kernel:0 (float32_ref 3x3x3x32) [864, bytes: 3456]\n",
      "cnn/conv_1_1/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/conv_1_2/kernel:0 (float32_ref 3x3x2x32) [576, bytes: 2304]\n",
      "cnn/conv_1_2/bias:0 (float32_ref 32) [32, bytes: 128]\n",
      "cnn/batch_normalization/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_2/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_2/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_3/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_3/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_2/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_4/kernel:0 (float32_ref 3x3x64x64) [36864, bytes: 147456]\n",
      "cnn/conv_4/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/batch_normalization_3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "cnn/conv_5/kernel:0 (float32_ref 3x3x64x128) [73728, bytes: 294912]\n",
      "cnn/conv_5/bias:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/batch_normalization_4/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "cnn/dense_1/kernel:0 (float32_ref 2048x256) [524288, bytes: 2097152]\n",
      "cnn/dense_1/bias:0 (float32_ref 256) [256, bytes: 1024]\n",
      "cnn/dense_output/kernel:0 (float32_ref 256x2) [512, bytes: 2048]\n",
      "cnn/dense_output/bias:0 (float32_ref 2) [2, bytes: 8]\n",
      "Total size of variables: 711970\n",
      "Total bytes of variables: 2847880\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay-41\n",
      " [*] Success to read [CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay-41], epoch [41]\n",
      " [*] Load SUCCESS\n",
      " [!] START_EPOCH is  42\n",
      "Epoch: [42] [   0/  60] time: 3.3047, loss: 0.00006918\n",
      "Epoch: [42] [   1/  60] time: 3.5589, loss: 0.00001892\n",
      "Epoch: [42] [   2/  60] time: 3.8126, loss: 0.00123278\n",
      "Epoch: [42] [   3/  60] time: 4.0825, loss: 0.00439048\n",
      "Epoch: [42] [   4/  60] time: 4.3603, loss: 0.00153516\n",
      "Epoch: [42] [   5/  60] time: 4.6480, loss: 0.00132504\n",
      "Epoch: [42] [   6/  60] time: 4.9389, loss: 0.00086557\n",
      "Epoch: [42] [   7/  60] time: 5.2411, loss: 0.00036867\n",
      "Epoch: [42] [   8/  60] time: 5.5083, loss: 0.00531215\n",
      "Epoch: [42] [   9/  60] time: 5.7805, loss: 0.00039071\n",
      "Epoch: [42] [  10/  60] time: 6.0542, loss: 0.00018145\n",
      "Epoch: [42] [  11/  60] time: 6.3055, loss: 0.00580059\n",
      "Epoch: [42] [  12/  60] time: 6.5664, loss: 0.00066222\n",
      "Epoch: [42] [  13/  60] time: 6.8309, loss: 0.00010889\n",
      "Epoch: [42] [  14/  60] time: 7.0933, loss: 0.00008994\n",
      "Epoch: [42] [  15/  60] time: 7.3636, loss: 0.00053466\n",
      "Epoch: [42] [  16/  60] time: 7.6282, loss: 0.00456108\n",
      "Epoch: [42] [  17/  60] time: 7.8967, loss: 0.07293309\n",
      "Epoch: [42] [  18/  60] time: 8.1646, loss: 0.00307880\n",
      "Epoch: [42] [  19/  60] time: 8.4338, loss: 0.00641266\n",
      "Epoch: [42] [  20/  60] time: 8.7031, loss: 0.00365431\n",
      "Epoch: [42] [  21/  60] time: 8.9633, loss: 0.03179585\n",
      "Epoch: [42] [  22/  60] time: 9.2253, loss: 0.24913140\n",
      "Epoch: [42] [  23/  60] time: 9.5014, loss: 0.00613253\n",
      "Epoch: [42] [  24/  60] time: 9.7824, loss: 0.05414652\n",
      "Epoch: [42] [  25/  60] time: 10.0570, loss: 0.02400624\n",
      "Epoch: [42] [  26/  60] time: 10.3227, loss: 0.00283456\n",
      "Epoch: [42] [  27/  60] time: 10.5860, loss: 0.05869358\n",
      "Epoch: [42] [  28/  60] time: 10.8495, loss: 0.00072934\n",
      "Epoch: [42] [  29/  60] time: 11.1141, loss: 0.00574908\n",
      "Epoch: [42] [  30/  60] time: 11.3840, loss: 0.00076537\n",
      "Epoch: [42] [  31/  60] time: 11.6518, loss: 0.00368876\n",
      "Epoch: [42] [  32/  60] time: 11.9180, loss: 0.00578947\n",
      "Epoch: [42] [  33/  60] time: 12.1751, loss: 0.00847425\n",
      "Epoch: [42] [  34/  60] time: 12.4255, loss: 0.00160158\n",
      "Epoch: [42] [  35/  60] time: 12.6853, loss: 0.00356918\n",
      "Epoch: [42] [  36/  60] time: 12.9375, loss: 0.00094145\n",
      "Epoch: [42] [  37/  60] time: 13.1924, loss: 0.00007154\n",
      "Epoch: [42] [  38/  60] time: 13.4573, loss: 0.00040386\n",
      "Epoch: [42] [  39/  60] time: 13.7058, loss: 0.00040171\n",
      "Epoch: [42] [  40/  60] time: 13.9678, loss: 0.00055910\n",
      "Epoch: [42] [  41/  60] time: 14.2244, loss: 0.01718204\n",
      "Epoch: [42] [  42/  60] time: 14.4752, loss: 0.00065570\n",
      "Epoch: [42] [  43/  60] time: 14.7264, loss: 0.00936568\n",
      "Epoch: [42] [  44/  60] time: 14.9812, loss: 0.00509281\n",
      "Epoch: [42] [  45/  60] time: 15.2338, loss: 0.00137696\n",
      "Epoch: [42] [  46/  60] time: 15.4894, loss: 0.01895162\n",
      "Epoch: [42] [  47/  60] time: 15.7528, loss: 0.00000489\n",
      "Epoch: [42] [  48/  60] time: 16.0049, loss: 0.01959942\n",
      "Epoch: [42] [  49/  60] time: 16.2766, loss: 0.00150090\n",
      "Epoch: [42] [  50/  60] time: 16.5311, loss: 0.00129670\n",
      "Epoch: [42] [  51/  60] time: 16.7818, loss: 0.00518872\n",
      "Epoch: [42] [  52/  60] time: 17.0367, loss: 0.01951605\n",
      "Epoch: [42] [  53/  60] time: 17.2910, loss: 0.00032972\n",
      "Epoch: [42] [  54/  60] time: 17.5442, loss: 0.00007857\n",
      "Epoch: [42] [  55/  60] time: 17.7907, loss: 0.00139353\n",
      "Epoch: [42] [  56/  60] time: 18.0457, loss: 0.00004791\n",
      "Epoch: [42] [  57/  60] time: 18.2948, loss: 0.00921458\n",
      "Epoch: [42] [  58/  60] time: 18.5915, loss: 0.00074922\n",
      "Epoch: [42] [  59/  60] time: 18.8578, loss: 0.00006691\n",
      "[42/50] - ptime: 18.9681 loss: 0.01141040 acc: 0.68000 lr: 0.00065610\n",
      "Epoch: [43] [   0/  60] time: 21.1887, loss: 0.00379790\n",
      "Epoch: [43] [   1/  60] time: 21.4394, loss: 0.04973748\n",
      "Epoch: [43] [   2/  60] time: 21.6981, loss: 0.00113141\n",
      "Epoch: [43] [   3/  60] time: 21.9546, loss: 0.00076915\n",
      "Epoch: [43] [   4/  60] time: 22.2197, loss: 0.00031467\n",
      "Epoch: [43] [   5/  60] time: 22.4929, loss: 0.00028981\n",
      "Epoch: [43] [   6/  60] time: 22.7818, loss: 0.00019919\n",
      "Epoch: [43] [   7/  60] time: 23.0535, loss: 0.00008398\n",
      "Epoch: [43] [   8/  60] time: 23.3553, loss: 0.01157811\n",
      "Epoch: [43] [   9/  60] time: 23.6249, loss: 0.00006036\n",
      "Epoch: [43] [  10/  60] time: 23.8856, loss: 0.00050867\n",
      "Epoch: [43] [  11/  60] time: 24.1390, loss: 0.00692461\n",
      "Epoch: [43] [  12/  60] time: 24.4086, loss: 0.00051259\n",
      "Epoch: [43] [  13/  60] time: 24.6813, loss: 0.00027543\n",
      "Epoch: [43] [  14/  60] time: 24.9853, loss: 0.00003497\n",
      "Epoch: [43] [  15/  60] time: 25.2485, loss: 0.00000778\n",
      "Epoch: [43] [  16/  60] time: 25.5007, loss: 0.00084599\n",
      "Epoch: [43] [  17/  60] time: 25.7540, loss: 0.00237166\n",
      "Epoch: [43] [  18/  60] time: 26.0073, loss: 0.00126826\n",
      "Epoch: [43] [  19/  60] time: 26.2602, loss: 0.00003573\n",
      "Epoch: [43] [  20/  60] time: 26.5116, loss: 0.00056096\n",
      "Epoch: [43] [  21/  60] time: 26.7642, loss: 0.00295754\n",
      "Epoch: [43] [  22/  60] time: 27.0276, loss: 0.00236691\n",
      "Epoch: [43] [  23/  60] time: 27.2909, loss: 0.00017305\n",
      "Epoch: [43] [  24/  60] time: 27.5419, loss: 0.00029594\n",
      "Epoch: [43] [  25/  60] time: 27.7897, loss: 0.00554258\n",
      "Epoch: [43] [  26/  60] time: 28.0462, loss: 0.00033929\n",
      "Epoch: [43] [  27/  60] time: 28.3084, loss: 0.00171342\n",
      "Epoch: [43] [  28/  60] time: 28.5713, loss: 0.00069522\n",
      "Epoch: [43] [  29/  60] time: 28.8319, loss: 0.00030651\n",
      "Epoch: [43] [  30/  60] time: 29.0877, loss: 0.00232832\n",
      "Epoch: [43] [  31/  60] time: 29.3363, loss: 0.00002464\n",
      "Epoch: [43] [  32/  60] time: 29.5875, loss: 0.00006497\n",
      "Epoch: [43] [  33/  60] time: 29.8363, loss: 0.00097210\n",
      "Epoch: [43] [  34/  60] time: 30.0838, loss: 0.00038607\n",
      "Epoch: [43] [  35/  60] time: 30.3391, loss: 0.00004403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [43] [  36/  60] time: 30.5878, loss: 0.00113085\n",
      "Epoch: [43] [  37/  60] time: 30.8350, loss: 0.00735760\n",
      "Epoch: [43] [  38/  60] time: 31.0933, loss: 0.00396785\n",
      "Epoch: [43] [  39/  60] time: 31.3469, loss: 0.00168989\n",
      "Epoch: [43] [  40/  60] time: 31.6011, loss: 0.00042477\n",
      "Epoch: [43] [  41/  60] time: 31.8459, loss: 0.00070360\n",
      "Epoch: [43] [  42/  60] time: 32.0928, loss: 0.00096747\n",
      "Epoch: [43] [  43/  60] time: 32.3473, loss: 0.00026676\n",
      "Epoch: [43] [  44/  60] time: 32.5949, loss: 0.00013018\n",
      "Epoch: [43] [  45/  60] time: 32.8516, loss: 0.00067303\n",
      "Epoch: [43] [  46/  60] time: 33.1096, loss: 0.00032883\n",
      "Epoch: [43] [  47/  60] time: 33.3782, loss: 0.00457751\n",
      "Epoch: [43] [  48/  60] time: 33.6491, loss: 0.00001778\n",
      "Epoch: [43] [  49/  60] time: 33.9238, loss: 0.00004993\n",
      "Epoch: [43] [  50/  60] time: 34.2022, loss: 0.00009122\n",
      "Epoch: [43] [  51/  60] time: 34.4711, loss: 0.00000903\n",
      "Epoch: [43] [  52/  60] time: 34.7240, loss: 0.00161304\n",
      "Epoch: [43] [  53/  60] time: 34.9729, loss: 0.00040675\n",
      "Epoch: [43] [  54/  60] time: 35.2202, loss: 0.00044426\n",
      "Epoch: [43] [  55/  60] time: 35.4727, loss: 0.00041465\n",
      "Epoch: [43] [  56/  60] time: 35.7235, loss: 0.00003799\n",
      "Epoch: [43] [  57/  60] time: 35.9752, loss: 0.00018741\n",
      "Epoch: [43] [  58/  60] time: 36.2303, loss: 0.00003921\n",
      "Epoch: [43] [  59/  60] time: 36.4848, loss: 0.00901169\n",
      "[43/50] - ptime: 16.9224 loss: 0.00223434 acc: 0.65000 lr: 0.00065610\n",
      "Epoch: [44] [   0/  60] time: 38.7008, loss: 0.00035528\n",
      "Epoch: [44] [   1/  60] time: 38.9611, loss: 0.00007919\n",
      "Epoch: [44] [   2/  60] time: 39.2160, loss: 0.00013417\n",
      "Epoch: [44] [   3/  60] time: 39.4655, loss: 0.00001048\n",
      "Epoch: [44] [   4/  60] time: 39.7210, loss: 0.00058279\n",
      "Epoch: [44] [   5/  60] time: 39.9746, loss: 0.00006783\n",
      "Epoch: [44] [   6/  60] time: 40.2277, loss: 0.00000473\n",
      "Epoch: [44] [   7/  60] time: 40.4761, loss: 0.00008484\n",
      "Epoch: [44] [   8/  60] time: 40.7226, loss: 0.00000183\n",
      "Epoch: [44] [   9/  60] time: 40.9836, loss: 0.00152967\n",
      "Epoch: [44] [  10/  60] time: 41.2324, loss: 0.00001007\n",
      "Epoch: [44] [  11/  60] time: 41.4967, loss: 0.00002434\n",
      "Epoch: [44] [  12/  60] time: 41.7503, loss: 0.00007643\n",
      "Epoch: [44] [  13/  60] time: 42.0079, loss: 0.00013782\n",
      "Epoch: [44] [  14/  60] time: 42.2691, loss: 0.00142481\n",
      "Epoch: [44] [  15/  60] time: 42.5219, loss: 0.00724097\n",
      "Epoch: [44] [  16/  60] time: 42.7693, loss: 0.00003962\n",
      "Epoch: [44] [  17/  60] time: 43.0202, loss: 0.00082713\n",
      "Epoch: [44] [  18/  60] time: 43.2716, loss: 0.00008712\n",
      "Epoch: [44] [  19/  60] time: 43.5219, loss: 0.00004753\n",
      "Epoch: [44] [  20/  60] time: 43.7728, loss: 0.00000986\n",
      "Epoch: [44] [  21/  60] time: 44.0237, loss: 0.00082504\n",
      "Epoch: [44] [  22/  60] time: 44.2723, loss: 0.00007668\n",
      "Epoch: [44] [  23/  60] time: 44.5206, loss: 0.00001155\n",
      "Epoch: [44] [  24/  60] time: 44.7701, loss: 0.00036226\n",
      "Epoch: [44] [  25/  60] time: 45.0274, loss: 0.00001549\n",
      "Epoch: [44] [  26/  60] time: 45.2774, loss: 0.00002807\n",
      "Epoch: [44] [  27/  60] time: 45.5249, loss: 0.00027853\n",
      "Epoch: [44] [  28/  60] time: 45.7712, loss: 0.00018582\n",
      "Epoch: [44] [  29/  60] time: 46.0417, loss: 0.00617963\n",
      "Epoch: [44] [  30/  60] time: 46.2911, loss: 0.00010058\n",
      "Epoch: [44] [  31/  60] time: 46.5434, loss: 0.00085112\n",
      "Epoch: [44] [  32/  60] time: 46.7942, loss: 0.00014088\n",
      "Epoch: [44] [  33/  60] time: 47.0466, loss: 0.00009314\n",
      "Epoch: [44] [  34/  60] time: 47.2985, loss: 0.00006290\n",
      "Epoch: [44] [  35/  60] time: 47.5635, loss: 0.00008398\n",
      "Epoch: [44] [  36/  60] time: 47.8199, loss: 0.00001421\n",
      "Epoch: [44] [  37/  60] time: 48.0712, loss: 0.00029288\n",
      "Epoch: [44] [  38/  60] time: 48.3236, loss: 0.00022435\n",
      "Epoch: [44] [  39/  60] time: 48.5735, loss: 0.00018907\n",
      "Epoch: [44] [  40/  60] time: 48.8253, loss: 0.00025761\n",
      "Epoch: [44] [  41/  60] time: 49.0867, loss: 0.00103133\n",
      "Epoch: [44] [  42/  60] time: 49.3370, loss: 0.00554729\n",
      "Epoch: [44] [  43/  60] time: 49.5875, loss: 0.00959272\n",
      "Epoch: [44] [  44/  60] time: 49.8393, loss: 0.00004184\n",
      "Epoch: [44] [  45/  60] time: 50.0891, loss: 0.00006929\n",
      "Epoch: [44] [  46/  60] time: 50.3403, loss: 0.00001303\n",
      "Epoch: [44] [  47/  60] time: 50.5899, loss: 0.00024411\n",
      "Epoch: [44] [  48/  60] time: 50.8419, loss: 0.00000935\n",
      "Epoch: [44] [  49/  60] time: 51.0931, loss: 0.00037775\n",
      "Epoch: [44] [  50/  60] time: 51.3504, loss: 0.00004890\n",
      "Epoch: [44] [  51/  60] time: 51.5991, loss: 0.00194100\n",
      "Epoch: [44] [  52/  60] time: 51.8544, loss: 0.00014837\n",
      "Epoch: [44] [  53/  60] time: 52.1039, loss: 0.00036050\n",
      "Epoch: [44] [  54/  60] time: 52.3538, loss: 0.00186201\n",
      "Epoch: [44] [  55/  60] time: 52.6019, loss: 0.00263740\n",
      "Epoch: [44] [  56/  60] time: 52.8531, loss: 0.00011242\n",
      "Epoch: [44] [  57/  60] time: 53.1183, loss: 0.00000123\n",
      "Epoch: [44] [  58/  60] time: 53.3721, loss: 0.00001700\n",
      "Epoch: [44] [  59/  60] time: 53.6272, loss: 0.00001086\n",
      "[44/50] - ptime: 16.5478 loss: 0.00078528 acc: 0.70000 lr: 0.00065610\n",
      "Epoch: [45] [   0/  60] time: 56.0750, loss: 0.00000217\n",
      "Epoch: [45] [   1/  60] time: 56.3246, loss: 0.00077428\n",
      "Epoch: [45] [   2/  60] time: 56.5717, loss: 0.00001048\n",
      "Epoch: [45] [   3/  60] time: 56.8263, loss: 0.00000351\n",
      "Epoch: [45] [   4/  60] time: 57.0787, loss: 0.00005683\n",
      "Epoch: [45] [   5/  60] time: 57.3307, loss: 0.00048493\n",
      "Epoch: [45] [   6/  60] time: 57.5791, loss: 0.00000392\n",
      "Epoch: [45] [   7/  60] time: 57.8292, loss: 0.00007751\n",
      "Epoch: [45] [   8/  60] time: 58.0817, loss: 0.00000815\n",
      "Epoch: [45] [   9/  60] time: 58.3310, loss: 0.00000897\n",
      "Epoch: [45] [  10/  60] time: 58.5801, loss: 0.00003664\n",
      "Epoch: [45] [  11/  60] time: 58.8378, loss: 0.00002707\n",
      "Epoch: [45] [  12/  60] time: 59.0891, loss: 0.00143658\n",
      "Epoch: [45] [  13/  60] time: 59.3415, loss: 0.00000185\n",
      "Epoch: [45] [  14/  60] time: 59.5948, loss: 0.00035832\n",
      "Epoch: [45] [  15/  60] time: 59.8555, loss: 0.00043182\n",
      "Epoch: [45] [  16/  60] time: 60.1097, loss: 0.00000070\n",
      "Epoch: [45] [  17/  60] time: 60.3588, loss: 0.00000428\n",
      "Epoch: [45] [  18/  60] time: 60.6076, loss: 0.00007439\n",
      "Epoch: [45] [  19/  60] time: 60.8589, loss: 0.00001400\n",
      "Epoch: [45] [  20/  60] time: 61.1062, loss: 0.00002288\n",
      "Epoch: [45] [  21/  60] time: 61.3590, loss: 0.00002023\n",
      "Epoch: [45] [  22/  60] time: 61.6084, loss: 0.00004323\n",
      "Epoch: [45] [  23/  60] time: 61.8571, loss: 0.00000039\n",
      "Epoch: [45] [  24/  60] time: 62.1059, loss: 0.00004440\n",
      "Epoch: [45] [  25/  60] time: 62.3563, loss: 0.00006480\n",
      "Epoch: [45] [  26/  60] time: 62.6055, loss: 0.00016178\n",
      "Epoch: [45] [  27/  60] time: 62.8587, loss: 0.00033164\n",
      "Epoch: [45] [  28/  60] time: 63.1129, loss: 0.00002440\n",
      "Epoch: [45] [  29/  60] time: 63.3648, loss: 0.00007383\n",
      "Epoch: [45] [  30/  60] time: 63.6187, loss: 0.00001461\n",
      "Epoch: [45] [  31/  60] time: 63.8663, loss: 0.00079870\n",
      "Epoch: [45] [  32/  60] time: 64.1180, loss: 0.00055526\n",
      "Epoch: [45] [  33/  60] time: 64.3766, loss: 0.00001257\n",
      "Epoch: [45] [  34/  60] time: 64.6242, loss: 0.00027007\n",
      "Epoch: [45] [  35/  60] time: 64.8760, loss: 0.00000588\n",
      "Epoch: [45] [  36/  60] time: 65.1245, loss: 0.00004349\n",
      "Epoch: [45] [  37/  60] time: 65.3737, loss: 0.00005817\n",
      "Epoch: [45] [  38/  60] time: 65.6234, loss: 0.00003742\n",
      "Epoch: [45] [  39/  60] time: 65.8732, loss: 0.00002975\n",
      "Epoch: [45] [  40/  60] time: 66.1280, loss: 0.00130733\n",
      "Epoch: [45] [  41/  60] time: 66.3797, loss: 0.00000586\n",
      "Epoch: [45] [  42/  60] time: 66.6296, loss: 0.00007338\n",
      "Epoch: [45] [  43/  60] time: 66.8778, loss: 0.00044041\n",
      "Epoch: [45] [  44/  60] time: 67.1317, loss: 0.00002905\n",
      "Epoch: [45] [  45/  60] time: 67.3885, loss: 0.00019498\n",
      "Epoch: [45] [  46/  60] time: 67.6361, loss: 0.00008144\n",
      "Epoch: [45] [  47/  60] time: 67.8898, loss: 0.00002427\n",
      "Epoch: [45] [  48/  60] time: 68.1404, loss: 0.00004315\n",
      "Epoch: [45] [  49/  60] time: 68.3902, loss: 0.00003809\n",
      "Epoch: [45] [  50/  60] time: 68.6386, loss: 0.00061690\n",
      "Epoch: [45] [  51/  60] time: 68.8918, loss: 0.00000438\n",
      "Epoch: [45] [  52/  60] time: 69.1435, loss: 0.00002518\n",
      "Epoch: [45] [  53/  60] time: 69.3950, loss: 0.00000419\n",
      "Epoch: [45] [  54/  60] time: 69.6491, loss: 0.00000865\n",
      "Epoch: [45] [  55/  60] time: 69.9021, loss: 0.00005697\n",
      "Epoch: [45] [  56/  60] time: 70.1515, loss: 0.00007334\n",
      "Epoch: [45] [  57/  60] time: 70.4011, loss: 0.00004671\n",
      "Epoch: [45] [  58/  60] time: 70.6515, loss: 0.00006267\n",
      "Epoch: [45] [  59/  60] time: 70.8984, loss: 0.00008410\n",
      "[45/50] - ptime: 16.4371 loss: 0.00016085 acc: 0.70000 lr: 0.00065610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [46] [   0/  60] time: 73.1163, loss: 0.00001458\n",
      "Epoch: [46] [   1/  60] time: 73.3670, loss: 0.00000626\n",
      "Epoch: [46] [   2/  60] time: 73.6205, loss: 0.00019121\n",
      "Epoch: [46] [   3/  60] time: 73.8688, loss: 0.00076138\n",
      "Epoch: [46] [   4/  60] time: 74.1222, loss: 0.00001819\n",
      "Epoch: [46] [   5/  60] time: 74.3756, loss: 0.00186552\n",
      "Epoch: [46] [   6/  60] time: 74.6242, loss: 0.00002141\n",
      "Epoch: [46] [   7/  60] time: 74.8784, loss: 0.00000203\n",
      "Epoch: [46] [   8/  60] time: 75.1354, loss: 0.00000377\n",
      "Epoch: [46] [   9/  60] time: 75.3870, loss: 0.00004668\n",
      "Epoch: [46] [  10/  60] time: 75.6344, loss: 0.00008497\n",
      "Epoch: [46] [  11/  60] time: 75.8904, loss: 0.00001002\n",
      "Epoch: [46] [  12/  60] time: 76.1440, loss: 0.00024576\n",
      "Epoch: [46] [  13/  60] time: 76.4018, loss: 0.00019034\n",
      "Epoch: [46] [  14/  60] time: 76.6691, loss: 0.00004074\n",
      "Epoch: [46] [  15/  60] time: 76.9137, loss: 0.00000427\n",
      "Epoch: [46] [  16/  60] time: 77.1690, loss: 0.00001524\n",
      "Epoch: [46] [  17/  60] time: 77.4194, loss: 0.00017014\n",
      "Epoch: [46] [  18/  60] time: 77.6720, loss: 0.00008501\n",
      "Epoch: [46] [  19/  60] time: 77.9209, loss: 0.00002498\n",
      "Epoch: [46] [  20/  60] time: 78.1860, loss: 0.00006626\n",
      "Epoch: [46] [  21/  60] time: 78.4363, loss: 0.00012375\n",
      "Epoch: [46] [  22/  60] time: 78.7120, loss: 0.00003561\n",
      "Epoch: [46] [  23/  60] time: 78.9629, loss: 0.00000620\n",
      "Epoch: [46] [  24/  60] time: 79.2149, loss: 0.00015814\n",
      "Epoch: [46] [  25/  60] time: 79.4659, loss: 0.00217359\n",
      "Epoch: [46] [  26/  60] time: 79.7217, loss: 0.00004763\n",
      "Epoch: [46] [  27/  60] time: 79.9777, loss: 0.00004411\n",
      "Epoch: [46] [  28/  60] time: 80.2370, loss: 0.00006385\n",
      "Epoch: [46] [  29/  60] time: 80.4916, loss: 0.00030028\n",
      "Epoch: [46] [  30/  60] time: 80.7480, loss: 0.00001928\n",
      "Epoch: [46] [  31/  60] time: 81.0007, loss: 0.00000188\n",
      "Epoch: [46] [  32/  60] time: 81.2505, loss: 0.00047885\n",
      "Epoch: [46] [  33/  60] time: 81.5120, loss: 0.00000244\n",
      "Epoch: [46] [  34/  60] time: 81.7707, loss: 0.00001224\n",
      "Epoch: [46] [  35/  60] time: 82.0271, loss: 0.00001058\n",
      "Epoch: [46] [  36/  60] time: 82.2742, loss: 0.00000432\n",
      "Epoch: [46] [  37/  60] time: 82.5257, loss: 0.00006004\n",
      "Epoch: [46] [  38/  60] time: 82.7837, loss: 0.00036051\n",
      "Epoch: [46] [  39/  60] time: 83.0345, loss: 0.00068022\n",
      "Epoch: [46] [  40/  60] time: 83.2821, loss: 0.00004343\n",
      "Epoch: [46] [  41/  60] time: 83.5380, loss: 0.00013651\n",
      "Epoch: [46] [  42/  60] time: 83.7970, loss: 0.00005062\n",
      "Epoch: [46] [  43/  60] time: 84.0492, loss: 0.00000530\n",
      "Epoch: [46] [  44/  60] time: 84.3226, loss: 0.00000312\n",
      "Epoch: [46] [  45/  60] time: 84.5710, loss: 0.00002134\n",
      "Epoch: [46] [  46/  60] time: 84.8257, loss: 0.00000118\n",
      "Epoch: [46] [  47/  60] time: 85.0858, loss: 0.00000487\n",
      "Epoch: [46] [  48/  60] time: 85.3530, loss: 0.00000454\n",
      "Epoch: [46] [  49/  60] time: 85.6067, loss: 0.00000267\n",
      "Epoch: [46] [  50/  60] time: 85.8554, loss: 0.00000784\n",
      "Epoch: [46] [  51/  60] time: 86.1055, loss: 0.00004365\n",
      "Epoch: [46] [  52/  60] time: 86.3563, loss: 0.00006955\n",
      "Epoch: [46] [  53/  60] time: 86.6063, loss: 0.00006071\n",
      "Epoch: [46] [  54/  60] time: 86.8554, loss: 0.00000965\n",
      "Epoch: [46] [  55/  60] time: 87.1044, loss: 0.00001009\n",
      "Epoch: [46] [  56/  60] time: 87.3588, loss: 0.00073516\n",
      "Epoch: [46] [  57/  60] time: 87.6124, loss: 0.00029262\n",
      "Epoch: [46] [  58/  60] time: 87.8556, loss: 0.00018898\n",
      "Epoch: [46] [  59/  60] time: 88.1092, loss: 0.00000230\n",
      "[46/50] - ptime: 16.5978 loss: 0.00016911 acc: 0.72000 lr: 0.00065610\n",
      "Epoch: [47] [   0/  60] time: 90.4098, loss: 0.00003337\n",
      "Epoch: [47] [   1/  60] time: 90.6586, loss: 0.00004654\n",
      "Epoch: [47] [   2/  60] time: 90.9129, loss: 0.00008323\n",
      "Epoch: [47] [   3/  60] time: 91.1643, loss: 0.00042822\n",
      "Epoch: [47] [   4/  60] time: 91.4146, loss: 0.00051495\n",
      "Epoch: [47] [   5/  60] time: 91.6641, loss: 0.00009713\n",
      "Epoch: [47] [   6/  60] time: 91.9101, loss: 0.00021728\n",
      "Epoch: [47] [   7/  60] time: 92.1604, loss: 0.00001407\n",
      "Epoch: [47] [   8/  60] time: 92.4174, loss: 0.00011933\n",
      "Epoch: [47] [   9/  60] time: 92.6684, loss: 0.00003182\n",
      "Epoch: [47] [  10/  60] time: 92.9169, loss: 0.00001821\n",
      "Epoch: [47] [  11/  60] time: 93.1678, loss: 0.00017810\n",
      "Epoch: [47] [  12/  60] time: 93.4232, loss: 0.00029841\n",
      "Epoch: [47] [  13/  60] time: 93.6713, loss: 0.00001247\n",
      "Epoch: [47] [  14/  60] time: 93.9252, loss: 0.00000248\n",
      "Epoch: [47] [  15/  60] time: 94.1863, loss: 0.00000019\n",
      "Epoch: [47] [  16/  60] time: 94.4391, loss: 0.00001649\n",
      "Epoch: [47] [  17/  60] time: 94.6890, loss: 0.00009970\n",
      "Epoch: [47] [  18/  60] time: 94.9422, loss: 0.00000009\n",
      "Epoch: [47] [  19/  60] time: 95.1986, loss: 0.00009338\n",
      "Epoch: [47] [  20/  60] time: 95.4533, loss: 0.00005280\n",
      "Epoch: [47] [  21/  60] time: 95.7074, loss: 0.00000376\n",
      "Epoch: [47] [  22/  60] time: 95.9581, loss: 0.00003667\n",
      "Epoch: [47] [  23/  60] time: 96.2081, loss: 0.00002148\n",
      "Epoch: [47] [  24/  60] time: 96.4563, loss: 0.00021273\n",
      "Epoch: [47] [  25/  60] time: 96.7067, loss: 0.00005575\n",
      "Epoch: [47] [  26/  60] time: 96.9583, loss: 0.00000092\n",
      "Epoch: [47] [  27/  60] time: 97.2064, loss: 0.00001974\n",
      "Epoch: [47] [  28/  60] time: 97.4554, loss: 0.00000797\n",
      "Epoch: [47] [  29/  60] time: 97.7002, loss: 0.00000659\n",
      "Epoch: [47] [  30/  60] time: 97.9510, loss: 0.00000244\n",
      "Epoch: [47] [  31/  60] time: 98.2074, loss: 0.00000413\n",
      "Epoch: [47] [  32/  60] time: 98.4643, loss: 0.00000761\n",
      "Epoch: [47] [  33/  60] time: 98.7164, loss: 0.00000457\n",
      "Epoch: [47] [  34/  60] time: 98.9648, loss: 0.00097951\n",
      "Epoch: [47] [  35/  60] time: 99.2152, loss: 0.00060309\n",
      "Epoch: [47] [  36/  60] time: 99.4589, loss: 0.00002274\n",
      "Epoch: [47] [  37/  60] time: 99.7165, loss: 0.00004101\n",
      "Epoch: [47] [  38/  60] time: 99.9659, loss: 0.00002145\n",
      "Epoch: [47] [  39/  60] time: 100.2208, loss: 0.00000566\n",
      "Epoch: [47] [  40/  60] time: 100.4758, loss: 0.00000501\n",
      "Epoch: [47] [  41/  60] time: 100.7268, loss: 0.00005426\n",
      "Epoch: [47] [  42/  60] time: 100.9789, loss: 0.00013972\n",
      "Epoch: [47] [  43/  60] time: 101.2286, loss: 0.00004792\n",
      "Epoch: [47] [  44/  60] time: 101.4777, loss: 0.00004602\n",
      "Epoch: [47] [  45/  60] time: 101.7275, loss: 0.00024283\n",
      "Epoch: [47] [  46/  60] time: 101.9762, loss: 0.00000011\n",
      "Epoch: [47] [  47/  60] time: 102.2283, loss: 0.00011058\n",
      "Epoch: [47] [  48/  60] time: 102.4808, loss: 0.00016374\n",
      "Epoch: [47] [  49/  60] time: 102.7319, loss: 0.00005243\n",
      "Epoch: [47] [  50/  60] time: 102.9832, loss: 0.00008698\n",
      "Epoch: [47] [  51/  60] time: 103.2402, loss: 0.00003023\n",
      "Epoch: [47] [  52/  60] time: 103.4898, loss: 0.00003492\n",
      "Epoch: [47] [  53/  60] time: 103.7375, loss: 0.00007872\n",
      "Epoch: [47] [  54/  60] time: 103.9856, loss: 0.00001400\n",
      "Epoch: [47] [  55/  60] time: 104.2384, loss: 0.00000138\n",
      "Epoch: [47] [  56/  60] time: 104.4917, loss: 0.00001291\n",
      "Epoch: [47] [  57/  60] time: 104.7444, loss: 0.00001540\n",
      "Epoch: [47] [  58/  60] time: 104.9974, loss: 0.00002267\n",
      "Epoch: [47] [  59/  60] time: 105.2527, loss: 0.00000292\n",
      "[47/50] - ptime: 16.4449 loss: 0.00009298 acc: 0.72000 lr: 0.00065610\n",
      "Epoch: [48] [   0/  60] time: 107.5018, loss: 0.00000337\n",
      "Epoch: [48] [   1/  60] time: 107.7508, loss: 0.00000493\n",
      "Epoch: [48] [   2/  60] time: 108.0023, loss: 0.00016279\n",
      "Epoch: [48] [   3/  60] time: 108.2521, loss: 0.00000556\n",
      "Epoch: [48] [   4/  60] time: 108.5021, loss: 0.00021847\n",
      "Epoch: [48] [   5/  60] time: 108.7538, loss: 0.00000146\n",
      "Epoch: [48] [   6/  60] time: 109.0027, loss: 0.00000276\n",
      "Epoch: [48] [   7/  60] time: 109.2539, loss: 0.00016261\n",
      "Epoch: [48] [   8/  60] time: 109.5014, loss: 0.00026287\n",
      "Epoch: [48] [   9/  60] time: 109.7553, loss: 0.00000453\n",
      "Epoch: [48] [  10/  60] time: 110.0039, loss: 0.00000343\n",
      "Epoch: [48] [  11/  60] time: 110.2675, loss: 0.00013134\n",
      "Epoch: [48] [  12/  60] time: 110.5311, loss: 0.00000424\n",
      "Epoch: [48] [  13/  60] time: 110.7799, loss: 0.00038044\n",
      "Epoch: [48] [  14/  60] time: 111.0301, loss: 0.00001055\n",
      "Epoch: [48] [  15/  60] time: 111.2924, loss: 0.00001395\n",
      "Epoch: [48] [  16/  60] time: 111.5617, loss: 0.00006682\n",
      "Epoch: [48] [  17/  60] time: 111.8111, loss: 0.00002819\n",
      "Epoch: [48] [  18/  60] time: 112.0665, loss: 0.00003354\n",
      "Epoch: [48] [  19/  60] time: 112.3204, loss: 0.00000973\n",
      "Epoch: [48] [  20/  60] time: 112.5705, loss: 0.00001649\n",
      "Epoch: [48] [  21/  60] time: 112.8207, loss: 0.00009768\n",
      "Epoch: [48] [  22/  60] time: 113.0686, loss: 0.00014430\n",
      "Epoch: [48] [  23/  60] time: 113.3210, loss: 0.00003982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [48] [  24/  60] time: 113.5809, loss: 0.00000078\n",
      "Epoch: [48] [  25/  60] time: 113.8305, loss: 0.00001922\n",
      "Epoch: [48] [  26/  60] time: 114.0928, loss: 0.00007449\n",
      "Epoch: [48] [  27/  60] time: 114.3425, loss: 0.00001762\n",
      "Epoch: [48] [  28/  60] time: 114.5989, loss: 0.00000617\n",
      "Epoch: [48] [  29/  60] time: 114.8596, loss: 0.00003316\n",
      "Epoch: [48] [  30/  60] time: 115.1110, loss: 0.00000015\n",
      "Epoch: [48] [  31/  60] time: 115.3624, loss: 0.00074708\n",
      "Epoch: [48] [  32/  60] time: 115.6166, loss: 0.00001087\n",
      "Epoch: [48] [  33/  60] time: 115.8659, loss: 0.00000400\n",
      "Epoch: [48] [  34/  60] time: 116.1171, loss: 0.00000699\n",
      "Epoch: [48] [  35/  60] time: 116.3668, loss: 0.00002459\n",
      "Epoch: [48] [  36/  60] time: 116.6151, loss: 0.00000147\n",
      "Epoch: [48] [  37/  60] time: 116.8690, loss: 0.00001476\n",
      "Epoch: [48] [  38/  60] time: 117.1212, loss: 0.00102525\n",
      "Epoch: [48] [  39/  60] time: 117.3716, loss: 0.00003277\n",
      "Epoch: [48] [  40/  60] time: 117.6251, loss: 0.00000863\n",
      "Epoch: [48] [  41/  60] time: 117.8816, loss: 0.00002381\n",
      "Epoch: [48] [  42/  60] time: 118.1318, loss: 0.00000028\n",
      "Epoch: [48] [  43/  60] time: 118.3831, loss: 0.00010819\n",
      "Epoch: [48] [  44/  60] time: 118.6350, loss: 0.00000103\n",
      "Epoch: [48] [  45/  60] time: 118.8852, loss: 0.00000013\n",
      "Epoch: [48] [  46/  60] time: 119.1409, loss: 0.00000793\n",
      "Epoch: [48] [  47/  60] time: 119.3946, loss: 0.00000824\n",
      "Epoch: [48] [  48/  60] time: 119.6453, loss: 0.00003489\n",
      "Epoch: [48] [  49/  60] time: 119.8980, loss: 0.00000820\n",
      "Epoch: [48] [  50/  60] time: 120.1492, loss: 0.00000314\n",
      "Epoch: [48] [  51/  60] time: 120.4030, loss: 0.00001202\n",
      "Epoch: [48] [  52/  60] time: 120.6540, loss: 0.00006821\n",
      "Epoch: [48] [  53/  60] time: 120.9109, loss: 0.00000408\n",
      "Epoch: [48] [  54/  60] time: 121.1643, loss: 0.00042115\n",
      "Epoch: [48] [  55/  60] time: 121.4145, loss: 0.00014641\n",
      "Epoch: [48] [  56/  60] time: 121.6672, loss: 0.00075602\n",
      "Epoch: [48] [  57/  60] time: 121.9158, loss: 0.00008853\n",
      "Epoch: [48] [  58/  60] time: 122.1683, loss: 0.00001099\n",
      "Epoch: [48] [  59/  60] time: 122.4194, loss: 0.00000069\n",
      "[48/50] - ptime: 16.5215 loss: 0.00009236 acc: 0.73000 lr: 0.00065610\n",
      "Epoch: [49] [   0/  60] time: 124.6729, loss: 0.00000410\n",
      "Epoch: [49] [   1/  60] time: 124.9232, loss: 0.00000790\n",
      "Epoch: [49] [   2/  60] time: 125.1758, loss: 0.00002691\n",
      "Epoch: [49] [   3/  60] time: 125.4298, loss: 0.00000073\n",
      "Epoch: [49] [   4/  60] time: 125.6801, loss: 0.00003233\n",
      "Epoch: [49] [   5/  60] time: 125.9301, loss: 0.00007505\n",
      "Epoch: [49] [   6/  60] time: 126.1831, loss: 0.00003103\n",
      "Epoch: [49] [   7/  60] time: 126.4371, loss: 0.00000298\n",
      "Epoch: [49] [   8/  60] time: 126.6893, loss: 0.00002207\n",
      "Epoch: [49] [   9/  60] time: 126.9418, loss: 0.00000091\n",
      "Epoch: [49] [  10/  60] time: 127.1973, loss: 0.00018386\n",
      "Epoch: [49] [  11/  60] time: 127.4495, loss: 0.00007793\n",
      "Epoch: [49] [  12/  60] time: 127.7002, loss: 0.00003539\n",
      "Epoch: [49] [  13/  60] time: 127.9514, loss: 0.00001688\n",
      "Epoch: [49] [  14/  60] time: 128.2071, loss: 0.00001375\n",
      "Epoch: [49] [  15/  60] time: 128.4622, loss: 0.00015857\n",
      "Epoch: [49] [  16/  60] time: 128.7114, loss: 0.00000594\n",
      "Epoch: [49] [  17/  60] time: 128.9648, loss: 0.00023536\n",
      "Epoch: [49] [  18/  60] time: 129.2154, loss: 0.00009384\n",
      "Epoch: [49] [  19/  60] time: 129.4631, loss: 0.00000800\n",
      "Epoch: [49] [  20/  60] time: 129.7151, loss: 0.00001300\n",
      "Epoch: [49] [  21/  60] time: 129.9684, loss: 0.00001365\n",
      "Epoch: [49] [  22/  60] time: 130.2206, loss: 0.00004349\n",
      "Epoch: [49] [  23/  60] time: 130.4695, loss: 0.00000337\n",
      "Epoch: [49] [  24/  60] time: 130.7218, loss: 0.00004359\n",
      "Epoch: [49] [  25/  60] time: 130.9747, loss: 0.00005497\n",
      "Epoch: [49] [  26/  60] time: 131.2267, loss: 0.00000355\n",
      "Epoch: [49] [  27/  60] time: 131.4803, loss: 0.00001918\n",
      "Epoch: [49] [  28/  60] time: 131.7279, loss: 0.00000151\n",
      "Epoch: [49] [  29/  60] time: 131.9787, loss: 0.00000121\n",
      "Epoch: [49] [  30/  60] time: 132.2293, loss: 0.00003602\n",
      "Epoch: [49] [  31/  60] time: 132.4796, loss: 0.00000145\n",
      "Epoch: [49] [  32/  60] time: 132.7289, loss: 0.00001564\n",
      "Epoch: [49] [  33/  60] time: 132.9787, loss: 0.00000867\n",
      "Epoch: [49] [  34/  60] time: 133.2371, loss: 0.00002463\n",
      "Epoch: [49] [  35/  60] time: 133.4879, loss: 0.00001502\n",
      "Epoch: [49] [  36/  60] time: 133.7369, loss: 0.00000566\n",
      "Epoch: [49] [  37/  60] time: 133.9847, loss: 0.00031633\n",
      "Epoch: [49] [  38/  60] time: 134.2368, loss: 0.00000403\n",
      "Epoch: [49] [  39/  60] time: 134.4876, loss: 0.00002879\n",
      "Epoch: [49] [  40/  60] time: 134.7501, loss: 0.00009442\n",
      "Epoch: [49] [  41/  60] time: 135.0005, loss: 0.00002514\n",
      "Epoch: [49] [  42/  60] time: 135.2517, loss: 0.00000267\n",
      "Epoch: [49] [  43/  60] time: 135.5063, loss: 0.00001940\n",
      "Epoch: [49] [  44/  60] time: 135.7571, loss: 0.00002436\n",
      "Epoch: [49] [  45/  60] time: 136.0083, loss: 0.00001587\n",
      "Epoch: [49] [  46/  60] time: 136.2606, loss: 0.00000010\n",
      "Epoch: [49] [  47/  60] time: 136.5091, loss: 0.00005466\n",
      "Epoch: [49] [  48/  60] time: 136.7598, loss: 0.00000108\n",
      "Epoch: [49] [  49/  60] time: 137.0084, loss: 0.00000163\n",
      "Epoch: [49] [  50/  60] time: 137.2640, loss: 0.00000080\n",
      "Epoch: [49] [  51/  60] time: 137.5185, loss: 0.00004599\n",
      "Epoch: [49] [  52/  60] time: 137.7666, loss: 0.00035776\n",
      "Epoch: [49] [  53/  60] time: 138.0180, loss: 0.00001144\n",
      "Epoch: [49] [  54/  60] time: 138.2683, loss: 0.00000058\n",
      "Epoch: [49] [  55/  60] time: 138.5248, loss: 0.00006214\n",
      "Epoch: [49] [  56/  60] time: 138.7771, loss: 0.00002507\n",
      "Epoch: [49] [  57/  60] time: 139.0242, loss: 0.00022899\n",
      "Epoch: [49] [  58/  60] time: 139.2740, loss: 0.00000674\n",
      "Epoch: [49] [  59/  60] time: 139.5227, loss: 0.00000393\n",
      "[49/50] - ptime: 16.4650 loss: 0.00004450 acc: 0.73000 lr: 0.00065610\n",
      "Epoch: [50] [   0/  60] time: 141.7284, loss: 0.00005170\n",
      "Epoch: [50] [   1/  60] time: 141.9811, loss: 0.00078378\n",
      "Epoch: [50] [   2/  60] time: 142.2362, loss: 0.00000259\n",
      "Epoch: [50] [   3/  60] time: 142.4862, loss: 0.00000179\n",
      "Epoch: [50] [   4/  60] time: 142.7425, loss: 0.00000767\n",
      "Epoch: [50] [   5/  60] time: 142.9915, loss: 0.00000189\n",
      "Epoch: [50] [   6/  60] time: 143.2489, loss: 0.00004147\n",
      "Epoch: [50] [   7/  60] time: 143.5020, loss: 0.00000163\n",
      "Epoch: [50] [   8/  60] time: 143.7548, loss: 0.00000073\n",
      "Epoch: [50] [   9/  60] time: 144.0052, loss: 0.00001201\n",
      "Epoch: [50] [  10/  60] time: 144.2583, loss: 0.00000699\n",
      "Epoch: [50] [  11/  60] time: 144.5092, loss: 0.00001252\n",
      "Epoch: [50] [  12/  60] time: 144.7572, loss: 0.00001109\n",
      "Epoch: [50] [  13/  60] time: 145.0072, loss: 0.00000830\n",
      "Epoch: [50] [  14/  60] time: 145.2588, loss: 0.00000267\n",
      "Epoch: [50] [  15/  60] time: 145.5109, loss: 0.00016568\n",
      "Epoch: [50] [  16/  60] time: 145.7605, loss: 0.00000223\n",
      "Epoch: [50] [  17/  60] time: 146.0083, loss: 0.00008638\n",
      "Epoch: [50] [  18/  60] time: 146.2627, loss: 0.00000181\n",
      "Epoch: [50] [  19/  60] time: 146.5184, loss: 0.00000045\n",
      "Epoch: [50] [  20/  60] time: 146.7699, loss: 0.00000161\n",
      "Epoch: [50] [  21/  60] time: 147.0178, loss: 0.00001241\n",
      "Epoch: [50] [  22/  60] time: 147.2709, loss: 0.00001082\n",
      "Epoch: [50] [  23/  60] time: 147.5238, loss: 0.00009169\n",
      "Epoch: [50] [  24/  60] time: 147.7708, loss: 0.00002282\n",
      "Epoch: [50] [  25/  60] time: 148.0227, loss: 0.00001306\n",
      "Epoch: [50] [  26/  60] time: 148.2835, loss: 0.00000120\n",
      "Epoch: [50] [  27/  60] time: 148.5362, loss: 0.00000013\n",
      "Epoch: [50] [  28/  60] time: 148.7881, loss: 0.00000027\n",
      "Epoch: [50] [  29/  60] time: 149.0375, loss: 0.00001658\n",
      "Epoch: [50] [  30/  60] time: 149.3049, loss: 0.00010996\n",
      "Epoch: [50] [  31/  60] time: 149.5720, loss: 0.00001573\n",
      "Epoch: [50] [  32/  60] time: 149.8200, loss: 0.00000378\n",
      "Epoch: [50] [  33/  60] time: 150.0731, loss: 0.00000003\n",
      "Epoch: [50] [  34/  60] time: 150.3289, loss: 0.00000122\n",
      "Epoch: [50] [  35/  60] time: 150.5830, loss: 0.00002516\n",
      "Epoch: [50] [  36/  60] time: 150.8422, loss: 0.00000124\n",
      "Epoch: [50] [  37/  60] time: 151.0916, loss: 0.00002750\n",
      "Epoch: [50] [  38/  60] time: 151.3553, loss: 0.00000899\n",
      "Epoch: [50] [  39/  60] time: 151.6033, loss: 0.00000066\n",
      "Epoch: [50] [  40/  60] time: 151.8529, loss: 0.00000298\n",
      "Epoch: [50] [  41/  60] time: 152.1087, loss: 0.00001710\n",
      "Epoch: [50] [  42/  60] time: 152.3633, loss: 0.00000637\n",
      "Epoch: [50] [  43/  60] time: 152.6165, loss: 0.00009726\n",
      "Epoch: [50] [  44/  60] time: 152.8672, loss: 0.00006850\n",
      "Epoch: [50] [  45/  60] time: 153.1217, loss: 0.00000852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50] [  46/  60] time: 153.3732, loss: 0.00002047\n",
      "Epoch: [50] [  47/  60] time: 153.6216, loss: 0.00006588\n",
      "Epoch: [50] [  48/  60] time: 153.8733, loss: 0.00022469\n",
      "Epoch: [50] [  49/  60] time: 154.1260, loss: 0.00001752\n",
      "Epoch: [50] [  50/  60] time: 154.3779, loss: 0.00000041\n",
      "Epoch: [50] [  51/  60] time: 154.6305, loss: 0.00000761\n",
      "Epoch: [50] [  52/  60] time: 154.8832, loss: 0.00003888\n",
      "Epoch: [50] [  53/  60] time: 155.1308, loss: 0.00001014\n",
      "Epoch: [50] [  54/  60] time: 155.3921, loss: 0.00001164\n",
      "Epoch: [50] [  55/  60] time: 155.6448, loss: 0.00000051\n",
      "Epoch: [50] [  56/  60] time: 155.8961, loss: 0.00000128\n",
      "Epoch: [50] [  57/  60] time: 156.1474, loss: 0.00001446\n",
      "Epoch: [50] [  58/  60] time: 156.4008, loss: 0.00000090\n",
      "Epoch: [50] [  59/  60] time: 156.6505, loss: 0.00000756\n",
      "[50/50] - ptime: 16.5327 loss: 0.00003652 acc: 0.73000 lr: 0.00065610\n",
      "Avg per epoch ptime: 16.11, total 50 epochs ptime: 157.36\n",
      " [*] Training finished!\n",
      " [*] Best Epoch:  5 , Accuracy:  0.800000011920929\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay-5\n",
      " [*] Finished testing Best Epoch: 5 , accuracy:  0.800000011920929 !\n"
     ]
    }
   ],
   "source": [
    "dataset = '4_Flowers_1s'\n",
    "epoch = 50\n",
    "batch_size = 100\n",
    "checkpoint_dir = 'checkpoint'\n",
    "log_dir = 'logs'\n",
    "trainhist_dir = 'trainhist'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "# --log_dir\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# --trainhist_dir\n",
    "if not os.path.exists(trainhist_dir):\n",
    "    os.makedirs(trainhist_dir)\n",
    "\n",
    "# open session\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    \n",
    "    # declare instance for GAN\n",
    "    CNN = CNN(sess, epoch=epoch, batch_size=batch_size, dataset_name=dataset, checkpoint_dir=checkpoint_dir, \n",
    "                log_dir=log_dir, trainhist_dir=trainhist_dir)\n",
    "\n",
    "    # build graph\n",
    "    CNN.build_model()\n",
    "\n",
    "    # show network architecture\n",
    "    CNN.show_all_variables()\n",
    "\n",
    "    # launch the graph in a session\n",
    "    CNN.train()\n",
    "    \n",
    "#     CNN.test(epoch)\n",
    "        \n",
    "sess.close()\n",
    "        \n",
    "# lrdecay\n",
    "# [*] Training finished!\n",
    "#  [*] Best Epoch:  5 , Accuracy:  0.800000011920929\n",
    "# INFO:tensorflow:Restoring parameters from checkpoint/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay/CNN_Pyramid_G_C5_D1_Kernel(3,3)_128_lrdecay-5\n",
    "#  [*] Finished testing Best Epoch: 5 , accuracy:  0.800000011920929 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "file='/home/huiqy/Music/CloudMusic/All_Time_Low.mp3' #文件名是完整路径名\n",
    "pygame.mixer.init() #初始化音频\n",
    "track = pygame.mixer.music.load(file)#载入音乐文件\n",
    "pygame.mixer.music.play()#开始播放\n",
    "time.sleep(60)#播放10秒\n",
    "pygame.mixer.music.stop()#停止播放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
